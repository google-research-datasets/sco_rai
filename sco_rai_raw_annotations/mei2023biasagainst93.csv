,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{mei2023biasagainst93,
    author = {Katelyn X. Mei and Sonia Fereidooni and Aylin Caliskan},
    title = {Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,SocialBias,Perceived_Problem,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,social biases,
10,NegativeWordProbability,Artifact,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,the probability of MLMs predicting negative words ,
11,DownstreamSentimentClassifier,Artifact,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,four downstream sentiment classifiers of these models.,
12,NegativeClassification,Artifact,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,more likely to be classified as negative,
13,LanguageModel,Agent,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,language models and their downstream tasks.,
14,BiasAgainstStigmatizedGroup,Causal_Theory,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,Bias against stigmatized groups,
15,DiseaseBias,Causal_Theory,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,"stigmatized conditions related to diseases,",
16,EducationBias,Causal_Theory,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,stigmatized conditions related to [...] disability,
17,MentalIllnessBias,Causal_Theory,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,stigmatized conditions related to [...] education,
18,DisabilityBias,Causal_Theory,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,stigmatized conditions related to [...] mental illness,
19, , , , , ,
20, , , , , ,
21, , , , , ,
22, , , , , ,
23, , , , , ,
24, , , , , ,
25, , , , , ,
26,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
27,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
28,SocialBias,constrainsAgent,LanguageModel,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,A growing body of work has shown that social biases are encoded in language models and their downstream tasks.
29,NegativeWordProbability,reflectsPrecept,BiasAgainstStigmatizedGroup,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,"the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions."
30,DownstreamSentimentClassifier,reflectsPrecept,BiasAgainstStigmatizedGroup,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models.
31,NegativeClassification,reflectsPrecept,DiseaseBias,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,"sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative"
32,NegativeClassification,reflectsPrecept,DisabilityBias,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,"sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative"
33,NegativeClassification,reflectsPrecept,EducationBias,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,"sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative"
34,NegativeClassification,reflectsPrecept,MentalIllnessBias,"The rapid deployment of artificial intelligence (AI) models de-demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society.A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stig-matized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness,religion, sexuality, socioeconomic status, and other relevant factors.We investigate bias against these groups in English pre-trainedMasked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stig-matized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large,XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We Use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stig-matized groups. When prompts include stigmatized conditions,the probability of MLMs predicting negative words is, on average,20 percent higher than when prompts have non-stigmatized con-ditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r=0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigma-tized groups.",1699,"sentences include stigmatized conditions related to diseases, disability, edu-cation, and mental illness, they are more likely to be classified as negative"
