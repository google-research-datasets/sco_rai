,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{panigutti2023, author = {Panigutti, Cecilia and Hamon, Ronan and Hupont, Isabelle and Fernandez Llorca, David and Fano Yela, Delia and Junklewitz, Henrik and Scalzo, Salvatore and Mazzini, Gabriele and Sanchez, Ignacio and Soler Garrido, Josep and Gomez, Emilia}, title = {The Role of Explainable AI in the Context of the AI Act}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,EuropeanUnion,Agent,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,EU,
10,GDPR,Agent,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,GDPR,
11,DebateOverRightToExplanation,Perceived_Problem,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,debates [...] on whether a right to explanation on ADM for data subjects was established by the GDPR,
12,HighLevelExpertGroup,Agent,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,HLEG,
13,WhitePaperOnAI,Artifact,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,European Commission White Paper On Artificial Intelligence - A European approach to excellence,
14,TrasparencyObligations,Perceived_Need,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,dedicated transparency obligations,
15,ProtectionsAndSafeguards,Strategy,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,"certain protections and safeguards for individuals when subjected to certain decisions based solely on automated processing of personal data, including profiling",
16,RightToAnExplanation,Perceived_Need,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,the right to obtain an explanation of the decision.,
17,LegalScholars,Agent,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,legal circles,
18,TransparencyRequirementGuidelines,Artifact,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,"transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication.",
19,EmphasisOnTransparency,Goal,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,"information to be provided, i.e., transparency, as a possible core requirement.",
20,NonHighRiskAISystems,Agent,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,certain AI systems that aren't considered high-risk per se,
21, , , , , ,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,EuropeanUnion,hasProducedArtifact,GDPR,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems."
26,GDPR,influencesPrecept,ProtectionsAndSafeguards,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,"Among other measures, the GDPR establishes certain protections and safeguards for individuals when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22)."
27,GDPR,reflectsPrecept,RightToAnExplanation,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,"In recital 71, it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision."
28,DebateOverRightToExplanation,constrainsAgent,LegalScholars,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR
29,HighLevelExpertGroup,hasProducedArtifact,TransparencyRequirementGuidelines,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,"The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication."
30,HighLevelExpertGroup,hasProducedArtifact,WhitePaperOnAI,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence
31,WhitePaperOnAI,reflectsPrecept,EmphasisOnTransparency,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,"European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement."
32,TrasparencyObligations,constrainsAgent,NonHighRiskAISystems,"The GDPR, which is EU law already in force, constitutes a key milestone to address concerns about the opacity of decision-making processes by automated systems, including algorithmic systems. The GDPR puts forward provisions for automated decision-making (ADM) on the basis of personal data. Among other measures, theGDPR establishes certain protections and safeguards for individu-als when subjected to certain decisions based solely on automated processing of personal data, including profiling (Article 22). These Measures include specific transparency obligations4. In recital 71,it is clarified that safeguards for data subjects should include the right to obtain an explanation of the decision. In addition, Article 13(2)(f), Article 14(2)(g), and Article 15(1)(h) require that data subjects are provided meaningful information about the logic involved in ADM. These GDPR provisions have led to debates in legal circles on whether a right to explanation on ADM for data subjects was established by the GDPR [61, 74, 116]. The above-mentioned provi-sions do not constitute an explicit requirement for the use of any specific method for understanding the decisions made in the con-text of ADM. However, it is arguable that, if an explanation should be provided when a certain automated decision is taken, then it would be necessary for the decision to be taken by transparent unexplainable models [50]. Additionally, some form of explanation may also be considered critical for the exercise of GDPR rights, such as to express one’s point of view on the decision and to contest the decision itself (Article 22(3)). Regardless of the GDPR provisions,the extent of technical requirements for explainable systems and to what degree an explanation would be technically feasible with complex AI models is still subject to debate and research [50]. Starting in 2018, the European Commission unveiled its AI strategy and coordinated plan for AI [37]. The Communication on AI for Europe [36] led to the appointment of the independent HLEG. The HLEG published their ethics guidelines, which includes transparency among the seven key requirements on trustworthy AI. In this context transparency includes elements of traceability, explainability and communication. While remaining non-binding, the key requirements were subsequently supported by the European Commission’s communication on Building Trust in Human-CentricArtificial Intelligence [38]. The Commission considered those key requirements as valuable input for its policy-making and based on European values, while acknowledging that the requirements maybe already reflected in existing provisions of EU law. The HLEG guidelines further inspired the European Commission White Paper On Artificial Intelligence - A European approach to excellence and trust [23] which indicated information to be provided, i.e., transparency, as a possible core requirement. Many national strategies of EU Member States incorporate similar ideas, and the need to Questions around ADM and transparency in GDPR have been the subject of dedicated guidance documents by the Article 29 Data Protection Working Party: Guidelines onAutomated individual decision-making and Profiling for the purposes of Regulation 2016/679 (wp251rev.01) and Guidelines on Transparency under Regulation 2016/679(wp260rev.01).further research into XAI in one way or another [113]. Followingpublic consultations on the White Paper, in 2021, in 2021 the Euro-pean Commission presented the AI Act. As previously mentioned,the AI Act sets out several requirements for high-risk AI systems,including transparency and human oversight that we analyze insection 5. In addition, although they are not strictly related to the concept of opacity as discussed herein, dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se. For instance, Article 52 foresees that natural persons should be informed when interacting with an AI system, unless this is obvious from the circumstances, and they should also be informed when certain artificially generated content appreciably resembles existing persons, places or events and may led to deception.",1141,dedicated transparency obligations are included in the AI Act for certain AI systems that aren't considered high-risk per se
