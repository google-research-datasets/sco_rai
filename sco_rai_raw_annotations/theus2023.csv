,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{theus2023, author = {Theus, Anna-Lena},
title = {Striving for Affirmative Algorithmic Futures: How the Social Sciences Can Promote More Equitable and Just Algorithmic System Design},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,RacialUnfairness,Perceived_Problem,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,regularly fail to recognize faces of color,
10,FacialRecognitionDataSets,Artifact,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,Contemporary facial recognition systems,
11,InclusionOfDiverseFaces,Strategy,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,to include diverse faces in the datasets used to train them ,
12,HistoricalInjustices,Artifact,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,"underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design",
13,RacialInequalities,Perceived_Problem,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,the racial inequalities that are baked into their datasets,
14, , , , , ,
15, , , , , ,
16, , , , , ,
17, , , , , ,
18,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
19,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
20,RacialUnfairness,constrainsAgent,FacialRecognitionSystems,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,Contemporary facial recognition systems regularly fail to recognize faces of color
21,FacialRecognitionDataSets,reflectsPrecept,RacialInequalities,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,Contemporary facial recognition systems [...] the racial inequalities that are baked into their datasets
22,InclusionOfDiverseFaces,constrainsAgent,FacialRecognitionSystems,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them
23,HistoricalInjustices,influencesPrecept,RacialUnfairness,"A great example for how the representation of people and cul-ture in algorithmic classification systems reflects the social context within which a system has been created and exists, are facial recog-nition technologies. Contemporary facial recognition systems reg-ularly fail to recognize faces of color, thereby further extending the racial inequalities that are baked into their datasets, which in turn reflect the under- or overrepresentation of race as well as the opera-tionalization of whiteness as the norm in technology development,and an implicit bias that already exists against minority groups in general [14, 20, 29, 63]. One common proposition for counter-ing facial recognition technologies’ discriminatory outcomes is to include diverse faces in the datasets used to train them [79]. But,as Nikki Stevens and Os Keyes [79] point out, a dataset does not become more equitable just because “everyone’s face is equally represented” (p. 834). They emphasize that the way in which race is baked into datasets is “reflective of their site of use” (p. 848) and focusing solely on representation deflects from the “logics and systems of inequality” (p. 848) that shape the purpose for which datasets is produced. Instead, instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design and result in outcomes that reflect racial judgments about the deviation from the norm, reinforces structural oppression based on categories such as race, class, and gender, and deepens and justifies historical in-equalities, all while hiding behind the argument of computational and mathematical neutrality [7, 14, 25, 27, 84]. Ultimately, explicit malicious intent is not required for algorithmic systems to produce harm, “a lack of concern for how the past shapes the present” (p.60)[14] is all it takes.",562,"instances of biased outcomes and failure are symptoms for an underlying discriminatory process, in which historical, unjust information organization practices carryover into contemporary algorithmic system design"
