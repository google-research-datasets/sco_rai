,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{barrett2023skindeepinvestigating,
    author = {Teanna Barrett and Quan Ze Chen and Amy X. Zhang},
    title = {Skin Deep: Investigating Subjectivity in Skin Tone Annotations for Computer Vision Benchmark Datasets},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-09, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,ComputerVision,Agent,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,computer vision,
10,AnnotationImprecision,Perceived_Problem,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,the imprecise nature of the annotation scales and measurements ,
11,AnnotatorUncertainty,Perceived_Problem,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,annotator and annotation uncertainty.,
12,Annotations,Artifact,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,skin tone annotations ,
13,SkinToneObjectivity,Causal_Theory,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,accepted skin tone as a more objective annotation than race,
14,OwnSkinTone,Other_Precept,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,we asked annotators to mark their own skin tone along the scale using the annotation tool after completing the tutorial.,
15,CrowdWorkers,Agent,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,crowd-workers annotators,
16,USSkinToneContext,Causal_Theory,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,the racial and skin tone-stratification contexts of the U.S.,
17,LighterSkinTone,Other_Precept,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,self-reported lighter skin tones,
18,DarkToLightScale,Strategy,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,dark-to-light scale,
19,ImplicitBiases,Causal_Theory,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,implicit biases or social environments,
20,Researchers,Agent,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,many researchers,
21,SocialMeaning,Causal_Theory,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,social meaning,
22,FitzpatrickDataSet,Artifact,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. ",
23,IJB-C,Artifact,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators",
24,SkinToneAnnotations,Artifact,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,skin tone is a viable annotation attribute,
25,Annotators,Agent,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,annotator,
26, , , , , ,
27,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
28,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
29,ComputerVision,hasProducedArtifact,SkinToneAnnotations,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,skin tone is a viable annotation attribute for computer vision
30,AnnotationImprecision,constrainsAgent,ComputerVision,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements "
31,AnnotatorUncertainty,constrainsAgent,ComputerVision,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty."
32,Annotations,reflectsPrecept,ImplicitBiases,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,here is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations.
33,SkinToneObjectivity,constrainsAgent,Researchers,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,many researchers have accepted skin tone as a more objective annotation than race
34,Annotations,reflectsPrecept,SocialMeaning,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,skin tone annotations may also carry social meaning
35,OwnSkinTone,constrainsAgent,Annotators,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial."
36,CrowdWorkers,hasProducedArtifact,FitzpatrickDataSet,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators"
37,CrowdWorkers,hasProducedArtifact,IJB-C,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators"
38,CrowdWorkers,hasProducedArtifact,Annotations,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate."
39,USSkinToneContext,constrainsAgent,Annotators,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"We also required annotators to be U.S.-based, so that they are also situated in the racial and skin tone-stratification contexts of the U.S."
40,LighterSkinTone,constrainsAgent,Annotators,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range."
41,DarkToLightScale,constrainsAgent,Annotators,"While skin tone is a viable annotation attribute for computer vision, its potential is limited by the current state of skin tone annotation processes. The primary problem identified by researchers is the imprecise nature of the annotation scales and measurements [12, 46]. The discrete categories in scales like FST do not sufficiently represent the possible skin tones of human subjects. Another aspect that has not been deeply explored is annotator and annotation uncertainty. There has been some documentation and analysis of the uncertainty of skin tone annotations [9, 14, 15, 32, 33, 80, 85],though our literature review finds this is rare. These works primarily provide analyses such as inter-rater reliability or some other measure of consensus or (dis)agreement. However, there is little investigation of the implicit biases or social environments that could also impact the certainty and consistency of the annotations. As many researchers have accepted skin tone as a more objective annotation than race, there has been little investigation into how skin tone annotations may also carry social meaning. Through our experiment, we provide empirical data on how different annota-tion processes such as differing scales and social aspects such as annotator positionality may impact annotation uncertainty. Annotation Tool: We based the design of our annotation tool on a range-based annotation system, Goldilocks [17], which utilizes a two-stage process to collect range ratings on a continuous scale (Figure 1). One goal in our experiment was to explore limitations of the level of granularity afforded by existing scales. Thus, we used continuous ratings to give annotators full freedom in assigning their ratings while referencing a scale rather than being tied to the established levels on a specific skin tone scale. Additionally, like in Goldilocks, each annotator provided both upper and lower bounds rather than a single rating value. This allowed for a per-annotator estimate of uncertainty around each annotation, improving our insights into uncertainty beyond disagreement metrics. To ground the pre-existing scales (either MST [71] or FST [28]), we displayed the color swatches that define the scale levels as anchors under our continuous scale. As MST and FST do not specify distancesbetween levels, we placed anchors uniformly across our scale. Wealso provided an anchor in the form of an example image drawn from the dataset of the task images. Experiment Setup: For an annotator on a rating task, the most important factors they will engage with are the scale and the items being rated. Thus, we set up the following conditions (denoted in small-caps) to test how differences here might affect the skin tone annotations produced:‚Ä¢ Scale Type: We tested the 6-point FST [28] scale (fitz) and the10-point MST [63] scale (monk).‚Ä¢ Scale Order: Since each scale is mapped to a [0, 1] range, we tested varying the scale order to be either lighter to darker (ld) where lower values represent lighter skin tones, or darker to lighter (dl)where lower values represent darker skin tones. Scale values are presented left-to-right for increasing values.‚Ä¢ Image Type: We tested 2 different types of images, based on whether a whole face was visible. Images from the skin condi-tion contained images of skin conditions without faces or with partial facial features [33] while images from the face condition contained portrait photos with the subject‚Äôs face [54]. In our experiments, we tested all combinations above, result-ing in 8 total combinations for how the annotation tool could be configured. Across all conditions, annotators were asked to first familiarize themselves with the tool by completing a tutorial using the tool under that configuration, with feedback mechanisms on their answer similar to gated instructions used in crowd task train-ing [50], before proceeding to the annotation task. To investigate annotator positionality, we asked annotators to mark their ownskin tone along the scale using the annotation tool after completing the tutorial. Based on prior recommendations [38], we provide tips for assessing their skin tone.Image Dataset: We drew images from two datasets: the Fitz-patrick17k [32] (skin) and the IARPA Janus Benchmark-C (IJB-C)[54] (face). The Fitzpatrick 17k dataset is a 17,000 image dataset of medical images of skin conditions from a variety of publicly available medical textbooks. The IJB-C dataset holds over 138,000 publicly available images from the Internet. Both datasets were manually annotated using FST and included annotations from crowd-workers annotators. We curated images from each dataset to achieve a spread across skin tones, resulting in 12 selected images for the face dataset and 11 images for the skin dataset. Annotators were then randomly assigned to annotation task sessions where they annotated one group of 6 images under one of our 8 conditions. Annotator Recruitment: For the main component of our annotation study, we recruited 160 U.S.-based annotators from AWSMTurk with the criteria of having completed at least 1000 tasks with a 95% or higher approval rate. Crowd annotators were paid$8.50 per task ($2.50 base pay and $1.00 bonus per annotation)for a 30 minute task, and were not allowed to participate in more than one condition. We conducted manual quality control checksfor spam behavior and redeployed instances where this was ob-served. After removing incomplete tasks, we had a final set from153 crowd annotators. Annotators were asked to provide general demographic information to determine the representation of our participant sample along race and gender identities in the U.S. con-text. This enabled us to diagnose and correct for demographic skew in our sample. After the AWS MTurk deployment, we noticed that our recruited annotator population skewed heavily towards those who self-reported as White (84.2%). Prior work surveying MTurkworkers has identified similar demographic imbalances [24]. To correct our demographic skew to be more in line with the U.S. adult population [13], we augmented the crowd annotators with an addi-tional sample of annotators recruited through social channels and personal networks, focusing on increasing the representation of those who identified as non-White. These annotators were paid in the form of a $10 gift card and were assigned one of the condi-tions randomly. At the end we recruited 12 additional non-crowdannotators. The final demographic distribution of our full annotator pool of 165 participants was: 78.2% White, 7.9% Asian, 6.7% Black, 4.8%Latino, 0.6% Native-American, and 1.8% multiple. The gender distri-bution of annotators was 40.6% female, 58.8% male, 0.6% non-binary.We also asked participants to self-report their own skin tone using the annotation tool; the distribution is shown in Appendix B. We discuss limitations of our sample in Section 6. Designer and Annotator Positionality: A limitation of prior literature was the opaqueness of the designer positionality. When the authors did not describe the decision-making behind their process, we could only guess what social, political, and ethical perspec-tives informed the design. By stating our positionality as designers,we hope to not only reflect on how our identities impacted our design but directly engage with the limits of our perspectives. The study was scoped to the U.S. as all of the authors reside in the U.S.and are at U.S. institutions. This informed decisions such as the ethnicity and gender categories we used in our survey. We alsorequired annotators to be U.S.-based, so that they are also situatedin the racial and skin tone-stratification contexts of the U.S.For Black populations in the U.S., skin tone has been used since the Atlantic Slave Trade to allocate degrees of social power to en-slaved people [64]. Even today, the associations to skin tone are reflected in sentencing trends in the U.S. justice systems [63]. Given the context, there is great responsibility placed on an annotator when they select skin tone for themselves and image subjects. Thus,we surveyed annotators about their comfort with the task of an-notating skin tone (Appendix C). Finally, as mentioned, we asked annotators to mark their own skin tone‚Äîthis enabled us to investigate another aspect of annotator positionality. 4.2.1 RQ 1: Do the scales (fitz, monk) correlate with each other for measuring skin tones? For our first research question, we wanted to confirm whether there is general agreement in the values produced between the two skin tone annotation scale types we used. Across both image types (face and skin), we found high positive correlation for the upper (ùëÖ2 = 0.721 and 0.680 respectively) and lower bounds (ùëÖ2 = 0.692 and 0.662 respectively) produced by an-notators using the two scales (Figure 2). This result largely serves as a check to validate that both skin tone scales were indeed able to capture differences across a range of skin tones and that annotators were generally able to utilize our annotation interface with existing scales for annotating skin tones. To examine whether our control variables affected agreement between annotators, we used a linear model multi-way ANOVA test to compare the effect of the inde-pendent variables: scale type (fitz, monk), scale order (ld, dl), andimage type (face, skin) variables as well as any pairwise interac-tions, on the dependent variable of standard deviation of the upper and lower bounds. The standard deviation of each bound is used as it is a common way to characterize inter-annotator agreement in a continuous rating scale setting (Figure 3). We found that for both lower and upper bounds, the only signif-icant variable (at ùëù = 1.5 √ó 10‚àí3 < 0.05, and ùëù = 2.9 √ó 10‚àí3 < 0.05 for each bound respectively) that affected annotator agreement was the image type, with images in the face condition showing more agreement than that of skin. As skin tone annotation is most commonly conducted on datasets involving portrait shots withvisible faces, this finding may suggest that skin tone annotation on less common image types, like images of skin patches without faces, may result in lower agreement between annotators as they have less context to draw from. However, the inclusion of faces also potentially biases annotators towards using other contextual features such as race and ethnicity, which may have affected their consistency. 4.2.3 RQ 3: Does scale type, ordering of scale, or image type affect the uncertainty of each annotator? While previously we explored the effects of our controlled variables on agreement between dif-ferent annotators, here we examined whether the configuration of annotation system affects each annotator‚Äôs own individual uncer-tainty during annotation. As we utilized a range-based annotation system, we were able to quantify this uncertainty by examining the size of the ranges produced by each annotator. Similar to the section before, we conducted an ANOVA test to compare our independent variables and pairwise interactions against the size of the range produced by each annotator (Figure 4).We found two significant effects: the scale order (ùëù = 0.029 <0.05) and the interaction of scale type √ó image type (ùëù = 0.010 <0.05). Of the latter interaction effect, we found that there was a statistically significant difference between the pairings (fitz x face)and (monk x face) at ùëù = 0.024 < 0.05 (identified via Tukey‚ÄôsHSD). Thus, we conclude that the order in which the skin tone is presented can affect individual annotators‚Äô own evaluation of their uncertainty. However, how it affected uncertainty seems to depend on the type of image and scale being used. Overall, we found that using a dark-to-light scale ordering tended to result in lower individual uncertainty. However, when annotating skin images, this trend was not observed for the Fitzpatrick scale. We hypothesize that this may have been the result of multiple factors at work: As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range. On the other hand, the lack of additional race and ethnicity context in the skin task may have worked in conjunction with the fitz scale‚Äôs specialization for directly annotating skin tone on skin patches, reducing this effect.4.2.4 RQ 4: Does the annotator‚Äôs own self-reported skin tone (positionality) bias their uncertainty or agreement? Finally, we examined whether the annotators‚Äô self-reported skin tone correlated with how they produced skin tone annotations. Specifically, we tested for two forms of potential biases that could occur. For value bias,we looked at whether the annotator‚Äôs own skin tone (relative to the mean across all annotators) correlated with their annotations(relative to the mean across annotations produced under the same experiment conditions). A positive or negative correlation here would indicate that an annotator biases their annotations towards or away from their own skin tone. As shown in Figure 5a, we note avery weak (not significant) positive trend, with the most prominent ùëÖ2 = 0.100 observed for skin images using the monk scale. This suggests that annotators may potentially bias their annotated skin tone towards their own. However, we also note the caveat that data points get much sparser the further we get from the mean skin tone, which we hypothesize is likely due to the demographic concentration of our annotators. For uncertainty bias, we examined whether the difference between the annotated image‚Äôs skin tone and the annotator‚Äôs ownskin tone correlated with their self-reported uncertainty in the form of the size of their range. A positive or negative correlation here would indicate that an annotator is more or less uncertain the further the annotated image‚Äôs skin tone is from their own skin tone.As shown in Figure 5b, we note a very weak (not significant) nega-tive trend, with the most prominent ùëÖ2 = 0.056 observed for skinimages using the monk scale. This may suggest that annotators are potentially more certain when annotating images that have a skin tone different from their own. However, as before, any potential trends may be a result of the lack of annotators on the darker end of the skin tone spectrum.",1762-5,"As a large majority of our annotators self-reported lighter skin tones, when utilizing our annotation process which establishes the lower bound first, a dark-to-light scale may result in better estimation of the lower bound from more easily contrasting skin tones, reducing the size of the final range."
