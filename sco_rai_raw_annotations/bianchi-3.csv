,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{bianchi2023, author = {Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and Ladhak, Faisal and Cheng, Myra and Nozza, Debora and Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and Caliskan, Aylin}, title = {Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,StableDiffusion,Agent,"One might hope that by generating images solely of non-human entities, we avoid reproducing representations of demographic groups and other problematic biases. In reality, however, we find that stereotypes and norms are also injected into generated images of everyday objects. Building upon the findings of Wolfe et al.[72] that visual semantic models like CLIP reproduce American racial hierarchies, we explore whether generated images from Stable Diffusion encode American norms. In prior work, De Vries et al.[18] test object recognition systems on a dataset of 117 classes of household objects, such as beds, doors, etc. and find that they work poorly on images from low-income countries. Using a set of objectsFigure 5: Generated images of everyday objects encode persis-tent stereotypes. The images generated from prompts with no identity descriptor perpetuate North American norms of objects’ appearances: these neutral prompts are typically extremely similar to images generated from prompts with“North America” (top row). These are most different from prompts with “Africa” (bottom row), which encode harmful stereotypes of poverty. We present two random examples foreach prompt.from this list, for each object, we used Stable Diffusion to generate 100 images using the following prompts: (1) A general prompt(“a photo of [OBJECT]”) (2) A North-America-specific prompt (“photo of [OBJECT] in North America”) (3) An Asia-specific prompt(“a photo of [OBJECT] in Asia.”) and (4) An Africa-specific prompt(“a photo of [OBJECT] in Africa”).2 We present random example in Figure 5. We see that seemingly neutral prompts about objects produce decisively culture-specific images: the general prompt produces outputs that are most similar to the North-America-specific prompt, while visually differing most greatly from outputs with theAfrica-specific prompt.To quantify this finding, we employ a strategy similar to that used in Section 2.2. We again study the representations in CLIP [56],the core representational component of Stable Diffusion. For each of the continent-specific prompts (e.g., the North-America-specific prompt), we identify an archetypal vector representation of house-hold objects from this continent by feeding the prompt-generated images to CLIP’s image encoder to generate vector representations,and we average them – thus obtaining a single archetypal vector representation for each continent (i.e., a vector for North America, vector for Asia, and a vector for Africa). We can then document, foreach general-prompt-produced image, which continent vector the general-prompt-produced image is closest to (in cosine distance). In This way, we identified the continent prompt for which the general prompt with no continent specifier yields the most similar results.Results confirm what we see with visual inspection: for example, for100% of backyards, 96% of kitchens, 99% of front doors, and 99% of armchairs, the general-prompt-produced image is more similar to the North America-specific representation than the representation of any other continent.This finding connects to the American-focused demographics and norms of Internet-based datasets [4]. Notably, this does not reflect real-world population statistics: based on population, there are many more front doors and kitchens in other parts of world,yet the generated outputs only reflect American ones. By gener-ating images that are stylized as American when prompted with everyday objects, these models create a version of the world that further entrenches the view of American as default. This “view from nowhere,” i.e. hiding a specific perspective and set of assumptions under the guise of neutrality, has been long-studied and criticized by sociologists for contributing to the exclusion and ostracizing of those who do not belong to the default group [27, 58].",1499-1500,Stable Diffusion,
10,GeographicBias,Perceived_Problem,"One might hope that by generating images solely of non-human entities, we avoid reproducing representations of demographic groups and other problematic biases. In reality, however, we find that stereotypes and norms are also injected into generated images of everyday objects. Building upon the findings of Wolfe et al.[72] that visual semantic models like CLIP reproduce American racial hierarchies, we explore whether generated images from Stable Diffusion encode American norms. In prior work, De Vries et al.[18] test object recognition systems on a dataset of 117 classes of household objects, such as beds, doors, etc. and find that they work poorly on images from low-income countries. Using a set of objectsFigure 5: Generated images of everyday objects encode persis-tent stereotypes. The images generated from prompts with no identity descriptor perpetuate North American norms of objects’ appearances: these neutral prompts are typically extremely similar to images generated from prompts with“North America” (top row). These are most different from prompts with “Africa” (bottom row), which encode harmful stereotypes of poverty. We present two random examples foreach prompt.from this list, for each object, we used Stable Diffusion to generate 100 images using the following prompts: (1) A general prompt(“a photo of [OBJECT]”) (2) A North-America-specific prompt (“photo of [OBJECT] in North America”) (3) An Asia-specific prompt(“a photo of [OBJECT] in Asia.”) and (4) An Africa-specific prompt(“a photo of [OBJECT] in Africa”).2 We present random example in Figure 5. We see that seemingly neutral prompts about objects produce decisively culture-specific images: the general prompt produces outputs that are most similar to the North-America-specific prompt, while visually differing most greatly from outputs with theAfrica-specific prompt.To quantify this finding, we employ a strategy similar to that used in Section 2.2. We again study the representations in CLIP [56],the core representational component of Stable Diffusion. For each of the continent-specific prompts (e.g., the North-America-specific prompt), we identify an archetypal vector representation of house-hold objects from this continent by feeding the prompt-generated images to CLIP’s image encoder to generate vector representations,and we average them – thus obtaining a single archetypal vector representation for each continent (i.e., a vector for North America, vector for Asia, and a vector for Africa). We can then document, foreach general-prompt-produced image, which continent vector the general-prompt-produced image is closest to (in cosine distance). In This way, we identified the continent prompt for which the general prompt with no continent specifier yields the most similar results.Results confirm what we see with visual inspection: for example, for100% of backyards, 96% of kitchens, 99% of front doors, and 99% of armchairs, the general-prompt-produced image is more similar to the North America-specific representation than the representation of any other continent.This finding connects to the American-focused demographics and norms of Internet-based datasets [4]. Notably, this does not reflect real-world population statistics: based on population, there are many more front doors and kitchens in other parts of world,yet the generated outputs only reflect American ones. By gener-ating images that are stylized as American when prompted with everyday objects, these models create a version of the world that further entrenches the view of American as default. This “view from nowhere,” i.e. hiding a specific perspective and set of assumptions under the guise of neutrality, has been long-studied and criticized by sociologists for contributing to the exclusion and ostracizing of those who do not belong to the default group [27, 58].",1499-1500,"We see that seemingly neutral prompts about objects produce decisively culture-specific images: the general prompt produces outputs that are most similar to the North-America-specific prompt, while visually differing most greatly from outputs with theAfrica-specific prompt.",
11,AmericaCentricity,Causal_Theory,"One might hope that by generating images solely of non-human entities, we avoid reproducing representations of demographic groups and other problematic biases. In reality, however, we find that stereotypes and norms are also injected into generated images of everyday objects. Building upon the findings of Wolfe et al.[72] that visual semantic models like CLIP reproduce American racial hierarchies, we explore whether generated images from Stable Diffusion encode American norms. In prior work, De Vries et al.[18] test object recognition systems on a dataset of 117 classes of household objects, such as beds, doors, etc. and find that they work poorly on images from low-income countries. Using a set of objectsFigure 5: Generated images of everyday objects encode persis-tent stereotypes. The images generated from prompts with no identity descriptor perpetuate North American norms of objects’ appearances: these neutral prompts are typically extremely similar to images generated from prompts with“North America” (top row). These are most different from prompts with “Africa” (bottom row), which encode harmful stereotypes of poverty. We present two random examples foreach prompt.from this list, for each object, we used Stable Diffusion to generate 100 images using the following prompts: (1) A general prompt(“a photo of [OBJECT]”) (2) A North-America-specific prompt (“photo of [OBJECT] in North America”) (3) An Asia-specific prompt(“a photo of [OBJECT] in Asia.”) and (4) An Africa-specific prompt(“a photo of [OBJECT] in Africa”).2 We present random example in Figure 5. We see that seemingly neutral prompts about objects produce decisively culture-specific images: the general prompt produces outputs that are most similar to the North-America-specific prompt, while visually differing most greatly from outputs with theAfrica-specific prompt.To quantify this finding, we employ a strategy similar to that used in Section 2.2. We again study the representations in CLIP [56],the core representational component of Stable Diffusion. For each of the continent-specific prompts (e.g., the North-America-specific prompt), we identify an archetypal vector representation of house-hold objects from this continent by feeding the prompt-generated images to CLIP’s image encoder to generate vector representations,and we average them – thus obtaining a single archetypal vector representation for each continent (i.e., a vector for North America, vector for Asia, and a vector for Africa). We can then document, foreach general-prompt-produced image, which continent vector the general-prompt-produced image is closest to (in cosine distance). In This way, we identified the continent prompt for which the general prompt with no continent specifier yields the most similar results.Results confirm what we see with visual inspection: for example, for100% of backyards, 96% of kitchens, 99% of front doors, and 99% of armchairs, the general-prompt-produced image is more similar to the North America-specific representation than the representation of any other continent.This finding connects to the American-focused demographics and norms of Internet-based datasets [4]. Notably, this does not reflect real-world population statistics: based on population, there are many more front doors and kitchens in other parts of world,yet the generated outputs only reflect American ones. By gener-ating images that are stylized as American when prompted with everyday objects, these models create a version of the world that further entrenches the view of American as default. This “view from nowhere,” i.e. hiding a specific perspective and set of assumptions under the guise of neutrality, has been long-studied and criticized by sociologists for contributing to the exclusion and ostracizing of those who do not belong to the default group [27, 58].",1499-1500,the general-prompt-produced image is more similar to the North America-specific representation than the representation of any other continent.,
12, , , , , ,
13,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
14,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
15,GeographicBias,constrainsAgent,StableDiffusion,"One might hope that by generating images solely of non-human entities, we avoid reproducing representations of demographic groups and other problematic biases. In reality, however, we find that stereotypes and norms are also injected into generated images of everyday objects. Building upon the findings of Wolfe et al.[72] that visual semantic models like CLIP reproduce American racial hierarchies, we explore whether generated images from Stable Diffusion encode American norms. In prior work, De Vries et al.[18] test object recognition systems on a dataset of 117 classes of household objects, such as beds, doors, etc. and find that they work poorly on images from low-income countries. Using a set of objectsFigure 5: Generated images of everyday objects encode persis-tent stereotypes. The images generated from prompts with no identity descriptor perpetuate North American norms of objects’ appearances: these neutral prompts are typically extremely similar to images generated from prompts with“North America” (top row). These are most different from prompts with “Africa” (bottom row), which encode harmful stereotypes of poverty. We present two random examples foreach prompt.from this list, for each object, we used Stable Diffusion to generate 100 images using the following prompts: (1) A general prompt(“a photo of [OBJECT]”) (2) A North-America-specific prompt (“photo of [OBJECT] in North America”) (3) An Asia-specific prompt(“a photo of [OBJECT] in Asia.”) and (4) An Africa-specific prompt(“a photo of [OBJECT] in Africa”).2 We present random example in Figure 5. We see that seemingly neutral prompts about objects produce decisively culture-specific images: the general prompt produces outputs that are most similar to the North-America-specific prompt, while visually differing most greatly from outputs with theAfrica-specific prompt.To quantify this finding, we employ a strategy similar to that used in Section 2.2. We again study the representations in CLIP [56],the core representational component of Stable Diffusion. For each of the continent-specific prompts (e.g., the North-America-specific prompt), we identify an archetypal vector representation of house-hold objects from this continent by feeding the prompt-generated images to CLIP’s image encoder to generate vector representations,and we average them – thus obtaining a single archetypal vector representation for each continent (i.e., a vector for North America, vector for Asia, and a vector for Africa). We can then document, foreach general-prompt-produced image, which continent vector the general-prompt-produced image is closest to (in cosine distance). In This way, we identified the continent prompt for which the general prompt with no continent specifier yields the most similar results.Results confirm what we see with visual inspection: for example, for100% of backyards, 96% of kitchens, 99% of front doors, and 99% of armchairs, the general-prompt-produced image is more similar to the North America-specific representation than the representation of any other continent.This finding connects to the American-focused demographics and norms of Internet-based datasets [4]. Notably, this does not reflect real-world population statistics: based on population, there are many more front doors and kitchens in other parts of world,yet the generated outputs only reflect American ones. By gener-ating images that are stylized as American when prompted with everyday objects, these models create a version of the world that further entrenches the view of American as default. This “view from nowhere,” i.e. hiding a specific perspective and set of assumptions under the guise of neutrality, has been long-studied and criticized by sociologists for contributing to the exclusion and ostracizing of those who do not belong to the default group [27, 58].",1499-1500,"Stable Diffusion [...] We see that seemingly neutral prompts about objects produce decisively culture-specific images: the general prompt produces outputs that are most similar to the North-America-specific prompt, while visually differing most greatly from outputs with theAfrica-specific prompt."
16,AmericaCentricity,constrainsAgent,StableDiffusion,"One might hope that by generating images solely of non-human entities, we avoid reproducing representations of demographic groups and other problematic biases. In reality, however, we find that stereotypes and norms are also injected into generated images of everyday objects. Building upon the findings of Wolfe et al.[72] that visual semantic models like CLIP reproduce American racial hierarchies, we explore whether generated images from Stable Diffusion encode American norms. In prior work, De Vries et al.[18] test object recognition systems on a dataset of 117 classes of household objects, such as beds, doors, etc. and find that they work poorly on images from low-income countries. Using a set of objectsFigure 5: Generated images of everyday objects encode persis-tent stereotypes. The images generated from prompts with no identity descriptor perpetuate North American norms of objects’ appearances: these neutral prompts are typically extremely similar to images generated from prompts with“North America” (top row). These are most different from prompts with “Africa” (bottom row), which encode harmful stereotypes of poverty. We present two random examples foreach prompt.from this list, for each object, we used Stable Diffusion to generate 100 images using the following prompts: (1) A general prompt(“a photo of [OBJECT]”) (2) A North-America-specific prompt (“photo of [OBJECT] in North America”) (3) An Asia-specific prompt(“a photo of [OBJECT] in Asia.”) and (4) An Africa-specific prompt(“a photo of [OBJECT] in Africa”).2 We present random example in Figure 5. We see that seemingly neutral prompts about objects produce decisively culture-specific images: the general prompt produces outputs that are most similar to the North-America-specific prompt, while visually differing most greatly from outputs with theAfrica-specific prompt.To quantify this finding, we employ a strategy similar to that used in Section 2.2. We again study the representations in CLIP [56],the core representational component of Stable Diffusion. For each of the continent-specific prompts (e.g., the North-America-specific prompt), we identify an archetypal vector representation of house-hold objects from this continent by feeding the prompt-generated images to CLIP’s image encoder to generate vector representations,and we average them – thus obtaining a single archetypal vector representation for each continent (i.e., a vector for North America, vector for Asia, and a vector for Africa). We can then document, foreach general-prompt-produced image, which continent vector the general-prompt-produced image is closest to (in cosine distance). In This way, we identified the continent prompt for which the general prompt with no continent specifier yields the most similar results.Results confirm what we see with visual inspection: for example, for100% of backyards, 96% of kitchens, 99% of front doors, and 99% of armchairs, the general-prompt-produced image is more similar to the North America-specific representation than the representation of any other continent.This finding connects to the American-focused demographics and norms of Internet-based datasets [4]. Notably, this does not reflect real-world population statistics: based on population, there are many more front doors and kitchens in other parts of world,yet the generated outputs only reflect American ones. By gener-ating images that are stylized as American when prompted with everyday objects, these models create a version of the world that further entrenches the view of American as default. This “view from nowhere,” i.e. hiding a specific perspective and set of assumptions under the guise of neutrality, has been long-studied and criticized by sociologists for contributing to the exclusion and ostracizing of those who do not belong to the default group [27, 58].",1499-1500,"for 100% of backyards, 96% of kitchens, 99% of front doors, and 99% of armchairs, the general-prompt-produced image is more similar to the North America-specific representation than the representation of any other continent."
