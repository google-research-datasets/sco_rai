,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{xiao2023inthename,
    author = {Yuxin Xiao and Shulammite Lim and Tom Joseph Pollard and Marzyeh Ghassemi},
    title = {In the Name of Fairness: Assessing the Bias in Clinical Record De-identification},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-19, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,BiasInNameDeIdentification,Perceived_Problem,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records,
10,Researchers,Agent,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,we,
11,RecallEqualityDifference,Perceived_Problem,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. ",
12,LowBias,Goal,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,very low bias,
13,LowAsianRecall,Perceived_Problem,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"very low recall in the Asian Racial group, ",
14,HigherAsianRecall,Goal,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,in the Asian Racial group [...] scores much higher,
15,RacialBias,Causal_Theory,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups,
16,NameSets,Artifact,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,16 name sets with diverse demographic settings,
17,FindingOfBias,Artifact,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,find that a majority of the examined methods demonstrate statistically significant differences in performance along most of the four demographic dimensions.,
18,NeuroNER,Agent,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,NeuroNER,
19,Philter,Agent,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,the rule-based Philter,
20,Stanza,Agent,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,Stanza,
21,MIST,Agent,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,MIST,
22,CommercialServices,Agent,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"The three commercial services—Amazon, Microsoft, and Google",
23, , , , , ,
24,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
25,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
26,BiasInNameDeIdentification,constrainsAgent,Researchers,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records
27,Researchers,hasProducedArtifact,NameSets,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings"
28,Researchers,hasProducedArtifact,FindingOfBias,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,We find that a majority of the examined methods demonstrate statistically significant differences in performance along most of the four demographic dimensions.
29,RecallEqualityDifference,constrainsAgent,NeuroNER,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. "
30,LowBias,constrainsAgent,Philter,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,We note that the rule-based Philter has very low bias
31,LowAsianRecall,constrainsAgent,Stanza,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"Stanza and NeuroNER attain very low recall in the Asian Racial group, "
32,LowAsianRecall,constrainsAgent,NeuroNER,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"NeuroNER attain very low recall in the Asian Racial group, "
33,HigherAsianRecall,constrainsAgent,MIST,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"in the Asian Racial group, while MIST scores much higher"
34,RacialBias,constrainsAgent,CommercialServices,"In this paper, we focus on assessing the bias in de-identifying a specific type of PHI data—people’s names—from clinical records. We choose names amongst the defined PHI types because they are commonly associated with specific demographic features and are particularly identifiable. As illustrated in Figure 1, we first identify (a) four demographic dimensions (i.e., gender, race, name popularity, and the decade of popularity) and prepare (b) 16 name sets with diverse demographic settings in Table 1. Each name set consists of 20 first and 20 last names, which can be paired to produce 400 full names in total.We then curate (c) 100 clinical templates from hospital discharge records [80]. For each name set, we duplicate each of the 100 tem-plates ten times and fill in full names randomly generated from thatname set. This creates a total of (d) 16,000 notes with 116,160 namementions for evaluation. We use these notes to conduct a large-scale empirical analysis of (e) nine de-identification baseline methods to inspect the bias along the four demographic dimensions. To measure the demographic information associated with a name,we define the following four demographic dimensions.• The gender of a name refers to the sex assigned at birth to someone with that name, because the phonological property of a name suggests the associated gender [35]. We examine two groups for gender: male and female.• The race of a name refers to the expected racial or ethnic identity of someone with that name, reflecting the varia-tion in prevalence that exists between different self-reported racial or ethnic groups [62]. We consider four racial or ethnic groups: White, Black, Asian, and Hispanic. Other groups are skipped due to prohibitively small community sizes.• The popularity of a name refers to the size of the population of a gender within a decade having that name. We compare three groups here: top, medium, and bottom popularity.• The decade of popularity refers to the decade in which anime is popular in the U.S. in terms of babies being given the name, as name trends change over time [58]. We assess three decade groups: 2000s, 1970s, and 1940s.Limitations of Standardized Demographic Categories. We acknowl-edge the limitation of using standardized self-reported racial cate-gorization and binary gender groups when composing the namesets. More fine-grained racial categorizations are possible in future work, and there could be variety in the linguistic norms and naming traditions even within each racial group we consider. Transgenderand non-binary gender groups are also important to consider in future work, as these groups may use gender-neutral names or have variations in name usage between records.We use standardized self-reported racial categorization and bi-nary gender groups because it is important to evaluate the per-formance of de-identification methods on data that is routinely collected in EHRs [21]. We emphasize that we do not perform any demographic inference as part of a classification system or training set in this work. We do not believe that these categories should be viewed as scientific truth and recognize the larger critical interroga-tion surrounding whether gender and ethnicity should be discerned from names in such systems [84]. Instead, we use these categories in the spirit in which they were created by the U.S. Office of Man-agement and Budget to “monitor and redress social inequality” [25].The examination of the impact of more fluid categorizations of gender, race, and religion is important for future work in this space. In this study, we compute the popularity of first names for each gender based on the U.S. Social Security dataset [6] across the entire population, rather than for each racial group. We then select names that are primarily associated with a self-identified racial group with a margin over 10% based on the mortgage application dataset in[126]. We note that this is different from picking the most popular names for each racial group independently.In the U.S. setting, all top popularity names, as evaluated by absolute frequency ranking, are identified with the White racial group. For this reason, we consider names associated with theBlack, Asian, or Hispanic groups that are of medium popularity.First names of medium popularity for each race and gender (i.e.,Name Sets 3, 4, 7, 8, 9, 10, 11, and 12) are randomly sampled from those with a frequency ranking between 400 and 8,000 in the entire population in the 2000s. First names of bottom popularity for theWhite group (i.e., Name Sets 5 and 6) are randomly sampled from those occurring exactly five times in the 2000s. We set each nameset to 20 names since based on the procedure described above, there are only 20 names that are of medium popularity in the 2000s and primarily used by Black males. We also ensure that first names of top popularity within each gender and decade are mutually exclusive(i.e., no shared first names in Name Sets 1, 2, 13, 14, 15, and 16).We prepare last names in a similar fashion based on the 2000 Census dataset alone [3], because we assume that the last name popularity is relatively fixed. Specifically, this means that the most popular last names for the White racial group in the 1970s and1940s are assigned to be the same as those in the 2000s.Limitations of the Datasets. We acknowledge that our datasets are limited to the U.S., and therefore, our findings need to be reproduced in other contexts with distinct name distributions. Furthermore,our use of the mortgage application dataset for self-reported racial matching is limited to those who have the financial security to apply for a loan. As we do not have access to other sources of names and self-reported races, we use the available data to demonstrate that—even in this presumably more privileged subset of the population—there are de-identification gaps. To evaluate model performance along each demographic dimension,we design experiments that control for other dimensions as follows.• We assess the impact of gender by pooling and comparing the results of male (i.e., 1, 3, 5, 7, 9, 11, 13, and 15) and femaleName Sets (i.e., 2, 4, 6, 8, 10, 12, and 16). Race, popularity,and decade of popularity all vary within these two groups.• We compare performance along race by pooling Name Sets3 and 4 for the White group, Name Sets 7 and 8 for the Black Group, Name Sets 9 and 10 for the Asian group, and NameSets 11 and 12 for the Hispanic group. These are the male and female names of medium popularity in the 2000s across the four racial groups. We examine the influence of popularity by forming and comparing names with varying levels of popularity within the White group, where top popularity is based on NameSets 1 and 2, medium popularity is based on Name Sets 3 and 4, and bottom popularity is based on Name Sets 5 and 6.• We evaluate the difference in performance among the three decade groups by comparing the male and female names of top popularity for the White group in each decade: NameSets 1 and 2 for the 2000s, Name Sets 13 and 14 for the 1970s,and Name Sets 15 and 16 for the 1940s. We manually curate 100 clinical note templates based on hospital discharge records from Beth Israel Lahey Health between 2017 and2019. We follow the HIPAA Safe Harbor provisions by marking the occurrence of names in the templates and replacing other PHIclasses with realistic, synthetic values. We note that our templates[80] are more complex than those used in existing benchmark datasets [87, 91, 94], with an average of 12,893 characters and 3.5unique names per template and each unique name appearing an average of 2.1 times per template. This design is more analogous to real-world de-identification challenges and more likely to expose flaws in less effective methods. In our large-scale empirical analysis, we examine nine popular de-identification methods of three different categories. For packages that offer multiple model options, we report the option with the highest performance in our experiments.2Three off-the-shelf NLP libraries with the NER function:• spaCy [64] (25.9k GitHub Stars) is widely adopted for indus-trial information extraction. We choose RoBERTa-base [82],which is pre-trained on a massive general-purpose corpus,as the backbone of its NER pipeline.• Stanza [105] (6.6k GitHub Stars) is a natural language anal-ysis package. We apply its 18-class NEW model variant based on the contextual string representations [16] and pre-trained on the OntoNotes corpus [130].• flair [15] (12.7k GitHub Stars) is a powerful NLP frame-work. We employ its large four-class NEW model variant built on XLM-R embeddings [39] and document-level fea-tures [111] and pre-trained on the CoNLL03 corpus [108].Three commercial services for PHI detection:• Amazon Comprehend Medical DetectPHI Operation [4] is aHIPAA-eligible NLP service. We segment input notes into pieces shorter than 20,000 characters, the maximum allowed input length, when making the API calls.• Microsoft Azure Cognitive Service for Language PHI Detec-tion [9] de-identified PHI information in unstructured texts.We divide notes into slices shorter than 5,120 characters to obey the input length threshold.• Google Cloud Data Loss Prevention De-identification API[1] inspects and redacts sensitive data intelligently. We select the outputs for the class PERSON_NAME and remove the titles before the recognized full names. We note that both Amazon Comprehend Medical DetectPHI Op-eration and Microsoft Azure Cognitive Service for Language PHIDetection are intended to be used for our specific case of free-text medical note de-identification. Google Cloud Data Loss PreventionDe-identification is intended for the general text. We use this service because other medically-focused services operated by Google do not operate on free-text notes. Specifically, Google Cloud HealthcareAPI for de-identification [2] only operates on FHIR JSON embed-dings and DICOM images, and Google Cloud Healthcare NaturalLanguage API [8] only recognizes medical knowledge categories.Three open-source de-identification toolkits:• NeuroNER [43, 44] (212 citations) is an NER tool based on the long short-term memory model [63]. We use the model pre-trained on the 2014 i2b2 de-identification corpus [119]with GloVe word embeddings [102] and collect the output for PATIENT and DOCTOR as the set of recognized names.• Philter (Protected Health Information filter) [97] (31 ci-tations) leverages the Python NLTK module and regularexpressions for rule-based de-identification.• MIST (MITRE Identification Scrubber Toolkit) [10] (156 ci-tations) is a suite of tools for identifying and redacting PHIin free-text medical records. We pre-train the model sup-plied by the Carafe engine, a conditional random field-based[76] sequence tagger, on the 2006 i2b2 de-identification cor-pus [127] as instructed and view the outputs for the classesPATIENT and DOCTOR as the set of recognized names. We find that a majority of the examined methods demonstrate sta-tistically significant differences in performance along most of the four demographic dimensions. Table 2 exhibits the recall equality difference and the hypothesis test results, where an asterisk next to a score indicates a statistically significant difference at the corre-sponding significance level. In particular, Amazon and Google give the highest recall equality difference for name popularity and the decade of popularity, respectively, which should be a call for ac-tion for these commercial services. Although NeuroNER delivers an overall competitive de-identification performance, its recall equal-ity difference is rather high, especially along the dimensions of race and popularity. We note that the rule-based Philter has very low bias and that flair achieves not only the highest recall but also relatively low recall equality differences along all four dimensions.At a fine-grained level, we plot in Figure 2 the recall of the demographic groups along each dimension by each method. Along The dimension of gender, all the methods score better or equally well for female names than male names. Nevertheless, these meth-ods act very differently in the four racial groups. More specifi-cally, Stanza and NeuroNER attain very low recall in the Asian Racial group, while MIST scores much higher. The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups.",126-31,"The three commercial services—Amazon, Microsoft, and Google—all perform better in the White and Hispanic racial groups than in the Black and Asian Racial groups"
