,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{blili-hamelin2023makingintelligence,
    author = {Borhane Blili-Hamelin and Leif Hancox-Li},
    title = {Making Intelligence: Ethical Values in IQ and ML Benchmarks},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-10, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,NoGeneralAIEvalTask,Perceived_Problem,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,"there is no such thing as a “general” task on which all AI systems should be evaluated,",
10,QuestionOfAppropriateness,Perceived_Problem,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,the question of curating a set of “appropriate” image labels,
11,Labels,Artifact,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,the act of labeling itself,
12,HELM,Artifact,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,HELM,
13,Benchmarks,Artifact,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,The tasks and metrics,
14,ModelEvaluation,Artifact,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,the benchmark,
15,Researchers,Agent,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,we,
16,ComputerVision,Agent,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,CV,
17,EthicalLadeness,Perceived_Problem,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,is contentious and ethically laden,
18,Complexity,Perceived_Problem,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,“complex and contested social constructs”.,
19,ArticulationOfTradeOffs,Perceived_Need,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,"better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”.",
20,HolisticDesiderata,Goal,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”.,
21,Incompleteness,Perceived_Problem,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,"the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark",
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,NoGeneralAIEvalTask,constrainsAgent,Researchers,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite."
26,QuestionOfAppropriateness,constrainsAgent,ComputerVision,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center.
27,Labels,reflectsPrecept,EthicalLadeness,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,"the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection"
28,HELM,reflectsPrecept,EthicalLadeness,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,"HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection."
29,Metrics,reflectsPrecept,Complexity,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,That tasks and metrics are “complex and contested social constructs”.
30,Benchmarks,reflectsPrecept,ArticulationOfTradeOffs,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,"The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”."
31,ModelEvaluation,reflectsPrecept,HolisticDesiderata,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”.
32,Benchmarks,reflectsPrecept,Incompleteness,"If we accept that there is no such thing as a “general” task on which all AI systems should be evaluated, then the question arises of which tasks we should use to evaluate AI. The space of possible tasks is at the very least extremely large and possibly infinite. It is not feasible to evaluate AI on all tasks. In the case of bench-marks like SuperGLUE, Raji et al. [89] argue that task selection is arbitrary and unsystematic: appearing motivated especially by “convenience” (such as by what is “easily available” and front of mind for those building the benchmark) and largely disconnected from domain knowledge about the general capability being purportedly benchmarked—e.g. of natural language understanding.These points parallel Anderson’s argument that even if we grant that there are value-neutral scientific facts, the choice of which of these facts are significant enough to seek out is value-laden [2]. Given limited scientific resources, we cannot possibly seek out all undiscovered facts out there. Choices about which truths and facts to pursue should be partly informed by their ethical, social, and political implications. For example, we should start asking questions like why a task like object recognition is prioritized in computer vision (CV) benchmarks. While there is a prima facie scientific reason for including object recognition as a task, that is still only one part of human vision, which includes many other abilities beyond object recognition. One potential downside of having object recognition as the paradigm CV task is that it puts the question of curating a set of “appropriate” image labels front-and-center. Thisleads to tensions when CV is applied to situations where the act of labeling itself is contentious and ethically laden: for example, when applied to automatic gender detection [94, 95]. Furthermore, social scientists have long recognized that the act of classification itself is moral and political [12]. HELM [63] explicitly embraces the value-laden character of benchmarks, and deliberately articulates a strategy for navigating what this implies in its approach to task selection. Rather than selecting tasks that are a proxy for putative progress towards amore general ability, HELM targets an evolving and revisable set of tasks illustrative of important and socially significant real-world use cases for large language models (“scenarios”), accompanied by a pluralistic set of metrics for measuring performance on those many scenarios. In recognizing the ethical and societal considerations that motivate their selection of tasks and metrics, the authors also explicitly acknowledge:(1) That tasks and metrics are “complex and contested social constructs”. (2) The importance of designing the benchmark in a way that helps better articulate the “explicit potential trade-offs” between the “desiderata” these constructs embody, including“help[ing] to ensure these desiderata are not treated as second-class citizens to accuracy”. (3) That the task of model evaluation should be about coming up with a holistic and plural set of desiderata that helps exam-ine whether the models evaluated are “socially beneficial systems”. 4) The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark. HELM is a “living benchmark” designed to “evolve according to the technology, applications, and social concerns”.In examining the limitations of their benchmark design, HELMhighlights another value-laden trade-off in task selection: between choosing tasks and metrics that enable straightforward ranking of models (e.g. through leaderboards built with “single-numbermetrics”) and choosing tasks and metrics that make ranking model performance in a decontextualized way more difficult. As the au-thors highlight, in some circumstances, benchmarks that enables straightforward rankings or aggregate scores can be appropriate:such as to “simplify decision-making”. [63] However, this is itself a choice always motivated by social, ethical, and political preferences.There is no value-neutral path to benchmarks with leaderboards.",276,"The importance of articulating the incompleteness of the task selection, and of foregrounding limitations that call for future work, alternative benchmarks, or revisions to the benchmark"
