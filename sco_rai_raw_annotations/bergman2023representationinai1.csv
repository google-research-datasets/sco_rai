,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{bergman2023representationinai,
    author = {A. Stevie Bergman and Lisa Anne Hendricks and Maribeth Rauh and Boxi Wu and William Agnew and Markus Kunesch and Isabella Duan and Iason Gabriel and William Isaac},
    title = {Representation in AI Evaluations},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-08, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Practitioners,Agent,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,particitioners,
10,StandardGroupings,Artifact,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location",
11,CensusCategories,Causal_Theory,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,census categories,
12,DiscrimiantionLaw,Causal_Theory,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,discrimination law,
13,Exclusion,Perceived_Problem,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,exclude many relevant groups,
14,CulturalNarrowness,Perceived_Problem,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"break down in cultural contexts outside the US, Canada, and Europe",
15,CulturalConversationsData,Artifact,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,The Casual Conversations dataset,
16,Participants,Agent,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,participants,
17,Authors,Agent,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,The authors,
18,Balance,Strategy,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender)",
19,DeepFakeDetection,Goal,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,DeepFake detection,
20,WiderEmployment,Goal,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,allow it to be employed more widely,
21,UrgeForGreaterUnderstanding,Artifact,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"we urge practitioners to look beyond them, and seek greater understanding of the data subject",
22,Researchers,Agent,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,we,
23,UnderstandGroupExpression,Strategy,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory techniques",
24, , , , , ,
25, , , , , ,
26,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
27,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
28,Practitioners,hasProducedArtifact,StandardGroupings,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"practitioners [...] groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location"
29,Participants,hasProducedArtifact,CulturalConversationsData,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera"
30,Authors,hasProducedArtifact,CulturalConversationsData,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors"
31,Researchers,hasProducedArtifact,UrgeForGreaterUnderstanding,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"we urge practitioners to look beyond them, and seek greater understanding of the data subjects"
32,CensusCategories,constrainsAgent,Practitioners,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories"
33,DiscrimiantionLaw,constrainsAgent,Practitioners,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law"
34,Exclusion,constrainsAgent,Practitioners,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,yet likely exclude many relevant groups
35,CulturalNarrowness,constrainsAgent,Practitioners,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe"
36,Balance,constrainsAgent,Authors,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling."
37,DeepFakeDetection,constrainsAgent,Authors,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,The authors [...] initial domain of application for the dataset is DeepFake detection.
38,WiderEmployment,constrainsAgent,Authors,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"The authors [...] However, in releasing the dataset the aim is to allow it to be employed more widely"
39,UnderstandGroupExpression,constrainsAgent,Practitioners,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques"
40,StandardGroupings,reflectsPrecept,CensusCategories,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law"
41,CulturalConversationsData,reflectsPrecept,DeepFakeDetection,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection."
42,CulturalConversationsData,reflectsPrecept,WiderEmployment,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely,"
43,UrgeForGreaterUnderstanding,reflectsPrecept,UnderstandGroupExpression,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques"
44,StandardGroupings,influencesPrecept,Exclusion,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups,"
45,StandardGroupings,influencesPrecept,CulturalNarrowness,"Several complex questions come to the foreground when considering groups, first-and-foremost being group selection. What are the relevant groups, and how can their rele-vance be determined? One approach in group selection is to choose among groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe [124]. An example of this practice in the field is the Casual Conver-sations dataset [66]. The Casual Conversations dataset contains video clips with sound, where paid participants carry out various common actions such as waving to the camera. The authors made an effort to balance, and clearly report on, the amount of examples for each of the chosen groups (e.g. self-identified, binary gender) in their sampling. The initial domain of application for the dataset is DeepFake detection. However, in releasing the dataset the aim is to allow it to be employed more widely, particularly for the purpose of rooting out fairness issues in models [66]. The follow-up iteration of Casual Conversations includes a nuanced, in-depth consideration of groupings [65]. Good practice for group selection will naturally be dependent on the context and goals of the technology in question [63, 118], e.g. the common demographic groupings in India are different from the standard groupings based off the sociopolitical context of theUnited States [124]. Further, the communities included in model evaluations tends to reflect who practitioners had in mind while building the system. It may be implicit, yet these are the commu-nities prioritised for the accrual of the benefits of the model, and limitation of harms [17, 70, 115, 118]. While the standard groupings are often not a bad place to start, we urge practitioners to look beyond them, and seek greater understanding of the data subjects, the impacted or relevant groups,and the groups that may have an intrinsic right to be included in the evaluation (i.e. have the right to have their experiences, views,welfare taken into consideration). An important first step would be to understand how groups might express differently in the do-main being modeled, for example via in-depth research, subject matter and experiential expert consultation, and participatory tech-niques [16, 21, 31, 88, 92]. For evaluating an AI system, what is of utmost importance to consider is: who will be affected by the model? Who must this model be high performing for? There is a growing literature on impact statements and ethical foresight that can guide practitioners’ efforts to directly consider relevant groups [99, 101, 102, 110].",523,"groupings that have become standard in the AI fairness field, e.g. age, gender, race, language, and geo-location (e.g. [4, 29, 66, 78]).These groupings are generally drawn from census categories or discrimination law [9] yet likely exclude many relevant groups, and can break down in cultural contexts outside the US, Canada, and Europe"
