,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{bianchi2023, author = {Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and Ladhak, Faisal and Cheng, Myra and Nozza, Debora and Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and Caliskan, Aylin}, title = {Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,StereotypAmplication,Perceived_Problem,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,"amplify this skew, representing software developers with nearly exclusively stereotypically white male features",
10,StableDiffusion,Agent,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,the model,
11,MaleOccupationalStereotypeAmplification,Causal_Theory,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics.,
12,FemaleOccupationalStereotypeAmplification,Causal_Theory,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,"house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female.",
13,IncomeBasedStereotypeAmplification,Causal_Theory,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,"jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics",
14,Images,Artifact,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,100 images,
15,PerpetuationOfPrestigeAndWhiteness,Artifact,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,perpetuates societal notions of prestige and whiteness,
16,ContributorsToUnequalLifeOutcomes,Artifact,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,adversely impacts life outcomes and opportunities for minority groups.,
17, , , , , ,
18,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
19,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
20,StereotypAmplication,constrainsAgent,StableDiffusion,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,"Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification."
21,StableDiffusion,hasProducedArtifact,Images,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,the model was used to generate 100 images
22,MaleOccupationalStereotypeAmplification,constrainsAgent,StableDiffusion,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics.
23,FemaleOccupationalStereotypeAmplification,constrainsAgent,StableDiffusion,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,"Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female."
24,IncomeBasedStereotypeAmplification,constrainsAgent,StableDiffusion,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,"we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics"
25,StableDiffusion,hasProducedArtifact,PerpetuationOfPrestigeAndWhiteness,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,"This pattern highlights that the phenomenon of stereotype amplification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances."
26,StableDiffusion,hasProducedArtifact,ContributorsToUnequalLifeOutcomes,"Given the many cases of model-generated images perpetuating stereotypes, we next turn our attention to quantifying the potential for stereotype amplification. Stereotype amplification is the process of real-world correlations between social identities like race and gender and social roles becoming distorted and exaggerated, possi-bly to the point of being perceived as ubiquitous [55]. Prior work has demonstrated that previous language models and word embed-dings can amplify biases in general, and stereotypes in particular,beyond rates found in the training data or the real world [23, 77].Given the foundational training dataset was constructed in theU.S., and given observations that machine learning representations reflect American norms and values and reproduce inequities of American society (Section 2.3, [31, 72, 73]), we focus on quantifying the extent of amplification of U.S. social associations. Further, we focus on the association between racial and gender categories and occupation, because in the U.S., race and gender are pervasive conceptualized as core demographic categories used socially and recorded by the census bureau, and national surveys quantify oc-cupation demographics in terms of these categories [66]. We are interested in the extent to which the U.S. ‘official’ demographic categorizations (Male/Female, White/Black/Asian) and associated occupations are perpetuated in the Stable Diffusion model and gen-erated images. (We further describe our use and the broader social context of these social categories in the Appendix.) For example, historical forces have shaped who becomes software developers, and this group reportedly self-identifies as majority white men. Given a prompt referring to a software developer, does the model lessen this skew and generate diverse skin tones and features, reflect this skew, or amplify this skew, representing software developers with nearly exclusively stereotypically white male features? We uncover many instances of the latter: near-total stereotype amplification. Based on the availability of data from the U.S. Bureau of LaborStatistics, we present ten occupations, of which several have been studied in prior work on biases in natural images, e.g. software developer and housekeeper [33, 68, 76], and others reportedly have substantial demographic imbalances [66]. First, we generate image of each occupation, and then we analyze the way the model repre-sents these images: For each occupation, the prompt “A photo of the face of [OCCUPATION]” (e.g. “A photo of the face of a house-keeper”) was fed to the model, the model was used to generate 100 mages, and the occupation and a random sample of the generated images are presented in Figure 3.2We now wish to quantitatively assess the extent to which the model represents each occupation as tied to a particular gender or race category. To do so, we study the representations in ContrastiveLanguage–Image Pre-training (CLIP) [56], which is the core rep-resentational component of Stable Diffusion. CLIP represents all images in a joint visual semantic space. For each of the U.S. 'officials two gender and three race categories (Male, Female, White, Black, Asian), we identify an archetypal vector representation of the demographic category as follows: First, we take the corresponding slice of images from the Chicago Face Dataset, a dataset of faces with self-identified gender and race [41] (e.g. the slice of images self-identified as White, the slice of images self-identified as Black, etc).Then, we feed this slice of images to CLIP’s image encoder to gener-ate vector representations, and we average them — thus obtaining a single archetypal vector representation of the demographic cat-egory (e.g., a vector for White, a vector for Black, so on and so forth). We now simply say that the model represents a generated occupation image as a particular demographic category (e.g. the model represents an image of a software developer as white) when the model representation of the image is most closely aligned (cosine distance) to the representation of this demographic category(e.g. White), not the alternatives (e.g. Black or Asian). We present additional details and context for this method in the Appendix. InFigure 4, we present our findings.We find that simple prompts that mention occupations and make no mention of gender or race can nonetheless lead the model to re-inforce occupational stereotypes. Model representations generated from seemingly neutral queries have gender and racial imbalances beyond nationally reported statistics [66] (Figure 4) and generate stereotypically raced and gendered features (Figure 3). Many occu-pations exhibit stereotype amplification: software developer and chef are strongly skewed towards male representations at proportions far larger than the reported statistics. Other queries, like house-keeper, nurse, and flight attendant, exhibit total amplification: foreach of these occupations, 100% of the generated images were rep-resented as female. Moreover, the generations are not only more imbalanced compared to U.S. labor statistics: the extent of amplifica-tion is unevenly distributed, in ways that compound existing social hierarchies. In Figure 4, we see that jobs with higher incomes like software developer and pilot skew more heavily toward white, male representations, while jobs with lower incomes like housekeeper are represented as more non-white and female than the national statistics. Notably, whereas cooks and chefs are both food prepara-tion occupations, chefs tend to be viewed as in a more prestigious role and make nearly double the mean annual income in the U.S.[67]. Although the percentage of cooks that self-identify as white is greater than the percentage of chefs that self-identify as white,the model nonetheless suppresses white cooks and non-white chefs and ultimately represents the majority of cook images as non-whiteand the majority of chef images as white.This pattern highlights that the phenomenon of stereotype am-plification perpetuates societal notions of prestige and whiteness,rather than merely amplifying existing demographic imbalances. Algorithmic amplification of associations between gender and race and occupations, and particularly the erasure of minority and his-torically disadvantaged groups from prestigious occupations, exac-erbates existing inequities and results in allocational and represen-tational harms [12, 17]. On one end, allocational harms may occur through stereotype threat, i.e. one’s performance being affected by the thought of confirming negative stereotypes about one’s own identity. For instance, in one study, African-American students did more poorly on exams under the pressure of racial stereotypes about test performance [64]. On the other end, allocational harm occur through direct stereotype influence; i.e. allocation of bene-fits being substantially determined by pervasive stereotypes. The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups. Many disciplines have raised concerns about this phenomenon and asserted a moral obligation to avoid exacerbating the existing injustices that disproportionately affect marginalized communities [28]. ",1497-9,The generated images’ enforcement of associations between dominant groups and higher status roles adversely impacts life outcomes and opportunities for minority groups.
