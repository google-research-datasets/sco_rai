,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{bianchi2023, author = {Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and Ladhak, Faisal and Cheng, Myra and Nozza, Debora and Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and Caliskan, Aylin}, title = {Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,StableDiffusion,Agent,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,Stable Diffusion,
10,Images,Artifact,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,100 images.,
11,Bias,Perceived_Problem,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups,
12,AntifeminineBias,Causal_Theory,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,ties emotionality and seductiveness specifically to stereotypically white feminine features.,
13,WhiteCentricNotionsOfAttractiveness,Causal_Theory,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,defines attractive-ness as near the “White ideal”,
14,AntifeminineBias,Causal_Theory,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,a person cleaning generates only faces with stereotypically feminine features. ,
15,Exoticization,Causal_Theory,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hai",
16,AntiBlackStereotypes,Causal_Theory,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived as Black men",
17,AntiMuslimStereotype,Causal_Theory,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,consistent with the American narrative that terrorists are brown bearded Middle Eastern men,
18,AntiLatinAmericanStereotype,Causal_Theory,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"the American Concept of “‘illegal’ Latin American immigrants [11, 21].",
19,Heteronormativity,Causal_Theory,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"reinforce heteronormativesocial institutions,",
20, , , , , ,
21,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
22,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
23,StableDiffusion,hasProducedArtifact,Images,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,Stable Diffusion to generate 100 images.
24,Images,reflectsPrecept,Stereotypes,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups
25,Bias,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups
26,AntifeminineBias,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,Stable Diffusion model [...] ties emotionality and seductiveness specifically to stereotypically white feminine features.
27,WhiteCentricNotionsOfAttractiveness,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,Stable Diffusion model defines attractive-ness as near the “White ideal”
28,AntifeminineBias,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,we found that a person cleaning generates only faces with stereotypically feminine features.
29,Exoticization,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65]."
30,AntiBlackStereotypes,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived as Black men"
31,AntiMuslimStereotype,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men"
32,AntiLatinAmericanStereotype,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21]."
33,Heteronormativity,constrainsAgent,StableDiffusion,"We begin by investigating this question: can simple descriptions that do not reference race, gender, ethnicity, or nationality language nonetheless lead models to reproduce harmful stereotypes? We present ten cases confirming that the answer is unequivocally yes.For each of ten commonly-used human descriptors, the prompt “Photo of the face of [DESCRIPTOR]” (e.g. “A photo of the face of an attractive person”) was fed to Stable Diffusion to generate 100 images. Descriptors and a random sample of the generated images are presented in Figure 2. 2We find that the generated images reify many dangerous societal associations by tying descriptors to visual features that are stereotypically associated with specific socially-constructed demo-graphic groups [30]. The Stable Diffusion model defines attractive-ness as near the “White ideal” (blue eyes, pale skin, or long, straight hair; [32]) and ties emotionality and seductiveness specifically to stereotypically white feminine features. Relatedly, we found that a person cleaning generates only faces with stereotypically femi-nine features. Meanwhile, the model exoticizes people with darker skin tone, non-European adornment, and Afro-ethnic hair [65].The former positions femininity as subordinate relative to white masculinity [8], while simultaneously perpetuating the legacy of whiteness as the default ideal, further subordinating those who don't belong to the white monolith [42, 69]. The term “exotic” has along history of being used to refer to populations that have been deemed “uncivilized” by a dominant group [47] and continues to contribute to sexualization and exclusion [46].Further, we find a poor person and a thug generate faces with dark skin tone and features stereotypically associated with Blackness[35], and a person stealing similarly generates faces with dark skin and stereotypically Black features, perpetuating patterns in media that are known to invoke anxiety, hostile behavior, criminalization,and increased endorsement of violence against people perceived asBlack men [9, 25, 49, 63]. Prompting the Stable Diffusion model to generate a terrorist results in brown faces with dark hair and beards,consistent with the American narrative that terrorists are brown bearded Middle Eastern men, justifying bans and violent policies against persons perceived as in this group [15, 16, 26]. Similarly, an illegal person generates brown faces, mirroring the American Concept of “‘illegal’ Latin American immigrants [11, 21].Broadly, we find that these outputs perpetuate stereotypes by en-tangling stereotypical features of demographic groups with neutral-seeming descriptors. Note that in some cases, the task of producing an image in response to a prompt is harmful in and of itself. For Instance, the very notion of generating an image of “the face of a poor person” is problematic in and of itself, as race, class, and other social categories are not immutable [61], and more broadly, images of particular characteristics that are or are meant to be uncorre-lated with visual attributes cannot be generated without making dangerous assumptions.Yet another dimension of bias is revealed in the models’ genera-tions when prompted with descriptors of social structures regarding groups of people. For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, which presume that marriage and family struc-tures are based on different-sex couples [36, 37]. These normative assumptions alienate those who do not conform to these norms, con-tributing to the well-documented phenomenon of minority stress:those with LGBTQ+ identities disproportionately experience stress and other mental health consequences as a result of homogenizing stereotypes, stigma, and discrimination [19, 45]. ",1495-6,"For prompts of “a happy couple” and “a happy fam-ily,” the straight-passing output images reinforce heteronormativesocial institutions, "
