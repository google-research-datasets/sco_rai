,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{venkatagiri2023diverseperspectives,
    author = {Sukrit Venkatagiri and Jacob Thebault-Spieker and Naomi Mine and Kurt Luther},
    title = {Diverse Perspectives Can Mitigate Political Bias in Crowdsourced Content Moderation},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-11, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Russia,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Russia’s Internet Research Agency (RU-IRA),
10,GrapplingWithPoliticalContent,Perceived_Problem,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,grapple with the political content on their platforms,
11,PoliticalAdBan,Strategy,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,ban all political advertising,
12,AuthorizationAndLabeling,Strategy,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,political ads needed to go through an authorization process and label all political ads,
13,PoliticalConversationBan,Strategy,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,policies disallowing conversations about national political campaigns in the US,
14,TargetingControls,Strategy,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,much coarser targeting controls for political advertising,
15,LinkedIn,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,LinkedIn,
16,Texas,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Texas,
17,Florida,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Florida,
18,Inefficiency,Perceived_Problem,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,"have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation)",
19,Efficiency,Perceived_Need,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,"have been shown to evaluate subjective, contextualized information more effectively",
20,Bias,Causal_Theory,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,inefficacy and systematic biases in judgment,
21,HeterogenityOfCrowds,Causal_Theory,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,the heterogeneity of crowd makeup,
22,DisinformationCampaign,Artifact,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,an associated disinformation campaign,
23,SocialMediaCompanies,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,social media companies,
24,Twitter,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Twitter,
25,Spotify,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Spotify,
26,Facebook,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Facebook,
27,Nextdoor,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Nextdoor,
28,GoogleAndYoutube,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Google and Youtube,
29,PoliticalContentHiding,Artifact,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,a feature allowing users to hide political content in their feeds ,
30,AntiCensorshipLaw,Artifact,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,"a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint.",
31,AutomatedModeration,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,automated approaches,
32,HumanModerators,Agent,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,incorporating human labor into these systems,
33, , , , , ,
34,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
35,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
36,Russia,hasProducedArtifact,DisinformationCampaign,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA)
37,GrapplingWithPoliticalContent,constrainsAgent,SocialMediaCompanies,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,social media companies to grapple with the political content on their platforms
38,PoliticalAdBan,constrainsAgent,Twitter,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Twitter would ban all political advertising
39,PoliticalAdBan,constrainsAgent,Spotify,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,would ban all political advertising [62] andSpotify followed suit in 2020
40,AuthorizationAndLabeling,constrainsAgent,Facebook,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Facebook enacted a policy in which accounts posting political ads needed to go through an authorization process and label all political ads
41,PoliticalConversationBan,constrainsAgent,Nextdoor,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Nextdoor Implemented policies disallowing conversations about national political campaigns in the US
42,TargetingControls,constrainsAgent,GoogleAndYoutube,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,Google and YouTube instituted much coarser targeting controls for political advertising
43,LinkedIn,hasProducedArtifact,PoliticalContentHiding,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,LinkedIn deployed a feature allowing users to hide political content in their feeds
44,Texas,hasProducedArtifact,AntiCensorshipLaw,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,"Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint."
45,Florida,hasProducedArtifact,AntiCensorshipLaw,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,"Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46]"
46,Inefficiency,constrainsAgent,AutomatedModeration,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,"automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation)"
47,Efficiency,constrainsAgent,HumanModerators,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,"human content moderators — who have been shown to evaluate subjective, contextualized information more effectively"
48,Bias,constrainsAgent,HumanModerators,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment
49,HeterogenityOfCrowds,constrainsAgent,HumanModerators,"In recent years, social media companies have begun to focus on the risks associated with political messaging on their platforms,including issues of fairness — or bias — and disinformation. The2016 US presidential election, and an associated disinformation campaign run by Russia’s Internet Research Agency (RU-IRA), was a catalyzing event that led social media companies to grapple with the political content on their platforms, study the role it plays in shaping public opinion, and begin thinking about imposing lim-itations [63]. For instance, in 2019, Twitter’s CEO, Jack Dorsey,announced that Twitter would ban all political advertising [62] andSpotify followed suit in 2020 [43]. In 2020, Facebook enacted a pol-icy in which accounts posting political ads needed to go through an authorization process and label all political ads [37], and Nextdoor Implemented policies disallowing conversations about national po-litical campaigns in the US [50]. Google and YouTube instituted much coarser targeting controls for political advertising [23] in2022, and LinkedIn deployed a feature allowing users to hide politi-cal content in their feeds [5].Beyond issues of disinformation, social media companies face growing concerns about perceived political biases on their plat-forms. One prominent example of these concerns was when Facebook decided to automatically curate, and then subsequently shut-ter, their Trending News feature due to critiques of political bias [18]. More recently, many states in the US have begun to explore legal avenues attempting to guarantee “fair” content moderation prac-tices [10]. For instance, in Texas, a law and subsequent court cases seek to ensure that a user of a social media system cannot be “censored because of their viewpoint”, predicated on the perception that some users are being censored because of their viewpoint. Similarly, Florida recently passed a law making it illegal to ban politicians accounts [46].Enforcement of political content policies, and concerns about(un)fair treatment of some political groups, creates an environment in which social media companies need to decide which content is political, and whether or not it should be allowed on their platform —in other words, to perform political content labeling and moderation.To achieve this efficiently and at scale, platforms often rely on algorithmic content labeling and moderation techniques. However, automated approaches have been shown to be insufficient when the content is highly subjective and contextual (e.g., hate speech or disinformation) [18]. Thus, human content moderators — who have been shown to evaluate subjective, contextualized information more effectively — are often hired to augment or help train algorithmic systems [24]. However, incorporating human labor into these systems brings its own risks: inefficacy and systematic biases in judgment. Even for experienced human moderators, identifying and labeling subjective content remains challenging [6], and, particularly in a polit-ically polarized social space, human content moderators may make unfair or biased decisions. Of particular concern to crowd labeling and evaluation is the risk of biases stemming from the composition of crowds [14], i.e., who comprises the crowd. In organizational psychology, prior work suggests that heterogeneous teams can be more effective at achieving their stated goals because a more diverse set of perspectives are included, but in other settings can actually hinder a teams’ effectiveness [30]. According to Duan et al. [20], it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] are avenues of possible bias. Given the public and legal interest in issues of social media and politics, and the potential for unfair content labeling and moder-ation, crowds’ effectiveness and fairly labeling political content becomes a high-stakes focal area for research.",1280-1,"it is unclear how the heterogeneity of crowd makeup might play out in political content labeling and moderation, as both approaches to aggregating crowd responses [36] and crowd composition [58] areavenues of possible bias."
