,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{weerts2023, author = {Weerts, Hilde and Xenidis, Rapha\""{e}le and Tarissan, Fabien and Olsen, Henrik Palmer and Pechenizkiy, Mykola},
title = {Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is Not a Decision Tree},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,RaceAndGenderUnfairness,Perceived_Problem,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,"failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men",
10,PoorFacialRecognitionPerformance,Perceived_Problem,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,"the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color.",
11,NetherlandsInstituteForHumanRights,Agent,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,Netherlands Institute for Human Rights,
12,FailureToEngageWithIntersectionality,Perceived_Problem,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,considering intersectional discrimination [...] failed to properly engage with this issue.,
13,FacialRecognitionSystems,Agent,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,several commercial facial recognition systems ,
14,DutchStudent,Agent,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,a Dutch student,
15,FindingOfEvidenceForDiscrimination,Artifact,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,"the disadvantage experienced by the student, together with scientific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race",
16,EUCourtofJustice,Agent,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,the CJEU,
17, , , , , ,
18,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
19,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
20,RaceAndGenderUnfairness,constrainsAgent,FacialRecognitionSystems,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,"several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men"
21,PoorFacialRecognitionPerformance,constrainsAgent,DutchStudent,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,"a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color."
22,NetherlandsInstituteForHumanRights,hasProducedArtifact,FindingOfEvidenceForDiscrimination,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,"Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with scientific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race"
23,FailureToEngageWithIntersectionality,constrainsAgent,EUCourtofJustice,"In their seminal work ""Gender Shades"", Buolamwini and Gebru [8]found that several commercial facial recognition systems intended to identify a person’s gender failed disproportionately for darker-skinned women, particularly compared to faces of lighter-skinned men. There are many reasons why the predictive performance of machine learning system differs across groups, including the use of features that are not equally predictive across groups and the use of a machine learning algorithm that is unable to adequately capture the data distributions of minority groups. In the case ofGender Shades, the primary culprit was the under-representation of darker-skinned women in facial recognition data sets. This typeof bias can be particularly problematic when the data distribution of majority groups differs substantially from the data distribution of minority groups.27 Again, we first need to consider whether the problem at stake falls within the material scope of EU discrimination law, which itself depends on the sector in which the facial recognition sys-tem is used. For example, if facial recognition is required to gain access to particular goods or services (with the exception of ad-vertisement, education and the media in relation to gender-based discrimination), disparate misclassification rates in relation to gen-der or skin color lead to denying access to protected groups fall within the material scope of Directives 2004/113/EC [40] and Di-rective 2000/43/EC [39].28 As race and gender are not used directly as input factors in the algorithms, a case like this might fall within the indirect discrimination doctrine.29 This would open up the possibility of an objective justification. For example, in 2022, a Dutch student filed a complaint against her university, stating that the face recognition check included in fraud detection software used during online exams, often failed– seemingly due to the student’s dark skin color. In an interim judgment, the Netherlands Institute for Human Rights states that the disadvantage experienced by the student, together with sci-entific research pointing towards disparate performance of face recognition algorithms, provide prima facie evidence for indirect discrimination in relation to race [35]30 and shifting the burden of proof to the university to prove the law was not violated. Furthermore, the case of facial recognition software provides an interesting case study for interrogating the boundaries of EU non-discrimination law. Would a particular disadvantage arising from the disparate quality of goods and services, for instance, face recognition, in relation to gender or race fall within the ban on discrimination? Arguably, there is a case for EU non-discrimination law in the area of goods and services to be applied to disparate product safety and performance across demographic groups. For Example, could the exclusive use of male crash dummies to test car be captured by the Gender Directive 2000/43/EC on goods and ser-vices, since it results in higher risks of injury for female occupants[68]? Even though the case law in this area is scarce and does not provide for immediate analogies (see e.g. C-236/09 Test-Achats [1]),the scholarship in this area points towards the applicability of EUnon-discrimination law [9, p. 94].31 In addition, the Court’s inclu-sion of the notion of ‘access’ within the scope of protection of EUlaw in Maniero, a case concerning the award of educational schol-arships, points towards the applicability of EU non-discrimination law to harms related to disparate quality of service. In that case,the Court indicated that ""there can be no education without the possibility to access it"" and that ""the directive’s objective, which is to combat discrimination in education, could not be achieved if discrimination were allowed at the access to education stage"" [27,para. 37]. In addition, the Advocate General in Maniero endorsed abroad interpretation of the notion of ‘access’: ""access to education has many component parts. It could be physical access to a build-ing; imposing a numerus clausus system to keep student numbers controlled; the ability to borrow or purchase books; the ability to pay for living expenses (amongst many others)"" [28, para. 33]. By Analogy, the disparate quality or performance of algorithmic sys-tems for protected groups could be understood as affecting their access to goods and services in a discriminatory manner. In such an extensive interpretation of non-discrimination guarantees, biased systems like the face recognition tools in our example could poten-tially fall under EU non-discrimination law regardless of whether they condition access to other goods and services.32 Finally, an important characteristic of the Gender Shades study was the emphasis on intersectional concerns: while facial recogni-tion systems generally performed worse for women and people of color, the disparity was the greatest for darker-skinned women. As Mentioned above, EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue. From these examples, we can see that EU non-discrimination law isin principle suited to deal with types of algorithmic unfairness that closely resemble human discrimination. However, reasoning by analogy to apply legal norms and principles to cases of algorithmic unfairness reveals gray areas and inconsistencies in the Court's Approach to discrimination. Some of these gaps could be filled via teleological interpretation of EU discrimination law in the digital context, for example in cases of disparate predictive performance,but this also opens up difficult normative questions. Moreover, the intelligibility of prediction-generating mechanisms and lack of transparency regarding important design choices of AI systems make it difficult for applicants to provide prima facie evidence to even start court proceedings. From a legal compliance perspective,since the CJEU rarely relies on statistical evidence in its judgments,it is difficult to derive general, abstract or readily transferable rules of thumb regarding requirements for thresholds, proportionality or justification from the highly particularized case law of the Court.",811-2,"EU law does not prevent the CJEU from considering intersectional discrimination, but the Court has so far failed to properly engage with this issue."
