,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{bianchi2023, author = {Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and Ladhak, Faisal and Cheng, Myra and Nozza, Debora and Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and Caliskan, Aylin}, title = {Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Chicago,Agent,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,Chicago,
10,USCensusBureau,Agent,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,US Census bureau,
11,Researchers,Agent,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,We,
12,AssessmentOfLPMethodology,Perceived_Problem,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al.,
13,LPMethodology,Agent,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,LP methodology,
14,SyntheticData,Artifact,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,we also experiment with synthetic data that mimics the 311 and census data sets.,
15,311Dataset,Artifact,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,"public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled.",
16,AmericanCommunitySurvey,Artifact,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,"data from the American CommunitySurvey [...] with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks.",
17,TighterViewOfUnfairness,Artifact,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,a tighter view of the set of all feasible levels of unfairness,
18,SmallerUnfairnessGap,Causal_Theory,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,does not exhibit as large of gap as the one observed in real data,
19, , , , , ,
20,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
21,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
22,Chicago,hasProducedArtifact,311Dataset,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,"Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled."
23,USCensusBureau,hasProducedArtifact,AmericanCommunitySurvey,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,"We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks."
24,Researchers,hasProducedArtifact,SyntheticData,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,"In order to additionally illustrate the methodology in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets."
25,AssessmentOfLPMethodology,constrainsAgent,Researchers,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al.
26,LPMethodology,hasProducedArtifact,TighterViewOfUnfairness,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,our LP gives a tighter view of the set of all feasible levels of unfairness
27,SyntheticData,reflectsPrecept,SmallerUnfairnessGap,"We now experimentally assess our LP methodology and compare it to bounds that can be obtained by using the closed-form equations developed by Kallus et al., with the aim of answering (Q3) of Sec.3.3. Our goal is to better understand how the bounds achieved by the LP formulation relate to the loose lower bound discussed in Prop. 4.2. We use the 311 data from the City of Chicago as our primary source [4] and the U.S. Census data as our secondary source [2]. Chicago’s public dataset of 311 requests contains, among other metadata, the location of each request and the amount of time it took for the request to be fulfilled. The Census dataset gives demographic information for different locations. Therefore, these two sources exemplify the kind of scenario that we used to motivate this work, and are ideal to empirically compare and contrast various approaches of auditing for fairness.5.1 DataPrimary Source. The 311 dataset [4] contains information on all311 requests since early 2018. The dataset lists the type of request(i.e., a request for snow removal v.s. a complaint about a downed tree), the “block group” that the request originated from (a block group is a geographic unit used by the Census bureau which, on average, encompasses several city blocks), and the time it took to fulfill the request. Since different request types have different av-erage response times, we first divide the 311 data set by requesttype. For request type ℓ, one can think of the 311 data as an ar-ray (𝑇 , 𝑍 )𝑛ℓ where 𝑛ℓ is the number of requests of type ℓ, 𝑇 ’s are the total processing time of each request, and 𝑍 ’s are the block group from which that request was issued. The requests are ini-tially ordered by the time each request was logged. However, as apre-processing step, we reordered them first according to 𝑍 , andthen within the same block group according to 𝑇 , so as to facilitate their representation as a linear program of the type we describe inSec. 4.2.Secondary Source. We used data from the American CommunitySurvey, a publicly available data product of the US Census bureau with detailed demographic information [2]. The dataset we used includes demographic information on every block group in Chicago,each of which, we reiterate, can encompass several city blocks. TheCensus dataset lists counts of the number of people who live in each block group by race and ethnicity. When running experiments on a given request type, blocks that have no corresponding request of that type are factored neither into the solution of our LP norinto the equations of [7]. Thus, such elements of 𝑍 are effectively dropped. Our experiments necessitated simplifying all of the racial and ethnic categories in the census data into a binary distinction,so we experimented with different splits, e.g., white non-Hispanic(𝐴 = 0) vs. the rest (𝐴 = 1).Synthetic Data. In order to additionally illustrate the methodol-ogy in a clean sandbox, we also experiment with synthetic data that mimics the 311 and census data sets. We do this by first generat-ing synthetic “census” data by drawing a Uniform[0.1, 0.9] random variable for each block, to determine the proportion of residents from the 𝐴 = 0 demographic. We then randomly assign each demographic in each block a random scale parameter, drawn again uniformly from two disjoint intervals on the real line, depending on whether 𝐴 = 0 or 𝐴 = 1, to influence the response time for the demographic 𝐴 in the block 𝑍 . This simulates bias with respect to demographics, as we use these scale parameters in exponential distributions to draw the random request times in each case. The Sampling thus proceeds as follows. To sample a request, we ran-domly assigned it a block according to a fixed block proportion 𝑃𝑍 .Then, we randomly choose a demographic 𝐴 from {0, 1} according to the synthetic census information. Then, we used the exponential distribution with the scale corresponding to the pair (𝐴, 𝑍 ) to sam-ple a response time for that request. Repeating these steps 𝑛 times generates the requests data. To get a large-scale picture of how our LP’s bounds relate to the loose lower bound, we first ran our LP and calculated the bounds of 7] on every request type ℓ with 𝑛ℓ < 10, 000 requests, for different demographic splits. For each request type and demographic split,we calculated the amount of our LP’s lower bound improvementon [7]’s lower bound, as the relative gain of the difference between[7]’s lower and upper bounds. We do this in order to judge the significance of the change across request types. Fig. 1 illustrates the histogram of these relative gains (higher is better), for two demographic splits. these histograms show that roughly half of the cases do not see much improvement. However, they also highlight that the other half sees a significant amount of accuracy improvement in assessingKS disparity, by narrowing the gap between the lower and upper bounds, as compared to the binary-decision approach of Kallus et al.[7].Next, we take closer look at a few special cases. We picked several request types and demographic splits for which the LP offered a significant improvement over the bounds obtainable from [7]. Forthese request type/demographic split pairs, we calculated [7]’s up-per and lower bounds on disparity for each relevant time threshold(each realized response time in the 311 data) and graphed the results, along with the aforementioned bounds. These are illustrated in Fig. 2.In Fig. 2 we plot the unfairness ranges given by [7] at every response time threshold: that is, for every response time, we create a binary decision version of the dataset and find the unfairness ranges given by [7]. The largest disparities are given by the bluecurve and the smallest disparities are given by the red curve. Wethen apply Eqs. (8) and (9) to process these curves and obtain the best (tight) upper bound and (loose) lower bound that we can deduce from [7]. We plot these upper and lower bounds on KS discrepancy,as the blue and red horizontal lines respectively.We then calculate our LP lower bound on the same data. Waploft this as the green horizontal line. The difference between [7]’slower bound and ours reveals that our LP gives a tighter view of the set of all feasible levels of unfairness. In general, a lower bound on unfairness (be it ours or [7]’s) tells us that even in the most optimistic view of the world, there must be at least some level of unfairness. The distances between our LP bound and [7]’sbound, then, represents 1) the extent to which we must reduce our optimism about the best possible scenario, and 2) the increased clarity we attain about the set of all possible levels of unfairness.Ideally, such increased clarity could help inform smarter policy decisions.The last graph in Fig. 2 shows results on the synthetic data described above. The synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world. However, even then we can observe the looseness of the bound derived solely based on discretization.Note that many of these request types have significant quality-of-life implications to the underlying population. Even relatively small improvements in the assessment of disparity can help the city intervene, and by doing so alter lives for the better. As such, we see these results as a clear indication of the increased applicability of the data combination methodology to continuous-valued urban decisions such as these.5.2.1 Block-by-block fairness. Even though ultimately the result of our audit is to output a lower bound on fairness, one advantage of our approach is that we explicitly determine the actual coupling that achieves this lower bound. That means that, along with the disparity assessment, we have insight about what is the most optimistic explanation for combining our primary and secondary data sources.This explanation can be understood as an attribution of which demographic groups the requests emanated from.More precisely, we have detailed information about demographic disparity on a block group-by-block group level, which we can use to visualize the demographic disparity of each block 𝑧 (i.e.,sup𝑡 |𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 0) − 𝐹𝑇 |𝑍,𝐴 (𝑡 |𝑧, 1)|) across the city, where 𝐹 is the coupling returned by our linear program. Though this is not exactly decomposition of the overall KS disparity, it still gives us a heuristic diagnostic tool to track the worst contributors to unfairness, evenwithing the most optimistic outlook of the lower bound.We do this in Fig. 3, through heat maps of block group disparities under the lower bound’s coupling. White regions indicate that no requests came from the block group(s) in that area. Otherwise, block groups are colored from green to red in order of increasing demographic disparity. The heat maps are presented in figure 3.It is interesting to note that three out of the four heat maps have substantial amounts of red block groups, despite all of the request types reflecting couplings that achieved a low KS disparity. This indicates that a coupling may be globally fair but still be unfair in many local areas. Additional heat maps are in the appendix.",1822-4,"synthetic data does not exhibit as large of gap as the one observed in real data, as our synthetic data does not capture all of the randomness and intricacies of the real world."
