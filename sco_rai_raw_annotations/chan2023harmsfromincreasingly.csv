,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{chan2023harmsfromincreasingly,
    author = {Alan Chan and Rebecca Salganik and Alva Markelius and Chris Pang and Nitarshan Rajkumar and Dmitrii Krasheninnikov and Lauro Langosco and Zhonghao He and Yawen Duan and Micah Carroll and Michelle Lin and Alex Mayhew and Maryam Molamohammadi and Katherine Collins and John Burden and Konstantinos Voudouris and Shalaleh Rismani and Wanru Zhao and David Krueger and Adrian Weller and Umang Bhatt and Tegan Maharaj},
    title = {Harms from Increasingly Agentic Algorithmic Systems},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-10, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Harm,Artifact,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,causing harm,
10,AlgorithmicSystems,Agent,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,algorithmic systems,
11,Humans,Agent,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,humans,
12,NegativeExternalities,Precept,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,significant negative externalities.,
13,LackOfRegulatoryBarriers,Precept,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,often without strong regulatory barriers,
14,BadModelPerformance,Precept,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,faulty modeling premises and bad model performance,
15,EconomicIncentives,Precept,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,significant economic and military incentives,
16, , , , , ,
17, , , , , ,
18, , , , , ,
19, , , , , ,
20, , , , , ,
21,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
22,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
23,NegativeExternalities,constrainsAgent,AlgorithmicSystems,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities."
24,AlgorithmicSystems,hasProducedArtifact,Harm,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,algorithmic systems play in causing harm.
25,LackOfRegulatoryBarriers,constrainsAgent,AlgorithmicSystems,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,"new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers"
26,BadModelPerformance,constrainsAgent,Humans,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system.
27,EconomicIncentives,constrainsAgent,AlgorithmicSystems,"The promised benefits of algorithmic systems have not always borne out, and benefits are often tempered by significant negative externalities. Although the deployment of algorithmic systems may result in increased safety or material improvements to human well-being [6, 102, 118], diverse lines of work in Fairness, Accountability,Transparency, and Ethics (FATE) have established the roles that algorithmic systems play in causing harm. Examples include the perpetuation of existing, unjust power relations [19, 38, 60, 104, 174,197], the generation of toxic language [7, 76], and information charms [42, 99, 119, 193]. Despite the clear evidence of harms from existing systems, new types of algorithmic systems are continually being developed and deployed, often without strong regulatory barriers [77]. The pace of development has been particularly rapid in the machine learning(ML) community. Just in the last five years, we have witnessed large improvements in the capabilities of systems to perform a variety of real-world tasks, including search [133], drug discovery [102, 175],and dialogue [138].Researchers in the FATE community have responded to the rapid pace of ML developments by emphasizing the need to anticipate harms, rather than just react to them. In particular, many have iden-tified the impact of computational modeling and development in social change [5, 95, 163] and scoped numerous taxonomies of risks,harms, and failures of algorithmic systems [153, 167, 193]. Whileit is crucial not to idealize or over-hype a model’s performance by ignoring model failures [23, 24, 28, 47, 120, 153, 187], it is also important not to understate (and thus fail to anticipate negative consequences of) what these models can do and may be capable of doing in the near future [31, 90, 103], especially given growing investments in the field [78]. In this paper, we continue the work of anticipating harms by drawing attention to increasingly agentic algorithmic systems. We use agency and agentic in a narrow sense for our work as applied to algorithmic systems, particularly ML systems. While recogniz-ing the many meanings of agency, as well as the need not to absolve humans of responsibility pertaining to algorithmic harms[48, 134, 196], we use the term agency consciously to counter the somewhat prevalent view that the developers of an algorithmic sys-tem have full control over its behavior. E.g. Johnson and Verdicchio[100] claim that “the behavior of computational artifacts is in the control of the humans that design them.” And in a systematic review on algorithmic accountability, Wieringa [196] defines algorithms “basically instructions fed to a computer”. While this descrip-tion is accurate for many purposes, we argue that, particularly forML-based algorithmic systems, it elides autonomous, responsive,and interactive qualities of these systems which can so easily lead to unforeseen outcomes. Cooper et al. [48], Nissenbaum [134] identify bugs – including faulty modeling premises and bad model performance – as one way in which humans may not have total control of the operation of an algorithmic system. However, we view agency as distinct from mistakes or bugs and demonstrate the unique and important harms that can result. We note there are significant economic and military incentives to build increasingly agentic systems. Indeed, many in the ML community are explicitly building such systems as a research goal [44, 155, 179].",652,there are significant economic and military incentives to build increasingly agentic systems
