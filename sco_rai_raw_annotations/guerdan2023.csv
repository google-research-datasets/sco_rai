,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{guerdan2023, author = {Guerdan, Luke and Coston, Amanda and Holstein, Kenneth and Wu, Zhiwei Steven}, title = {Counterfactual Prediction Under Outcome Measurement Error}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,OHIEDataset,Artifact,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset,
10,JOBDataset,Artifact,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,"the JOB dataset, which investigates the effect of job retraining on employment status",
11,Researchers,Agent,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,We,
12,Oregon,Agent,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,the U.S. state of Oregon,
13,LowIncomeIndividuals,Agent,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,low-income individuals seeking job retraining,
14,DiagnosisPrediction,Perceived_Problem,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,predict diagnosis with an active chronic medical condition,
15,UnemploymentPrediction,Perceived_Problem,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,construct an evaluation task predicting unemployment under enrollment (t = 1) and no enrollment (t = 0) in a job retraining program conditional on covariates.,
16, , , , , ,
17,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
18,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
19,Oregon,hasProducedArtifact,OHIEDataset,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,"the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset"
20,LowIncomeIndividuals,hasProducedArtifact,JOBDataset,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,"the JOB dataset, which investigates the effect of job retraining on employment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. "
21,DiagnosisPrediction,constrainsAgent,Researchers,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,We predict diagnosis with an active chronic medical condition
22,UnemploymentPrediction,constrainsAgent,Researchers,"In 2008, the U.S. state of Oregon expanded access to its Medicare program via a lottery system [21]. This lottery provided an opportunity to study the effects of Medicare enrollment on healthcare utilization and medical outcomes via an experimental design, commonly referred to as the Oregon Health Insurance Experiment (OHIE). Lottery enrollees completed a pre-randomization survey recording demo-graphic factors and baseline health status and were given a one-year follow-up assessment of health status and medical care utilization.We refer the reader to Finkelstein et al. [21] for details. We use the OHIE dataset to construct an evaluation task that parallels the labelchoice bias analysis of Obermeyer et al. [50]. We use this dataset rather than synthetic data released by Obermeyer et al. [50] be-cause (1) treatment was randomly assigned, ruling out positivity and ignorability violations possible in observational data, and (2) OHIE data contains covariates necessary for predictive modeling. We predict diagnosis with an active chronic medical condition over the one-year follow-up period given 𝐷 = 58 covariates, including health history, prior emergency room visits, and public services use.We predict chronic health conditions because findings from Obermeyer et al. [50] indicate that this outcome variable is a reasonable choice of proxy for patient medical need. We adopt the randomized lottery draw as the treatment. 5We conduct a second real-world evaluation using the JOB dataset, which investigates the effect of job retraining on employ-ment status [66]. This dataset includes an experimental sample collected by LaLonde [40] via the National Supported Work (NSW) program (297 treated, 425 control) consisting primarily of low-income individuals seeking job retraining. Smith and Todd [71]combine this sample with a “PSID” comparison group (2,490 con-trol) collected from the general population, which resulted in a final sample with 297 treated and 2,915 control. This dataset includes𝐷 = 17 covariates including age, education, prior earnings, and interaction terms. 482 (15%) of subjects were unemployed at the end of the study. Following Johansson et al. [31], we construct an evaluation task predicting unemployment under enrollment (𝑡 = 1)and no enrollment (𝑡 = 0) in a job retraining program conditional on covariates. 5.3.2 Synthetic OME and selection bias. We experimentally manipulate OME to examine how outcome regressions perform undertreatment-conditional error of known magnitude. We adopt diag-nosis with a new chronic condition and future unemployment as a target outcome for OHIO and JOBS, respectively. We observe proxy outcomes by flipping target outcomes with probability (𝛼0, 𝛽0). Wekeep (𝛼1, 𝛽1) fixed at (0, 0). This procedure of generating proxy outcomes by flipping available labels is a common approach for vet-ting the feasibility of new methodologies designed to address OME [44, 47, 77]. This approach offers precise control over the magnitude of OME but suffers from less ecological validity than studying multiple naturalistic proxies [50]. We opt for this semi-synthetic evaluation because (1) the precise measurement relationship between naturally occurring proxies may not be fully known, (2)the measurement relationship between naturally occurring prox-ies cannot be manipulated experimentally, and (3) there are fewRCT datasets (i.e., required to guarantee causal assumptions) that contain multiple proxies of the same target outcome.Models used for decision support are typically trained using data gathered under a historical decision-making policy. When Prior decisions were made non-randomly, this introduces selection bias (𝑇 ̸ ⊥⊥ 𝑋 ) and causes distribution shift between the popula-tion that received treatment 𝑡 in training data, and the full pop-ulation encountered at deployment time. Therefore, we emulate selection bias in the training dataset, and evaluate models over held-out test set of randomized data. We insert selection bias in OHIE data by removing individuals from the treatment (lottery winning) arm with household income above the federal poverty line (10% of the treatment sample). This resembles an observational setting in which low-income individuals are more likely to receive an opportunity to enroll in a health insurance program (e.g., Medicaid, which determines eligibility based on household income in relation to the federal poverty line). We restrict our analysis to single-person households, yielding 𝑁 = 12, 994 total samples (6, 189 treatment, 6, 805 control). We model selection bias in JOBS data by including samples from the observational and experimental cohorts in the training data. Because the PSID comparison group consists of individuals with higher income and education than the NSW group, there is consider-able distribution shift across the NSW and PSID cohorts [31, 40, 71].Therefore, a model predicting unemployment over the control pop-ulation (consisting of NSW and PSID samples) may suffer from bias when evaluated against test data that only includes samples from the NSW experimental arm. Thus we split data from the NSWexperimental cohort 50-50 across training and test dataset, and onlyinclude PSID data in the training dataset.5.3.3 Experimental setup. We include a Conditional Target (CT) model in place of a TPO model because counterfactual outcomes are not available in experimental data. CT provides a reasonable upper-bound on performance because identifiability conditions are satisfied in an experimental setting [53]. However, it is not possible to report accuracy over potential outcomes because counterfac-tual outcomes are unobserved. Therefore, we report error in estimates 𝜏 − ˆ𝜏, for𝜏 := E[𝑌 ∗ | 𝑇 = 1] − E[𝑌 ∗ | 𝑇 = 0], ˆ𝜏 := E[ ˆ𝜂1 (𝑋 )] − E[ ˆ𝜂0 (𝑋 )]where 𝜏 corresponds to the ground-truth treatment effect reported by prior work [16, 31] and ˆ𝜂𝑡 is a learned model discussed in § 5.1.One subtlety of this comparison is that our outcome regression target the conditional average treatment effect, while 𝜏 reflects the ATE across the full population. Following prior evaluations [31],we compare all methods against the ATE because the ground-truth CATE is not available for JOBS or OHIE data. 6 We report results over a test fold of randomized data that does not contain flipped outcomes of selection bias. Appendix A.4 contains additional setup details.5.3.4 Results. Figure 5 shows bias in ATE estimates 𝜏 − ˆ𝜏 over 10 runs on JOBS and OHIE data. The left panel compares CP, UT, UP,and the oracle CT model against RW-SL/SL with oracle parameters(𝛼0, 𝛽0), (𝛼1, 𝛽1). We show performance of RW-SL with learned pa-rameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) on the right panel. The left panel show that CP is highly sensitive to measurement error. This is because measurement error introduces bias in estimates of the conditional expectations, which propagates to treatment effect estimates. Be-cause UT and UP do not condition on treatment, they estimate an average of the outcome functions 𝜂∗0 and 𝜂∗1 , and generate predic-tions near 0. Therefore, while UT and UP perform well on OHIO data due to a small ground-truth ATE (𝜏 = 0.015), they perform poorly on JOBS (𝜏 = −0.077). SL and RW-SL with oracle parameters𝛼𝑡 , 𝛽𝑡 perform comparably to the CT model with oracle access to target outcomes across all measurement error settings.While we observe that re-weighting improves performance in our synthetic evaluation (given oracle parameters), we do not observe a similar advantage of RW-SL over SL in this experiment.Our results parallel other empirical evaluations of re-weightingfor counterfactual modeling tasks on real-world data (e.g., see §3.4.2 in [13]). One potential explanation for this finding is that our predictive model class (multi-layer MLPs) is large enough to learn the target regressions 𝜂∗0 and 𝜂∗1 for OHIO and JOBS data, even after our insertion of synthetic selection bias. As a result, re-weighting may not be required to learn a reasonable estimate of 𝜂∗0 and 𝜂∗ given available data. This interpretation is supported by strong performance of the oracle CT model. As shown on the right panel of Figure 5, RW-SL performance is highly sensitive to the choice of anchor assumption used to estimate parameters ( ˆ𝛼0, ˆ𝛽0), ( ˆ𝛼1, ˆ𝛽1) as indicated by increased bias in ˆ𝜏 and greater variability over runs. In particular, RW-SL performs poorly when Min/Max and Br/Max pairs of anchor assumptions are used to estimate error rates because the max anchor assumption is violated on OHIO and JOBS data. We shed further light on this finding by fitting the CT model to estimate ˆ𝜂∗0 , ˆ𝜂∗1 on OHIE data,then computing inferences over a validation fold 𝑋𝑣𝑎𝑙 . This analysis reveals thatmin𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 2.23 · 𝑒 −6, max𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗0 ≈ 0.85min𝑥 ∈𝑋𝑣𝑎𝑙ˆ𝜂∗1 ≈ 1.02 · 𝑒 −5, max𝑥 ∈𝑋𝑣𝑎𝑙· ˆ𝜂∗1 ≈ 0.81 which suggests that the min anchor assumption that min𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 =0 is reasonable for 𝑡 ∈ {0, 1}, while the max anchor assumption thatmax𝑥 ∈𝑋𝑣𝑎𝑙 ˆ𝜂∗𝑡 = 1 is violated for both 𝑡 ∈ {0, 1}. Therefore, because the min anchor assumption is satisfied for these choices of target outcome, and the ground-truth base rate is known in this experi-mental setting, RW-SL demonstrates strong performance given theBr/Min combination of anchor assumptions. In contrast, because the max anchor is violated, estimating 𝛽𝑡 by taking the supremum of ˆ𝜂𝑡 (𝑥) introduces bias in ˆ𝛽𝑡 , which results in poor performance of RW-SL with Min/Max and Br/Max anchors. Applying this same procedure to the unemployment outcome targeted in JOBS data also reveals a violation of the max anchor assumption. These results underscore the importance of selecting anchor assumptions in close consultation with domain experts because it is not possible to verify anchor assumptions by learning ˆ𝜂∗𝑡 when the target outcome of interest is unobserved. ",1590-3,we construct an evaluation task predicting unemployment under enrollment (t = 1) and no enrollment (t = 0) in a job retraining program conditional on covariates.
