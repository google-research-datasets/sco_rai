,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{meyer2023, author = {Meyer, Anna P. and Albarghouthi, Aws and D'Antoni, Loris},
title = {The Dataset Multiplicity Problem: How Unreliable Data Impacts Predictions},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,DisadavantagedLabelModificationRestrictions,Strategy,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"label modification to members of the disadvantaged group (i.e., Black people or women)",
10,AdvantagedLabelModificationRestrictions,Strategy,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"restrict label modification to the advantaged group (i.e., White people or men)",
11,RobustnessChange,Artifact,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,greatly affects robustness,
12,RacialAndGnderDiscrepencies,Causal_Theory,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women",
13,DifferentialRobustness,Perceived_Problem,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,the non-targeted group sees higher robustness rategains than the targeted group,
14,DatasetMultiplicityFramework,Artifact,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfactual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset.",
15,LabelPerturbations,Strategy,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,label perturbations,
16,USStates,Agent,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,for all states except Wisconsin,
17,Researchers,Agent,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,We,
18, , , , , ,
19,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
20,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
21,Researchers,hasProducedArtifact,DatasetMultiplicityFramework,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset."
22,DisadavantagedLabelModificationRestrictions,constrainsAgent,Researchers,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class."
23,AdvantagedLabelModificationRestrictions,constrainsAgent,Researchers,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"We restrict label modification to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. "
24,RobustnessChange,reflectsPrecept,LabelPerturbations,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,limiting all label perturbations to one racial group for Income greatly affects robustness
25,RacialAndGnderDiscrepencies,constrainsAgent,USStates,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women"
26,DifferentialRobustness,constrainsAgent,Researchers,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfac-tual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets' factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label er-rors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predic-tions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted [...] and 3.4 for measuring label-error multiplicity in linear models.4To speed up the evaluation, we use a high-throughput computing cluster. (We request 8GB memory and 8GB disk, but all experiments are feasible to run on a standard laptop.) This approach does not have a direct baseline with which to compare, as ours is the first work to propose and analyze the dataset-multiplicity problem.Datasets and tasks. We analyze our approach on three datasets:the Income prediction task from the FolkTables project [20], theLoan Application Register (LAR) from the Home Mortgage Disclo-sure Act publication materials [22], and MNIST 1/7 (i.e., the MNISTdataset limited to 1’s and 7’s) [32]. We divide each dataset into train (80%), test (10%), and validation (10%) datasets and repeatable experiments across 10 folds, except when a standard train/test split is provided, as with MNIST. We perform classification on theIncome dataset (whether or not an individual earned over $50,000),on LAR (whether or not a home mortgage loan was approved),and on MNIST (binary classification limited to 1’s and 7’s). Additionally, in the appendix we evaluate the regression version of Income by predicting an individual’s exact income. For all of theIncome experiments, we limit the dataset to only include data from a single U.S. state to speed computations. In Sections 4.1 and 4.3 we present results from a single state, Wisconsin, while in Section 4.2we compare results across five different US states.Accuracy-Robustness Tradeoff. There is a tradeoff between ac-curacy and robustness to dataset multiplicity that is controlled by the regularization parameter 𝜆 in the ridge regression formula𝜃 = (X⊤X − 𝜆𝐼 )−1X⊤y. Larger values of 𝜆 improve robustness at the expense of accuracy. Figure 2 illustrates this tradeoff. All results below, unless otherwise stated, use a value of 𝜆 that maximizes accuracy.Experiment goals. Our core objective is to see how robust linear models are to dataset label multiplicity. We measure this sensitiv-ity with the robustness rate, that is, the fraction of test points that receive invariant predictions (within a radius of 𝜖) given a certain level of label inaccuracies. The robustness rate is a proxy for the sta-bility of a modeling process under dataset multiplicity, so knowing this rate — and comparing it across various datasets, demographic groups, and algorithms — can help ML practitioners analyze the trustworthiness of their models’ outputs. In Section 4.1, we describe the overall robustness results for each dataset. Then, in Section 4.2,we perform a stratified analysis across demographic groups andshow how varying the dataset multiplicity model definition can significantly change data’s vulnerability to dataset multiplicity. Finally, in Section 4.3, we discuss results of the over-approximate approach and how it can be used to evaluate dataset multiplicityrobustness. For each dataset, the robustness rates are relatively high(> 80%) when fewer than 0.25% of the labels can be modified, and stay above 50% for 1% label error .Despite globally high robustness rates, we must also consider the non-robust data points. In particular, we want to emphasize that forIncome and LAR, each non-robust point represents an individual whose classification hinges on the labels of only a small number of training samples. That is, given the assumed uncertainty about the labels' accuracy, it is plausible that a ‘clean’ dataset would output different test-time predictions for these samples. Some data points will almost surely fall into this category — if not, that would mean the model was independent from the training data, which is not our goal! However, if a sample is not robust to a small number of label modifications, perhaps the model should not be deployed on dat sample. Instead, if the domain is high-impact, the sample could be evaluated by a human or auxiliary model (see Section 5 for more discussions on how to handle non-robust test samples).Returning to Table 1, many data points are not robust at low label error rates, e.g., when 1% of labels may be wrong, 49.3% ofIncome test samples are not robust. Likewise, 38.7% of LAR test samples can receive the opposite classification if the correct subset of 1% of labels change. These low robustness rates call into question the advisability of using linear classifiers on these datasets unless one is confident that label accuracy is very high.In Section 4.1, we showed dataset multiplicity robustness results given the assumption that all labels in the training dataset were potentially inaccurate. However, in practice, label errors may be systemic. In particular, two of the datasets we analyzed in Section 4.1contain data that may display racial or gender bias. We hypothesize that the Income dataset likely reflects trends where women and people of color are underpaid relative to white men in the UnitedStates, and that the LAR dataset may similarly reflect racial and gender biases on the part of mortgage lending decision makers. To Leverage this refined understanding of potential inaccuracies in the labels, in this section we evaluate test data robustness under the following two targeted dataset multiplicity paradigms:• ‘Promoting’ the disadvantaged group: We restrict label modification to members of the disadvantaged group (i.e., Black people or women); furthermore, we only change labels from the negative class to the positive class.• ‘Demoting’ the advantaged group: We restrict label modifi-cation to the advantaged group (i.e., White people or men);furthermore, we only allow change labels from the positive class to the negative class. This setup is compatible with the worldview (for example) that men are overpaid.Before delving into the results, we want to acknowledge that this analysis has a few shortcomings. First, for simplicity we use binary gender (male/female) and racial (White/Black) breakdowns. Clearly,this dichotomy fails to capture complexities in both gender and racial identification and perceptions. Second, the targeted dataset multiplicity models that we use also over-simplify both how dis-crimination manifests and how it can interact with other identities not captured by the data. Finally, we are not social scientists or domain experts and it is possible that the folk wisdom we rely on to propose data biases does not fully capture the patterns in the world. Rather, readers should treat this section as an analysis of toy phenomena’ meant to illustrate how our technique can be used for real-world tasks. Figure 3 shows that in all cases the targeted multiplicity definition yields significantly higher overall robustness rates than a broad multiplicity definition does. Notably, limiting all label perturbations to one racial group for Income greatly affects robustness: using the original multiplicity definition (no targeting), no test samples are robust when 12% of labels can be modified. However, when limiting label errors to Black people with the negative label, the robustness rate remains over 95% — it turns out this is not surprising, since fewer than 2% of the data points have race=Black. However, more than 80% of samples have race=White, and limiting label changes toWhite people with the positive label still yields over 60% robustness when 12% of labels can be changed. Similarly, using targeted dataset multiplicity definitions for LAR can increase overall robustness by up to 30 percentage points.Demographic group robustness rates. We also investigate the ro-bustness rates for different demographic groups, both under the original, untargeted multiplicity assumptions and under the tar-geted versions. Each row of Figure 4 compares baseline (untargeted) robustness rates, stratified by demographic groups, with targeted versions ofM for five US states. We observe two trends: first, there are com-monly racial and gender discrepancies (see the pairs of dotted lines).E.g., for all states except Wisconsin, men consistently have higher baseline robustness rates than women (sometimes by a margin of over 20%). Second, using various targeted versions of M has unequal impacts across demographic groups. The top row of Figure 4 shows that targeting on race=Black (i.e., allowable label pertur-bations can change Black people’s labels from −1 to 1) modestly improves dataset multiplicity robustness rates for Black people, but massively improves them for White people. We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group, across the other versions of M, aswell. On LAR, similar results hold. (Graphs and discussion are in the appendix.)",197-99,"We see similar trends,namely, that the non-targeted group sees higher robustness rategains than the targeted group"
