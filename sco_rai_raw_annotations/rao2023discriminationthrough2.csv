,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{rao2023discriminationthrough,
    author = {Varun Nagaraj Rao and Aleksandra Korolova},
    title = {Discrimination through Image Selection by Job Advertisers on Facebook},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-11, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,OverrepresentationOfWomenAndBlackPeople,Strategy,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,over-represent women and Blackpeople,
10,ComplexIncentives,Perceived_Problem,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,advertiser selection of people in job adimages [...] is complex and varies across different occupations.,
11,AttemptToDiversify,Strategy,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,may be attempting to diversify their existing workforce ,
12,DiscriminationThroughImageSelection,Strategy,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,to discriminate through image selection.,
13,Advertisers,Agent,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,advertisers,
14, , , , , ,
15, , , , , ,
16, , , , , ,
17, , , , , ,
18, , , , , ,
19, , , , , ,
20, , , , , ,
21, , , , , ,
22, , , , , ,
23, , , , , ,
24, , , , , ,
25, , , , , ,
26, , , , , ,
27,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
28,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
29,OverrepresentationOfWomenAndBlackPeople,constrainsAgent,Advertisers,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,we observe that most advertisers over-represent women and Blackpeople
30,ComplexIncentives,constrainsAgent,Advertisers,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations.
31,AttemptToDiversify,constrainsAgent,Advertisers,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,"some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce "
32,DiscriminationThroughImageSelection,constrainsAgent,Advertisers,"With the goal of studying whether advertisers may be selecting images to discriminate by race in addition to gender, we now focus our analysis on a select set of advertisers and study the depiction of people in their images according to both gender and race.Data Collection: We chose a diverse set (in terms of worker hours,domains, skill sets, demographic distribution, and income levels) of occupations and associated advertisers to study based on Bureau Of Labor Statistics (BLS) data from 2021 [12], contingent worker supplement from 2018 [11] and availability of job ads in the AdLibrary containing images of people, leading to the following:Occupation Specific Advertisers (N=15): BestBuy, Doordash, Eataly,Geico Careers, Drive with HopSkipDrive, Instacart, Drive With Lyft, Nationwide Job Search for Education, NationwideJob Search for Information Technology, Nurse Recruiter, NYPD Recruit, Safeway, TSA, Uber, UPS Jobs.Job Aggregator Advertisers (N=3): Monster, SimplyJobs, Talent.We map occupation specific advertisers to the closest BLS categories based on the type of ads; e.g., NYPD Recruit is mapped to the BLScategory “Police officers”.We scraped ads of the chosen advertisers from the Ad Library inOctober - November 2021 (and repeated the scrape in October 2022).See Table 4 and Table 5 in Appendix A.2 for a detailed summary of per-advertiser statistics in 2021. Data Annotation: We recruited U.S. based Amazon MechanicalTurk (MTurk) workers, and compensated them above the federal minimum wage to annotate all images according to perceived gen-der and race. For each image, 2 workers were asked to count, if they exist, the number of people in the image according to gen-der (man, woman) and race (White, Black, Asian, Hispanic). We Included form logic to ensure annotations were meaningful before they were submitted. We included an “Other” option for the anno-tators if it wasn’t possible to determine gender or race or both (∼9%of total annotations). For this annotation task, we asked annotators to count all people in the images, and not just employees of the occupation being advertised. The obtained the final count of people in the image by averaging the annotations of the two annotators.As in the previous analysis, our method makes the simplifying assumption of binary gender for the sake of comparison with the binary BLS data. Furthermore, it does not annotate the images by skin tone [10] since such annotation would make a comparison with BLS data impossible. We restrict our analyses to the two most prevalent races in the data: Black and White.Analysis: For a specific occupation, we judge whether an adver-tiser may be trying to skew their ads, to discourage or attract people of a specific demographic, by comparing the rate of representation of gender (woman) and race (Black, White) of people depicted in the images the advertiser uses with that of the U.S. workforce (given by BLS) for that occupation. If the rates of representation are differ-ent, we say that there is a deviation in representation and indicate whether it is an over- or under-representation.For a given advertiser and across all their images, we compute the rate of representation of people. Across the 15 advertisers in Figure 4 and columns 6-8 of Table 4,we observe that most advertisers over-represent women and Blackpeople, and under-represent White people. We draw definitive conclusions for advertisers for whom the error bars do not cross the reference line, finding the following:Women: 10 (over), 4 (under), 1 (inconclusive);White People: 0 (over), 12 (under), 3 (inconclusive);Black People: 8 (over), 1 (under), 6 (inconclusive).Our findings indicate that advertiser selection of people in job adimages with regards to gender and race is complex and varies across different occupations. Broadly, across a diverse set of occupations we find that some advertisers (e.g., NYPD Recruit and TSA) may be attempting to diversify their existing workforce by including more women and Black people than are employed in those professions per LBS in their images.Evidence for Proactive Advertiser Selection by Monster: Advertis-ers may intentionally vary image selection based on their specific needs over time. To test this hypothesis, we studied Monster.com(a job aggregator), which had the most number of campaigns, andcompared image use in 2021 and 2022. We find evidence of deliber-ate image use and variation over time across different occupations.In 2021, we find evidence of stereotypical image use across oc-cupations. To enable this analysis, we extracted the occupation from the destination page URL of the ads, and used it to group our findings according to the top 3 roles - security officer, engineer, and technician. Our results are presented in Table 3. We find that the representation of women and Black people was mostly balanced for the engineer (45% women, 48% Black) and security officer (49%women and 50% Black) roles, but was skewed for technicians (28%women and 21% Black). We observed a White majority only in the case of technicians (64%).Next, in order to delve deeper into possible advertiser intent behind image selection, we performed a reverse Google Image Search the images. We observed that the images were generic and not present on the corresponding employer pages, implying a possible deliberate attempt by Monster.com to discriminate through image selection. Examples of such stereotypical images are in Figure 5a.An expanded list of such occupations with stereotypical image use include: Black Man - pipe fitter, White Man - IT recruiter, BlackWoman - security officer, White Woman - Sr. Software Engineer,Lactation Consultant.In 2022, in contrast to the stereotypical image selection in 2021,most ads contained a single image with multiple people of diverse demographic characteristics as in Figure 5b. As a result, the percent-age of women, White and Black people remains consistent across occupations as is evident in Columns 3, 5, 7 of Table 3.",1775-8,a possible deliberate attempt by Monster.com to discriminate through image selection.
