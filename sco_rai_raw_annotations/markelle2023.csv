,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{markelle2023, author = {Kelly, Markelle and Kumar, Aakriti and Smyth, Padhraic and Steyvers, Mark}, title = {Capturing Humans’ Mental Models of AI: An Item Response Theory Approach}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,MTurkWorkers,Agent,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2","203 Amazon Mechanical Turk workers, all located in the U.S.,",
10,ExpectationOfDifferentPerformance,Causal_Theory,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2",participants expect AI agents to perform very differently from themselves,
11,OverOrUnderReliance,Perceived_Problem,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2",under- or over-reliance on an AI agent,
12,ExpectationOfUnifiedintelligence,Causal_Theory,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2","participants expect a more unified, single intelligence from an AI agent than they do another person",
13,InaccuratePerformanceAssesment,Causal_Theory,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2","participants’ other-assessments did not fully converge to AI agents true performances, even given feedback ",
14,StudyResults,Artifact,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2",participated in the study,
15, , , , , ,
16, , , , , ,
17, , , , , ,
18, , , , , ,
19, , , , , ,
20, , , , , ,
21, , , , , ,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,MTurkWorkers,hasProducedArtifact,StudyResults,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2","203 Amazon Mechanical Turk workers, all located in the U.S., participated in the study, which was conducted in January 2023"
26,ExpectationOfDifferentPerformance,constrainsAgent,MTurkWorkers,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2","participants expect AI agents to perform very differently from themselves, especially in the absence of feedback."
27,OverOrUnderReliance,constrainsAgent,MTurkWorkers,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2",This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it.
28,ExpectationOfUnifiedintelligence,constrainsAgent,MTurkWorkers,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2","in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary."
29,InaccuratePerformanceAssesment,constrainsAgent,MTurkWorkers,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understandingof human-AI teams. Extending relevant work from cognitive sci-ence, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. [...] 203 Amazon Mechanical Turk workers, all located in the U.S., par-ticipated in the study, which was conducted in January 2023. Participants could only complete the experiment once (and were disqual-ified if they had participated in the earlier pilot study). To ensure high-quality participation, workers were required to be AMT Masters and have a 95% approval rating; they were paid $7 plus a bonus of up to $2. These incentive bonuses were based on the participants other-assessment performance. The experimental protocol was ap-proved by the University of California, Irvine Institutional ReviewBoard.Participants were assigned to an agent type (other human orAI agent) and agent category (high accuracy, low accuracy, or no feedback). Participants were evenly divided between these six experimental condition combinations. [...] we outline three key takeaways from our experiments and discuss how they might affect human-AI collaboration.1. Over-differentiation of AI from self. We observe that mental models of an AI agent’s ability are highly differentiated from self-perceived ability (e.g., Figures 3 and 4 and Table 3). On average,participants expect AI agents to perform very differently from themselves, especially in the absence of feedback. This bias could lead to under- or over-reliance on an AI agent in a team setting;additional work is needed to better understand and counteract it. 2. “General intelligence” bias. While we observe that people can pick up on the strengths and weaknesses of AI agents (see also[6, 51]), in our experiments participants expect a more unified, single intelligence from an AI agent than they do another person,even after observing evidence to the contrary. In particular, the other agents in our experiments have near-identical inter-topic performance variations, but participants perceive much higher cor-relations between topics for AI agents (see Section 6.2.2). This can look like a failure to recognize how well the AI agent performs in its strongest areas, and how poorly it performs in its weakest areas(see Section 6.2.3), potentially resulting in over- or under-use of an AI decision making aid. Again, further research is needed to determine the extent of this bias and how it might be counteracted in human-AI teams.3. Incomplete development of mental models. In our experiments,participants’ other-assessments did not fully converge to AI agents true performances, even given feedback (see Figure 4). This extended to the per-topic level; participants did not accurately es-timate AI agents’ strengths and weaknesses after 16 rounds of feedback (see Figure 6). This phenomenon could serve as motiva-tion to give the teammates of an AI agent extra information to aid in OMM development, e.g., a “primer” or onboarding process [15]or prediction explanations [2, 52].Looking ahead, we believe our modeling framework could be useful for capturing people’s OMMs in the context of hybrid human-AI teams. In this paper, we investigate how a person perceives an AI agent, in terms of their different abilities, or strengths and weaknesses, and the difficulties of specific problems for the agent.Our findings and framework could help predict when a person is likely to defer to an AI agent (and thus help predict overall team performance) and identify biases that could lead to over- or under-use of the agent.Limitations. Our experiments and results are limited to a singletask and setting, and involve only Amazon Mechanical Turk work-ers, who are not necessarily knowledgeable on the task or on AIin general. In future work, it will be important to investigate these research questions across other settings, e.g., for image classifica-tion or other tasks, and with other users, e.g., human experts who already interact with an AI agent on a regular basis.","1723, 1726, 1731-2","In our experiments, participants’ other-assessments did not fully converge to AI agents true performances, even given feedback "
