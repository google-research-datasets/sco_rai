,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{benbouzid2023, author = {Benbouzid, Bilel}, title = {Fairness in Machine Learning from the Perspective of Sociology of Statistics: How Machine Learning is Becoming Scientific by Turning Its Back on Metrological Realism}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,ProPublica,Agent,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,ProPublica,
10,BiasAgainstMinorities,Perceived_Problem,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,biased against ethnic minorities,
11,Northpointe,Agent,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,Northpointe,
12,DiscoveryOfUnbalancedRates,Artifact,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,unbalanced rate of false positives and false negatives.,
13,COMPAS,Agent,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,COMPAS,
14,EvidenceOfCalibration,Artifact,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,aimed at making the model equally accurate in different groups (an approach known as calibration).,
15, , , , , ,
16, , , , , ,
17,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
18,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
19,ProPublica,hasProducedArtifact,DiscoveryOfUnbalancedRates,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives.
20,BiasAgainstMinorities,constrainsAgent,COMPAS,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives.
21,Northpointe,hasProducedArtifact,EvidenceOfCalibration,"It was the controversy surrounding COMPASS, software used by certain courts in the United States to predict recidivism, that made group fairness a central research topic in FairML. Disagreements Between data journalists with ProPublica, an investigative news organization, and the engineers of Northpointe, the firm that devel-oped COMPASS, reflected different ways of assessing the fairness of the algorithm. ProPublica discovered that COMPAS’s risk assessment tool was biased against ethnic minorities by unbalanced rate of false positives and false negatives. The situation was intuitively unjust, but the focus of the ensuing debate was primarily on the contrast between the measure that ProPublica chose to use and those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration). How should the choice be made between metric based on the value of equal opportunity, and one based on the (engineering) value of modeling efficiency? It is impossible for a model to satisfy both metrics at the same time, a fact that has even been formalized as a theorem [43].The difficulty of combining these different metrics has been conceptualized in terms of three broad families of group fairness:separation (equalizing recall), sufficiency (equalizing precision or ac-curacy) and independence (often called demographic parity). While The first two refer to different ways of eliminating differences between groups in the rate of various types of errors, the third requires the algorithmic classification to produce equal results for different groups. Moritz Hardt, one of the most influential researchers in the field [44], has argued that, out of the various group fairnessmetrics, equality of opportunity (a member of the separation family) offers the best means to produce fairness. Hardt presents mathemat-ical arguments for equal opportunity, showing in particular that this metric is of interest because it performs better at maximizing model utility than does statistical parity. But his position can also be understood in terms of at least three political reasons.The first is that calibration cannot be considered as a social jus-tice intervention as such, and can instead be framed as the “normal”result of the successful work of the data scientist to ensure model accuracy for all groups. In merely calibrating the model, we do not take on the challenge of introducing morality into the machine(although as Daston, 1995, has shown, scientific accuracy is subject to a particular moral economy of its own). The second considera-tion is that equality of opportunity is task specific, thus involving both the modeler and the user in understanding and refining themodels. The third is that statistical parity (and thus the “indepen-dence” family of group fairness metrics: Castelnovo et al., 2021)can be seen as the very opposite of fairness. To compensate for the(undesirable) effect of the dependence of the classifications on the variable that is subject to demographic disparity or discrimination,this notion of fairness requires different groups to be treated differ-ently. It involves a form of affirmative action that is not rooted in any meritocratic principle [35].In proposing equality of opportunity as a metric, Hardt is not merely making a statistical argument. His stance is also a political one: he takes a position on one side of the classical opposition in moral philosophy between equality of outcomes (represented hereby statistical parity) and equality of opportunity.",38,"those that Northpointe proposed in response, which were aimed at making the model equally accurate in different groups (an approach known as calibration)."
