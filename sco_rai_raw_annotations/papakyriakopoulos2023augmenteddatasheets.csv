,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{ma2023yousounddepressed,
    author = {Anna Ma and Elizabeth Patitsas and Jonathan Sterne},
    title = {You Sound Depressed: A Case Study on Sonde Health’s Diagnostic Use of Voice Analysis AI},
    year = 2023
}
", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,DisparateHarms,Perceived_Problem,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,raises serious questions about the disparate harms these technologies can have on different populations,
10,HiringUnfairness,Perceived_Problem,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,may be disproportionately unlikely to be hired given speech-based hiring screening software [285].,
11,MedicalNotes,Artifact,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,Doctors increasingly use SLT to efficiently take patientnotes,
12,SLT,Agent,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,Speech Language Technologies (SLT),
13,Transcriptions,Artifact,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,resulting transcriptions,
14,Bias,Causal_Theory,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,are prone to bias,
15,DataVariability,Perceived_Problem,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,"there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features.",
16,NonStandardSpeaker,Agent,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,Individuals who do not speak “standard ” varieties of a language ,
17,HealthHarms,Perceived_Problem,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,which could result in serious health harms if transcribed incorrectly,
18,IncarceratedSurveilence,Artifact,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,surveil the phone calls of incarcerated individuals,
19,RacialUnfairness,Causal_Theory,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231].,
20, , , , , ,
21, , , , , ,
22,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
23,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
24,DisparateHarms,constrainsAgent,SLT,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations
25,HiringUnfairness,constrainsAgent,NonStandardSpeaker,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,Individuals who do not speak “standard ” varieties of a language [185] may be disproportionately unlikely to be hired given speech-based hiring screening software [285].
26,MedicalNotes,influencesPrecept,HealthHarms,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,"Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly"
27,SLT,hasProducedArtifact,IncarceratedSurveilence,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,SLT technology has been developed to surveil the phone calls of incarcerated individuals
28,Transcriptions,influencesPrecept,RacialUnfairness,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231].
29,Bias,constrainsAgent,SLT,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,Speech technologies—as with any machine learning applications—are prone to bias
30,DataVariability,constrainsAgent,SLT,"The ubiquity of Speech Language Technologies (SLT) in every-day life raises serious questions about the disparate harms these technologies can have on different populations. Use cases such as automated speech recognition (ASR), speaker recognition, speechsynthesis, speech quality assessment, speech enhancement and denoising are now integrated in smartphones, cameras and virtual-assistants [260], and applied in domains such as customer service [16], finance [85, 178, 199], navigation [63, 273, 279], health [171],education [191, 195, 196, 202, 276], and law [107]. The downstream impacts of biased SLT can be severe. Individuals who do not speak “standard ” varieties of a language [185] may be disproportion-ately unlikely to be hired given speech-based hiring screening soft-ware [285]. Doctors increasingly use SLT to efficiently take patientnotes, which could result in serious health harms if transcribed incorrectly [171]. Moreover, SLT technology has been developed to surveil the phone calls of incarcerated individuals; the resulting transcriptions—likely disproportionately inaccurate for Black individuals [128]—can result in differential treatment [231]. Speech technologies—as with any machine learning applications—are prone to bias; these biases often stem from nonrepresentative or inaccurate training data [35, 124, 128, 129, 186]. There is a mis-match between the “world as it is,” and the “world according to data”[186]; there are further mismatches between the data used to train a machine learning model, and the data testing the model in practice[129]. Generating robust SLT is an especially complex task given the diversity of human speech, comprising of different languages,accents, dialects, varieties, and speech impediments. An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features. Finally, as with any dataset, it is imperative to center ethical dataset creation and usage—regarding the privacy, respect, and protection of data subjects, interviewers,and transcription annotators.",881-2,"An additional layer of complexity arises from the collection of speech data used to train SLT applications: there is high variability across necessary tasks, from noise in a recording environment to transcription of language and acoustic features."
