,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{boggust2023, author = {Boggust, Angie and Suresh, Harini and Strobelt, Hendrik and Guttag, John and Satyanarayan, Arvind},
title = {Saliency Cards: A Framework to Characterize and Compare Saliency Methods},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,NeedToUnderstandEffectivenessOfSaliencyCards,Perceived_Problem,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,"evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks",
10,PopularityAndEase,Strategy,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,their popularity in prior work (U1–U8) and ease of implementation,
11,LackOfConsiderationForAlgorithmicDifferences,Perceived_Need,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,rarely considered algorithmic differences or evaluated the suitability of a particular method for their task,
12,PrioritizationOfEfficiency,Strategy,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset.,
13,PrioritizationOfDeterminism,Strategy,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,prioritized determinism based on their personal experience.,
14,PrioritizationOfSemanticDirectness,Strategy,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,prioritized semantic directness to help them communicate results to business clients without ML experience. ,
15,ConflictingPriorities,Perceived_Problem,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,priorities often differ or conflict ,
16,ModelTestsOfInputSensititivity,Strategy,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,"regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs.",
17,ImportanceOfFaithfulness,Strategy,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,faithfulness as an ideal attribute of saliency methods.,
18,NoExpectationOfMinimality,Causal_Theory,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty.,
19,Skepticism,Perceived_Problem,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,skeptical that existing evaluations appropriately assessed model sensitivity.,
20,GoodDocumentation,Artifact,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,Good documentation,
21,Researchers,Agent,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,we,
22,Users,Agent,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,users,
23,ImprovedUnderstanding,Goal,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,make saliency methods easier to understand and use,
24, , , , , ,
25, , , , , ,
26, , , , , ,
27,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
28,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
29,NeedToUnderstandEffectivenessOfSaliencyCards,constrainsAgent,Researchers,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,"we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks"
30,PopularityAndEase,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation
31,LackOfConsiderationForAlgorithmicDifferences,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task
32,PrioritizationOfEfficiency,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset.
33,PrioritizationOfDeterminism,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,U9 prioritized determinism based on their personal experience.
34,PrioritizationOfSemanticDirectness,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,"For instance, U7 prioritized semantic directness to help them communicate results to business clients without ML experience."
35,ConflictingPriorities,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,Our user study also revealed that user priorities often differ or conflict
36,ModelTestsOfInputSensititivity,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,"U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs."
37,ImportanceOfFaithfulness,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,participants often cited faithfulness as an ideal attribute of saliency methods.
38,NoExpectationOfMinimality,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty.
39,Skepticism,constrainsAgent,Users,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,U8 was skeptical that existing evaluations appropriately assessed model sensitivity.
40,GoodDocumentation,influencesPrecept,ImprovedUnderstanding,"Through nine semi-structured user interviews, we evaluate saliency cards to understand how they can help users understand, compare, and select methods appropriate for their tasks. We recruited par-ticipants from within our professional network (4/9) and through an open call in our organizations and on Twitter (5/9). Participants Came from diverse backgrounds (including research, radiology, com-putational biology, and consultancy) and had varying levels of machine learning expertise and familiarity with saliency methods.Fig. 4 illustrates user demographics, describes their saliency method use cases, and summarizes the results of our interview studies.Eight participants (U1–U8) had used saliency methods in some capacity. With these participants, we began by asking open-ended questions about their experience with saliency methods, such a “What tasks do you use saliency methods for?”, “How do you decidewhich saliency method to use?”, and “What do you do if saliencymethods disagree?”. Next, we discussed each saliency method at-tribute. We asked participants to describe if and how each attributewas important to their task and rank the attributes by importance.Finally, we walked users through example saliency cards (Fig. 2)and had users give us feedback on the design and usefulness.The radiologist (U9) did not have experience using saliency meth-ods but was interested in their application to medical decision-making. Since they were less familiar with ML, we structured this conversation slightly differently. We discussed five attributes —determinism, hyperparameter dependence, semantic directness,minimality, and perceptual correspondence. For each, we provideda definition, showed radiology examples demonstrating its implica-tions (e.g., saliency with and without minimality), and discussed ifand when it would be important to them in a clinical setting.We conducted 30–60min interviews via video chat and compen-sated users with $30 Amazon gift cards. Our study received an IRBexemption from our organization. We obtained informed consentfrom participants, stored user data securely, and anonymized userdetails in the paper. See Sec. A.3 for additional study details.4.1 Saliency Cards Help Users SelectTask-Appropriate Saliency MethodsDespite experience with a broad range of saliency methods, includ-ing LIME [49] (U3–U7), vanilla gradients [20, 54] (U5, U6, U8), inte-grated gradients [57] (U4–U6), Grad-CAM [52] (U1, U2), SHAP [40](U3–U5), and SmoothGrad [55] (U5), users chose saliency methods based on their popularity in prior work (U1–U8) and ease of imple-mentation (U2, U7). Users rarely considered algorithmic differences or evaluated the suitability of a particular method for their task. Asa result, users were often unsure if their chosen saliency method was indeed appropriate for their task and worried that a different method could produce more accurate results. Consequently, users wanted a more principled selection strategy based on formal evalu-ations but found extracting insight from existing documentation tedious. As U6 described, given that “new methods come out everyday” and “reading all the papers is a difficult task that takes a lot of time,” even researchers find it challenging to acquire the knowledge needed to select saliency methods well-suited to their tasks.In contrast, we found that the attribute-based structure of saliencycards allowed users to more systematically select saliency methods based on properties important to their task. Users prioritized each attribute based on their task requirements, experiences and pref-erences, and the expectations of their teammates. For example, U1 Prioritized computational efficiency because their research requires them to compute saliency maps for every input in their dataset. An inefficient saliency method would be incompatible with their model evaluation process and prevent them from quickly iterating on model design choices. U9 prioritized determinism based on their personal experience. They were uncomfortable interpreting non-deterministic saliency maps because they do not encounter non-determinism in other medical technologies. When prioritizing attributes, users in applied domains also needed to consider their teammates' expertise. For instance, U7 prioritized semantic direct-ness to help them communicate results to business clients without ML experience. After prioritizing attributes, users utilized the visual saliency card documentation to juxtapose attribute summaries andpick a well-suited saliency method in just a few minutes. Our user study also revealed that user priorities often differ or conflict — a surprising finding given that existing evaluations are often framed as “tests” every saliency method should pass [1, 19, 57]. While one user would prioritize an attribute, another would de-prioritize or explicitly not desire that attribute. For example, U8 Prioritized minimality because they train machine learning models on long amino acid sequences, and their biochemist coworkers interpret the saliency method results. Without minimality, con-firming the models have learned biologically meaningful features would require the biochemists to analyze the interactions between potentially hundreds of amino acids. However, U4 explicitly preferred a less minimal method. They worried that a minimal salience method might only highlight the features necessary for the model's prediction. Since they use saliency methods to manually analyze a few inputs, they want to view every feature relevant to the model's prediction to ensure their models do not learn spurious correlations.Even users in similar roles had different priorities. For instance,despite both being researchers who use saliency methods to an-alyze model behavior, U2 and U6 viewed the importance of input sensitivity differently. U2 regularly tests models by perturbing back-ground features, so without input sensitivity, a saliency method could incorrectly assign importance to changes the model considers important. On the other hand, U6 did not care about input sensi-tivity because they only use in-distribution data and do not worry about noise or perturbations impacting the inputs. The frequency of conflicting priorities suggests there is not an ideal saliency method for every user and task. Thus, documentation is crucial to help users find a saliency method appropriate for their use case.4.2 Saliency Card Attributes Provide a DetailedVocabulary for Discussing Saliency MethodsSaliency cards provide a more precise attribute-based vocabulary that helps users communicate about saliency methods. At the start of our interviews, participants often cited faithfulness as an ideal at-tribute of saliency methods. Faithfulness broadly refers to a saliency method's ability to reflect model reasoning accurately and corre-lates with the saliency card’s sensitivity attributes. However, after discussing the ten saliency card attributes, users had a more detailed language to describe saliency method characteristics. For example,U5 initially expected all saliency methods to achieve faithfulness.However, after working with saliency cards, U5 more precisely ar-ticulated that they expected saliency methods to be label and modelsensitive. They did not care about a method’s input sensitivity, even though it is typically considered part of faithfulness. As a saliency method developer, U5 needs to be able to communicate their exact design goals so users can understand the benefits, limitations, and appropriate use cases of the saliency method. If they described their saliency method as faithful, users could incorrectly assume it is input sensitive, deploy it in an inappropriate setting, and mis interpret the results. Using a shared attribute-based vocabulary, users and developers can better communicate about a saliency methods specific attributes, evaluative results, and prescribed use cases The saliency card attributes also helped lay users discuss salient methods. Before our user study, U9 (a radiologist) had little expe-rience with machine learning and was entirely unfamiliar with saliency methods. However, by using the vocabulary of saliency card attributes, our conversation revealed differences in their expressed needs and expectations in the literature about what lay users want in a saliency method. For example, minimality is of-ten considered an essential attribute because it makes the visual saliency map easier to interpret [33, 55, 58]. However, U9 did not expect a saliency method to be minimal because they were accustomed to using noise in medical imaging to attenuate measurement uncertainty. Using the saliency card attributes gave U9 terminol-ogy they could use to communicate with ML experts and software vendors in charge of developing and deploying saliency methods.Without this language with which to communicate, radiologists might not as deeply engage in the deployment process, leavingML experts to rely on incorrect assumptions about radiologists expectations. However, with direct channels of communication, MLexperts could work with radiologists to increase transparency in the deployment process, ensure they interpret saliency method results appropriately, and, even, develop new saliency methods explicitly designed for clinical imaging settings. 4.3 Saliency Cards Inspire Areas for FutureWork and New Documentation PracticesThe attribute summaries led users to ask new questions about evaluating saliency methods and to hypothesize future research di-rections. By documenting evaluation results for a saliency method,saliency cards reveal that particular attributes and methods have been more heavily evaluated than others. For instance, comparing the saliency cards for integrated gradients [57] (Fig. A2) and Grad-CAM [52] (Fig. 2) reveals that integrated gradients has been more rigorously tested for input sensitivity. Whereas previously, users would have had to extract evaluative results from multiple academic papers, saliency cards surface these discrepancies directly, inspiring users to hypothesize about Grad-CAM’s performance on missing evaluations and express interest in completing the testing suite. Fur-ther, by categorizing individual evaluations, saliency cards exposethat evaluations for the same attribute have varying testing strate-gies, such as testing meaningful [4] vs. noisy perturbations [34]or focusing on images [27] vs. natural language modalities [19].Users were surprised to see the evaluation diversity, leading them to hypothesize new evaluation measures. For instance, some users were intrigued to run perceptibility tests on their data and models. As U5 put it, “If I have a specific use case in mind, I want to see the metrics on that specific use case.” They brainstormed ideas about integrating saliency cards into a suite of evaluations that generate customized saliency cards based on the user’s model and datasets.Inspecting some attributes revealed limitations of saliency cards and existing evaluations. Saliency cards group evaluations into user-centric attributes, but some attributes are challenging to test accurately. During our user study, U8 was skeptical that existing evaluations appropriately assessed model sensitivity. Model sen-sitivity evaluations test that a saliency method responds to mean-ingful model changes, but U8 argued that it is almost impossible to guarantee that a change to a black-box model is meaningful. For Instance, a standard model sensitivity test measures the saliencymethod’s response to layer randomization, but layer randomization might not be meaningful if that layer is redundant. In that case,layer randomization tests could incorrectly punish a model for not responding to an insignificant change. This issue might be solved as additional research invents new evaluations, including model sensitivity tests. However, it could also be that some attributes, like model sensitivity, are too broad. Perhaps breaking model sensitivity down into more precise categorizations, like layer randomization sensitivity, would provide more straightforward documentation.Similarly, we expect the methodology attributes to evolve from open-ended descriptions to more consistent reports. For instance,the vocabulary used to describe computational efficiency may vary across saliency developers and research areas based on typical com-puting resources and dataset sizes. As more saliency methods are documented and more evaluations are developed, we expect the saliency card attributes and their descriptions will evolve to bet-ter characterize saliency methods, facilitate cross-card comparison,and communicate with users.Saliency method developers were inspired to document their methods with saliency cards and hoped consistent and thorough documentation would increase method adoption. Good documenta-tion can make saliency methods easier to understand and use, “If you want people to use your method, your need to have them understand it.”[U8]. Currently, saliency method developers have to generate doc-umentation content that ranges from novel algorithmic decision and implications in the paper to implementation considerations in the public code repository. This process can feel unprincipled,so developers were excited to have a template that fully captured critical considerations. For example, when developing their saliency method, U8 documented their method’s computational efficiency and hyperparameter dependence in their code repository, explain-ing “We tried to make our documentation accessible to users. I tried todo some of this, but in an ad hoc way, and I didn’t hit all of these [at-tributes].” They looked forward to adding additional documentation and making a saliency card for their method.",291-4,Good documentation can make saliency methods easier to understand and use
