,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{author = {Deng, Wesley Hanwen and Yildirim, Nur and Chang, Monica and Eslami, Motahhare and Holstein, Kenneth and Madaio, Michael},
title = {Investigating Practices and Opportunities for Cross-Functional Collaboration around AI Fairness in Industry Practice},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,AddressingAIFairness,Perceived_Problem,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",worked on addressing AI fairness in their role. ,
10,BridgingRoles,Strategy,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evaluations.",
11,FocusOnModelOutput,Strategy,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",“mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” ,
12,NeedForTranslation,Perceived_Need,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",lack an understanding of how to translate their understanding into effective evaluations of fairness,
13,AIPractitioners,Agent,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc)",
14,DifferencesInAIFairnessUnderstanding,Perceived_Problem,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration.",
15,NotAwareOfAIHarms,Perceived_Need,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",“not aware of representational harms caused by AI systems at all.”,
16,InabilityToDiscussAIFairness,Perceived_Need,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","the topic of AI fairness, often struggled with what and how to discuss.",
17,AbstractTutorials,Perceived_Problem,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations.,
18,UserFacingPractitioners,Agent,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","user- and product- facing roles (e.g., UX designers and PMs) ",
19,Piggybacking,Strategy,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",“piggy-backing” as a tactic,
20,ChangeManagement,Strategy,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",“change management”—approaches to prepare for and support organizational changes.,
21,PreferenceForQuantitativeMeasures,Strategy,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",favors quantitative over qualitative methodologies ,
22,InvisibilityOfEfforts,Perceived_Problem,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",their efforts during the collaborations were often invisible to their team members or leadership.,
23,ExpectationOfShortEffort,Perceived_Problem,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement.",
24,DesireForBetterStrategies,Perceived_Need,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles.",
25,TechnicalPractitioners,Agent,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","technical roles on their teams (DS, MLE)",
26,ScaffoldingActivities,Artifact,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues:,
27,FairnessHackathons,Artifact,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications",
28,GlossaryOfFairnessMetrics,Artifact,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare.,
29,StakeholderDocumentation,Artifact,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization,
30,RacialBiasChecklists,Artifact,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work",
31,FairnessCoalitions,Artifact,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems,
32, , , , , ,
33, , , , , ,
34,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
35,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
36,AddressingAIFairness,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. "
37,BridgingRoles,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evaluations."
38,FocusOnModelOutput,constrainsAgent,TechnicalPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” "
39,NeedForTranslation,constrainsAgent,UserFacingPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2)but may lack an understanding of how to translate their understanding into effective evaluations of fairness"
40,AIPractitioners,hasProducedArtifact,ScaffoldingActivities,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues:
41,DifferencesInAIFairnessUnderstanding,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration."
42,NotAwareOfAIHarms,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",some of their coworkers were “not aware of representational harms caused by AI systems at all.”
43,InabilityToDiscussAIFairness,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss."
44,AIPractitioners,hasProducedArtifact,FairnessHackathons,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications"
45,AbstractTutorials,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations.
46,UserFacingPractitioners,hasProducedArtifact,GlossaryOfFairnessMetrics,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare.
47,UserFacingPractitioners,hasProducedArtifact,StakeholderDocumentation,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization"
48,Piggybacking,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fairness work
49,AIPractitioners,hasProducedArtifact,RacialBiasChecklists,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work"
50,AIPractitioners,hasProducedArtifact,FairnessCoalitions,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems
51,ChangeManagement,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” "
52,PreferenceForQuantitativeMeasures,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles"
53,InvisibilityOfEfforts,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12",participants shared that their efforts during the collaborations were often invisible to their team members or leadership.
54,ExpectationOfShortEffort,constrainsAgent,TechnicalPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement."
55,DesireForBetterStrategies,constrainsAgent,AIPractitioners,"We adopted a purposive sampling approach [20], with the aim of recruiting industry AI practitioners from diverse roles (e.g., data sci-entists, UX researchers, product managers, etc) who have worked on addressing AI fairness in their role. During the study, all participants self-reported that they had experience collaborating with other roles in their work on AI fairness. Table 1 provides an overview of participants' product areas and roles. Throughout the paper, we use[Acronym of the role][Role number] to refer to our participants and we use [W][Workshop session number] to refer to the workshop session participants attended.We recruited our participants through direct contacts at large technology companies, through recruitment posts on social media(e.g., Linkedin and Twitter), and snowball sampling from those par-ticipants. In total, 25 practitioners completed the recruitment screening form for our interview, of whom 18 met our recruitment criteria, responded to our interview study invitation, and participated in the study. In the end, 17 participants completed the interview study.We invited all interview participants to attend a workshop ses-sion. 6 out of 17 interview participants responded to our invitation and joined the workshops (W1 [DS2, DS4, UX5], W2 [DS1, MLE1,DS5]). We then sent another round of workshop recruitment to new participants, using a purposive sampling approach similar to the previous round (e.g., direct contacts and social media) and received 42 responses. In the end, 19 of those responded to our scheduling email, with 6 able to participate in the second round of workshops(W3 [PM3, UX6], W4 [SME1, DS6], W5 [DS1, SWE2, SME2])2. We Scheduled each workshop to include participants spanning different roles and organizations. Similar to prior work studying AI fairness with industry practitioners [27, 51, 73], we encountered a large drop-out rate, potentially due to the sensitivity of the topic of fairness. In addition, scheduling the workshops with participants from different roles and companies was constrained by practition-ers’ joint availability, further adding challenges to participation in the study.All participants were compensated at a rate of $35 per hour for their participation. In addition, for both interviews and workshops,we told participants that we would not ask them to reveal any confidential or personally identifying information about their colleagues and that we would anonymize all responses at the individual, team,and organization levels. Finally, participants were told that they were free to skip any questions they were uncomfortable answering,and were free to leave the workshop session at any time for any reason.  FINDINGS 4.1 Bridging Gaps in Understanding andEvaluation to Improve Collaboration We found that participants (spanning a range of formal roles) took on critical bridging roles by identifying and creating opportunities to foster their team’s learning about AI fairness, contextualizing abstract concepts and guidelines to make AI fairness work concrete,and aligning mismatched goals and metrics for fairness evalua-tions. Throughout this section, we highlight how participants use these bridging activities to attempt to overcome barriers to cross-functional collaboration around AI fairness.4.1.1 Bridging the gaps in incompatible disciplinary evaluationsaround AI fairness. We find that our participants take on bridging work to overcome tensions in the methods or metrics that various disciplines use to assess or measure fairness in AI systems, metrics which may be incompatible with each other. For example, participants in our study reported that technical roles on their teams (DS, MLE) tend to evaluate their models by “mainly focusing on the output of the models they built without thinking about how [these models] interact with real customers” (SWE2). In contrast, user- and product- facing roles (e.g., UX designers and PMs) often have a better understanding of “shareholders and customers’ concerns” (PM2) but may lack an understanding of how to translate their understanding into effective evaluations of fairness (e.g., in ways that respond to customers’ concerns).3 As a consequence, multiple participants mentioned that in order to facilitate communication about fairness among team members with diverse backgrounds, they needed to bridge the goals for fairness assessments between technical roles focused primarily on the model outputs (i.e., model-focused eval-uation) and user- and product- facing roles who tend to focus on biases and harms perceived by users (i.e., user-focused evaluation).Participants shared that they bridged these evaluation gaps by initiating and organizing meetings for cross-functional teams to align model-focused and user-focused fairness evaluations. For example, PM1 repurposed some of their regular team-level all-hands meetings, which were originally designed for team members to report on their work progress and goals, to “co-evaluation meetings”(PM1) for fairness issues. In particular, PM1 spent extra effort designing activities to scaffold technical roles and user-facing roles in understanding and aligning each others’ perspectives on evaluating fairness issues: “I will have the entire team together and have the failure mode effect analysis, we see what’s happening with the false positive rate and false negative rate of the model for our use cases and making sure that we always align on this [...] for every single step[in building the model], our team makes sure that there is a fairness requirement from the product team and there is a specific guidelines of implementation from the engineers.” (PM1). UX5 shared that they organized similar “co-evaluation sessions”(UX5) in their company, with the purpose to create spaces to understand the differences and similarities between the evaluation metrics for AI fairness that different roles employed. “In these sessions, I come up with hierarchies and frameworks whilst everybody else was talking talking talking [...] and sharing these artifacts in the moment [...] and showing what are the connections between different evaluations people were just talking about [...] and then people made the connections between the AI [models] and the product and they suddenly started to collaborate because they finally understood each other's goals [...] and started to use similar terminologies.” (UX5) However, participants shared that, while they attempted to bridge the gaps between model-facing and user-facing evaluation of AI fairness, the culture of AI development (broadly speaking) of-ten prioritizes quantitative over qualitative evaluation approaches[12, 34], making this bridging work around fairness evaluation less effective. In Section 4.2.2, we expand on practitioners’ strategies on navigating the mismatches between quantitative and qualitative evaluation approaches in organizations that disincentivize fairnesswork. 4.1.2 Creating collaborative processes and spaces to bridge gaps in understanding about AI fairness. We found that the incompatibil-ity between different teams was not limited to fairness evaluation methods and metrics. In fact, most participants shared that there were crucial differences in their understanding of AI fairness in general, which introduced challenges for effective collaboration. Without a common grounding of what AI fairness entails for the specific domain they were working in, “disagreements in terms of definitions of bias and fairness will affect how people use the [fairness] toolkits and all the downstream collaborations” (RS1). During the workshop (W1), UX5 told us that the conversations around fairness among their team members “stayed at a superficial level and went nowhere” when team members failed to align their own understandings of what “fairness” means with what it means to their users —in the context of their specific application. In another workshop(W5), SME2 shared in their “journey map” activity that they realized some of their coworkers were “not aware of representational harms caused by AI systems at all.” This made them realize the importance of “check[ing] each others’ understanding of [fairness] problems before just diving into the conversation around fixing the problems.”To bridge these gaps and inconsistencies in AI practitioners understanding of AI fairness, participants designed various collaborative activities that were intended to scaffold conversations among cross-functional team members. These activities range from small team design workshops to company-wide hackathons. For instance,as a UX designer working on image captioning and product rec-ommendation applications, UX1 held design workshops “similar to those workshops us designers often conduct with external clients and users” with their team members working on AI fairness. In these workshops, UX1 led the conversations with their team members working on different aspects of the products to explore what “a fair product recommendation system mean[s]” (UX1). However, participants mentioned that team members, especially those who were new to the topic of AI fairness, often struggled with what and how to discuss. To overcome these communication barriers, participants drew on toolkits to scaffold the design process. For example, UX4 incorporated a toolkit they often used in user research sessions called “ideation cards”—a deck of cards including a hundred questions concerning the value and ethics around product design—to facilitate cross-role design workshops and better engage team members in asking questions probing how their AI products might cause fairness issues under different scenarios. UX4 shared that ideation cards helped their team to have constructive debates around the meaning of being fair to different stakeholders who might be affected by the AI service.To foster participation and engagement in conversations around AI fairness, some participants described how they tailored collaborative activities to the skills and knowledge of specific roles. For Example, UX2, PM2, and UX5 mentioned hosting company-wide hackathons, a familiar and engaging format for technical roles like engineers, to better engage engineers in critically examining their understanding of AI fairness when building AI applications . These hackathons often happened “at the problem formulation stage when the team starts to work on algorithmic fairness or transparency relevant topics” (UX2), serving as a chance for team members to begin the conversation around fairness issues of their AI products and learn more about each others’ perspectives on fairness. During W1,while discussing the ""tailwinds"" of their current collaboration, UX5 brought up how the hackathon event they designed for building more responsible AI helped engage diverse roles in conversation about AI fairness. During this workshop session, other participants in technical roles (DS2 and DS4) expressed their interest in participating in similar events and implementing this format in their own organizations to potentially promote collaboration on AI fairness initiatives across roles.4.1.3 Developing educational resources and documentation to support understanding about fairness. Complementing the efforts for building a common ground between team members about AI fair-ness, participants also reported developing documentation and other resources (in addition to hosting workshops) that aimed to increase their team’s knowledge about AI fairness and facilitate conversa-tions about this topic in collaborations. In particular, participants with strong technical backgrounds (DS1, MLE1, RS2, DS6) who report being familiar with the state of the art of technical fair AIresearch literature shared that they created accessible educational materials to help their team members learn about technical AI fair-ness concepts (e.g., fairness metrics, bias mitigation algorithms,and their limitations). For example, DS6 created a guidebook covering “common fairness metrics, rationales for some bias mitigation algorithms, and also different types of harms that could be caused by algorithms.”MLE1 worked in a small start-up company offering consulting services for building responsible AI and self-reported being the most knowledgeable team member around AI fairness in their growing engineering team. After repeatedly getting similar questions from colleagues around “AI fairness concepts and confusions when reading some paper they saw from FAccT,” MLE1 decided to use their spare time5 to develop “shareable documentations and materials” to provide their team members with a “free crash course[s]” that explained basic AI fairness concepts using accessible language. This documentation then became standard onboarding materials for new employees in their company to learn more about fairness in AI.4.1.4 Translating and contextualizing abstract AI fairness concepts in concrete terms. Participants in our study repeatedly mentioned that publicly available AI fairness guidelines and tutorials (and the fairness concepts presented in them) are “usually too abstract” (DS4) for tackling the AI fairness issues in their organizations. As a result,when attempting to follow these AI fairness guidelines and tutorials,technical roles often found them “not always relevant to the task at hand” (MLE2) and struggled to “understand which fairness metrics or techniques to use for the specific application” (DS6). To this end, we observed translation and contextualization work between technical roles and user-facing roles to better understand what fairness means for their specific domain and use case or user populations.Participants in technical roles (DS, SWE) — who are often responsible for directly conducting algorithmic fairness analyses— often proactively initiated collaboration with user-facing roles (PM, customer services) to better contextualize abstract fairness metrics used in analysis in real world scenarios. For instance, as a data scientist, DS5 worked closely with UX researchers to “build a glossary to contextualize the abstract concept of fairness metrics” such as equalized odds and demographic parity using real-world scenarios in their AI services for healthcare. Similarly, SWE1 reported that they went beyond assessing model fairness through fairness metrics, trying to contextualize the analysis through conversations with customer service managers. Participants shared that the contextualizing process sensitized data scientists about “how their model might interact with users in an unintended, harmfulway” (DS5), helping technical roles learn more about other roles perspectives to better inform their fairness analysis.Furthermore, we found that multiple user- and product-facing roles (UX, PM) often spent extra effort contextualizing abstract fairness guidelines to their actual practices for other team members,in order to “get the entire teams on the same page around how the models might cause fairness issues when they are interacting with users” (PM1). In particular, PM1 annotated the AI fairness guide-lines developed by their company (a technology company with over 25,000 employees) with concrete examples and prompts to explore,for instance, “what does this [guide]line entail for the sentiment analysis product [they] were developing.“ During W1, UX5 shared with DS2 and DS4 that they developed “modules that characterize how different guidelines could be represented back in concrete examples... to help colleagues understand what these [AI fairness] guidelines would mean in an actual solution.” For example, based on their user experience research, UX5 documented the stakeholders who directly interacted with their system — and those who might not directly interact with the systems but who might still be affected — in a shareable document that was available across multiple AI team in their organization. In the workshop, DS2 and DS4 both agreed with UX5 that these modules they created were extremely valuable In section 4.3, we discuss the consequences of AI team members needing to use their spare time to develop resources to bridge disciplinary gaps.to facilitate communications among team members in ways that attend to the real-world context for fairness issues. 4.2 Piggybacking as a Tactic for Collaboration under Organizational Constraints In parallel to “bridging,” we find that participants employed “piggy-backing” as a tactic to push fairness work forward in organizations that might not otherwise provide resources or incentives for fair-ness work. The “piggybacking” observed in our study took multiple forms: a) some participants piggybacked on related institutional processes, such as privacy impact assessments, to get buy-in for fairness work, b) some also positioned their work within the “quantification”culture of AI development to better communicate with technical roles on their teams. However, when sharing how these piggyback-ing tactics may have enabled them to conduct fairness work on cross-functional teams, participants also expressed their concerns around the limitations and compromised nature of these tactics. 4.2.1 Piggybacking on institutionalized procedures. To address challenges in AI fairness work, some participants (DS3, PM1, UX4,SME1) shared their strategies around piggybacking on organization-wide mandatory privacy-related procedures (e.g., questionnaires,checklists) in order to raise awareness about AI fairness and put AI fairness efforts into practice. For example, DS3 developed a set of checklists to help the AI product teams reflect and assess if their AI features contained potential racial biases that might harm their users, but ran into challenges to incorporate these artifacts into the current day to day AI work. However, since “team members and leadership are extremely cognizant about privacy” at DS3’s com-pany, the privacy team in the company had already developed a questionnaire called a “privacy impact assessment,” containing 10 privacy-related questions for all product teams to complete before launching any AI features or products. DS3 shared stories about how they built allyship with the privacy team and later piggybacked on the “privacy impact assessment” to promote their AI fairness effort: “After multiple times getting rejected by our company to implement our fairness checklists, my teammate had this brilliant idea which was: What if we piggyback onto an existing process that already exists?. . . all we did was add an extra set of questions specifically concerning machine learning fairness to the privacy impact assessment. . .the beauty of that is that we don’t have to persuade people to fill out this form since they already have to fill out the larger privacy impact assessment in order to launch their products or features.” (DS3)Furthermore, DS3 told us that adding fairness-related questions to the privacy impact assessment helped to bring the awareness ofAI fairness to DS3’s entire organization, as there were increasing amount of “people from other teams reaching out and saying that they want to work on fairness too.” By doing so, participants were also able to build a coalition of team members and develop a network of allies within the organization who were committed to advancing fairness in the company’s AI systems. Similarly, during the interview, PM1 brought up the concept of “change management”—approaches to prepare for and support organizational changes. PM1 told us that “change management is very, very difficult for all of software practice, but especially with responsible AI, when we need to go the extra mile and explain why this is so important.” Therefore, PM1 would “always start by adding little things to [their] privacy practice instead of creating an entirely new fairness thing... we don’t have to evangelize about how important privacy is.”However, both DS3 and PM1 shared their desire to implement stand-alone fairness assessment procedures “eventually when there is more buy-in for AI fairness” (PM1). DS3 shared that their current efforts around piggybacking on the privacy team might not be sustainable, and they wanted higher-level leadership to allocate more resources for AI fairness work. Furthermore, DS3 brought up the need for further exploring the trade-offs and complementarity between privacy and fairness work instead of smuggling fairness in with privacy assessments.4.2.2 Piggybacking on the quantification culture of AI development. As mentioned in Section 4.1.1, various disciplines may value different evaluation goals and methods for AI fairness. Current development culture still largely favors quantitative over qualitative methodologies and forms of evidence [6, 12, 34]. As a result,many participants reported drawing on quantitative approaches in order to overcome communication barriers with team members in technical roles. For example, PM1 shared strategies around using a quantitative score to fit their AI fairness work into existing AI workmetric systems and the “numerical culture of AI modeling work” : “It is always hard to convince the teams across organizations to accept something new, unless it is something that they are already familiar with [...] so we started using a scale of 1 to 10. If your [model’s] feature isn't even showing any explainability or analysis around fairness than the score will be low. Engineers, they just don’t like low scores”(PM1). In other words, PM1 used these scores to make the value of working on AI fairness directly relevant to the data scientists on the team. During the workshops, another product manager, PM3, told us that when they are communicating with data scientists, “using [a fairness toolkit] to calculate numbers like demographic disparity made it much more straightforward for communicating with other data scientists about the impact of working on fairness.” Within the same workshop, UX6 concurred with PM3 by sharing that “having quantifiable fairness related scores,” was a commonly used strategy that worked well when collaborating with data scientists, and shared with us that they often change their communication strategy to include significant amount of quantitative data when working with data scientists on AI fairness. However, many participants were aware of the potential pitfalls of using “scores” and “percentages” in AI fairness work. For example, PM2 told us that even though they would always “make sure to show something quantitative” when communicating with engineers and data scientists as a “trick to communicate,” they don’t believe quantitative data is “necessarily the best way to represent the concept of fairness.” During the workshop (W3), while PM3 shared relying on quantification dramatically helped them with commu-nicating with the AI development team, they also acknowledged that the numbers produced using fairness toolkits served as an(often inaccurate) proxy of the actual fairness harms that might be caused by their AI products, “essentially losing a lot of nuances of fairness” (PM3). As a result, while creating their “ideal journeymap,” PM3 and others in their workshop shared the desire for bet-ter processes to help navigate the current AI development culture around prioritizing quantification to appropriately address socio-technical challenges that may require drawing on and integrating both quantitative and qualitative evidence of (un)fairness.4.3 Invisible Labor and Burdens inCross-functional CollaborationWhile working towards more effective cross-functional col-laboration, participants shared that their efforts during the collaborations were often invisible to their team members or leadership. Worse, sometimes other team members would hold unrealistic expectations of AI fairness work, creating additional burden and frustration for participants.Participants reported that one reason their “bridging” and“piggybacking” efforts encountered difficulties was that other team members did not always understand what AI fairness work actually involves. This contributed to under-recognition and unrealistic expectations. For example, during the workshop (W2), DS1, MLE1 andDS5, all from different organizations, had discussed how creating educational documentation or accessible visualizations to bridgethe knowledge gaps among participants required substantial effort.MLE1 mentioned that updating the educational documentation to keep up with AI fairness research was also extremely time consum-ing, sometimes requiring several rounds of major iteration even within a week. The value of such extra efforts, however, was usually overlooked by their team leadership. As DS5 concurred with DS1and MLE1 during the workshop session, “teams don’t understand the amount of work that goes into doing AI fairness work. It’s not quite as simple as pulling from a table and doing a statistical analysis.You have to put thoughts into making intentional analysis choices and think broadly about the socio-technical nature of AI fairness. (DS5) Furthermore, participants shared that other team members, often technical roles who just started working on AI fairness, often held unrealistic expectations that collaboration around fairness would be a one-off effort, rather than an iterative, thoughtful process requiring long-term engagement. For example, UX5 shared that some colleagues expected design workshops around AI fairness (see4.1.2) to be a “one time thing” rather than a recurring activity—for instance, a data scientist on UX5’s team asked “did[n’t] we already do that last time and get the answer?” when UX5 scheduled another design workshop around AI fairness. In addition, since AI fairness efforts were not reflected in most organizations' annual employee evaluations or other organizational incentives, individuals who were motivated to do fairness work often felt responsible for “holding the entire team accountable for the fairness work” (MLE2), rather than having that accountability located in, for instance, “ethics owners” [60] or other organizational leadership. Similarly, MLE1 shared that they felt “frustrated by the attitude of [others] dodging responsibility for fairness work.” How-ever, MLE1 reported that they often felt uncomfortable, as a woman of color, “giv[ing] everyone a lecture on why fairness should be prior-itized."" As a result, “bridging” work often falls onto a small number of team members who are self-motivated to do the work, yet who are not incentivized or supported in sustaining their efforts long term [cf. 58]. As RS2 shared, “engineers and data scientists just kinda leave the conversation behind, walking out of the meetings [...], and I have to spend more time to do most of the work to make sure the team can crystallize certain principles or certain best practices.” As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles. When creating the “ideal journeymap” during the workshop (W5), SWE2 and SME2 wanted team members to continuously address AI fairness across the lifecycle of AI development, instead of “discussing fairness almost like an empty gesture at the beginning of each season” (SME2). In the same workshop, SME2 expressed their desire for organizations to culti-vate a shared understanding among different roles regarding the continuous efforts required to address fairness in AI, as well as each user's unique contributions to collaboratively building more fair and responsible AI systems. Furthermore, participants repeatedly brought up the importance of changing the “education and train-ing team members received” (PM3) to set up a common grounding and expectations among members of their organization about the iterative, socio-technical nature of AI fairness practice. ","707, 708-12","As a result, participants across roles all desired better strategies to help team members better understand the iterative nature of fairness work, as well as to better advocate for and incentivize long-term collaboration across roles."
