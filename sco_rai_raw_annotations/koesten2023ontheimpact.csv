,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, , , , ,
3,~~~ Annotator Details ~~~~,"{koesten2023ontheimpact,
    author = {Laura Koesten and Timothée Schmude and Torsten Möller and Sebastian Tschiatschek},
    title = {On the Impact of Explanations on Understanding of Algorithmic Decision-Making},
    year = 2023
}", , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-07, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,AMSAlgorithm,Agent,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,The AMS algorithm,
10,CategoryAssignment,Artifact,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,assign job-seekers to one out of four categories,
11,PersonalAttributes,Artifact,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"personal attributes, such as age, gender, and education",
12,ConfirmationAndCorrection,Artifact,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,confirmed or corrected,
13,PEAEmployee,Agent,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,an employee of the Public Employment Agency,
14,RegSupportMeasures,Strategy,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,regular support measures,
15,JobSeekers,Agent,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,Job-seekers,
16,Teenagers,Agent,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,Teenagers,
17,Disabled,Agent,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,people with disabilities,
18,Over50,Agent,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,people over 50,
19,AdditionalSupportMeasures,Strategy,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,additional support measures,
20,Training,Strategy,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well as a representation of the local job market.",
21,JobSeekerData,Artifact,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,several years of job-seekers’ data,
22,AlgorithmicBias,Perceived_Problem,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,Biases in the algorithm,
23,DetrimentalEffects,Perceived_Problem,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,detrimental effects on the support and treatment that job-seekers would receive,
24, , , , , ,
25, , , , , ,
26,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
27,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
28,AMSAlgorithm,hasProducedArtifact,CategoryAssignment,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,The AMS algorithm was constructed to assign job-seekers to one out of four categories
29,PEAEmployee,hasProducedArtifact,ConfirmationAndCorrection,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,confirmed or corrected by an employee of the Public Employment Agency
30,JobSeekers,hasProducedArtifact,JobSeekerData,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education"
31,RegSupportMeasures,constrainsAgent,JobSeekers,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,Job-seekers would receive regular support measures to improve their chances of finding employment
32,AdditionalSupportMeasures,constrainsAgent,Teenagers,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"Teenagers, people with disabilities, and people over 50 would receive additional support measures"
33,AdditionalSupportMeasures,constrainsAgent,Disabled,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"Teenagers, people with disabilities, and people over 50 would receive additional support measures"
34,AdditionalSupportMeasures,constrainsAgent,Over50,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"Teenagers, people with disabilities, and people over 50 would receive additional support measures"
35,Training,constrainsAgent,AMSAlgorithm,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"The algorithm’s model was trained on trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well as a representation of the local job market."
36,AlgorithmicBias,constrainsAgent,AMSAlgorithm,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,The weighting of personal features in the algorithm’s classification can be considered biased
37,DetrimentalEffects,constrainsAgent,JobSeekers,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,detrimental effects on the support and treatment that job-seekers would receive
38,CategoryAssignment,influencesPrecept,RegSupportMeasures,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility"
39,CategoryAssignment,influencesPrecept,AdditionalSupportMeasures,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,"Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility"
40,JobSeekerData,influencesPrecept,AlgorithmicBias,"The AMS algorithm was constructed to assign job-seekers to one out of four categories (""high"", ""medium"", or ""low"" employment chances, plus special cases), depending on their personal attributes, such as age, gender, and education. Every prediction would be confirmed or corrected by an employee of the Public Employment Agency. The groups of employability were defined as follows:• ""Medium"": Job-seekers would receive regular support measures to improve their chances of finding employment.• ""High"": Job-seekers were expected to find new employment quickly and would receive fewer support measures.• ""Low"": Job-seekers were expected to require more support and would be referred to another facility.• Other: Teenagers, people with disabilities, and people over 50 would receive additional support measures independent of their employability scoring [3].The algorithm’s model was trained on several years of job-seekers’ data, mainly consisting of personal attributes (features): gender, age, citizenship, education, impairment, obligations of care(only women), occupational group, prior occupations; as well asa representation of the local job market.5 People with similar personal attributes would be grouped and compared to the ""standard group"" [24] – young men with secondary school education – to be assigned a short-term and long-term employability score. For further details please refer to the supplementary material and Allhutteret al. [3] Biases in the algorithm. The weighting of personal features in the algorithm’s classification can be considered biased in that several attributes such as gender and nationality led to decreased employ-ability predictions [24]. However, these biases would in theory lead to higher support measures for job-seekers possessing these attributes, in effect supporting those that were potentially disadvantaged in the job market [25]. Nonetheless, Allhutter et al. [2]point out how the algorithm’s practical implementation could have detrimental effects on the support and treatment that job-seekers would receive. In summary, the AMS algorithm is an example of how personal and systemic considerations can lead to value con-flicts and ethical dilemmas in algorithmic decision-making, which is why we chose it as a suitable example for the study. Explanation and task examples were chosen such that these issues were brought to the participants’ attention, without however preempting any judgment or value statement.",962,The weighting of personal features in the algorithm’s classification can be considered biased
