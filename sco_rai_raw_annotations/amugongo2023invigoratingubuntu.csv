,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{amugongo2023invigoratingubuntu,
    author = {Lameck Mbangula Amugongo and Caitlin C. Corrigan and Nicola J. Bidwell},
    title = {Invigorating Ubuntu Ethics in AI for healthcare: Enabling equitable care},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-09, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,DisparitiesDueToStereotypes,Perceived_Problem,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,"historical, socioeconomic disparities which are a result of implicit and explicit social stereotypes, such as about rac",
10,Bias,Perceived_Problem,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices.,
11,UnfairPredictions,Artifact,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,inaccurate or unfair predictions,
12,EconomicSkew,Perceived_Problem,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations,
13,UbuntuEmphasisOnDiversity,Causal_Theory,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,"Ubuntu ethics [...] emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences.",
14,LocalPopulations,Agent,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,local populations,
15,Marginalization,Perceived_Problem,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,they are part of the marginalised group.,
16,AIHealthcareModels,Agent,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,AI models in healthcare,
17,LackOfRepresentativeness,Perceived_Need,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,the training data is not representative of the population ,
18,DataCuration,Artifact,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,data collected,
19,VisuallyImapredPeople,Agent,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,Visually impaired individuals,
20, , , , , ,
21, , , , , ,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,DisparitiesDueToStereotypes,constrainsAgent,AIHealthcareModels,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,"AI models in healthcare mirror the historical, socioeconomic disparities which are a result of implicit and explicit social stereotypes, such as about race."
26,Bias,constrainsAgent,AIHealthcareModels,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,"In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices."
27,AIHealthcareModels,hasProducedArtifact,UnfairPredictions,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,the model may make inaccurate or unfair predictions
28,UnfairPredictions,reflectsPrecept,LackOfRepresentativeness,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,"If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions"
29,EconomicSkew,constrainsAgent,AIHealthcareModels,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions
30,UbuntuEmphasisOnDiversity,constrainsAgent,AIHealthcareModels,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,"One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences."
31,LocalPopulations,hasProducedArtifact,DataCuration,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,"This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. "
32,Marginalization,constrainsAgent,VisuallyImapredPeople,"Fairness, in the context of AI in healthcare, emphasises the need for AI systems to treat all individuals equally while accounting for the existing privilege and future empowerment. Recent studies report differences in patient diagnosis [74], treatment [66] and cost of ac-cessing health care [62]. Wiens, Creary and Sjoding [84] argue that AI models in healthcare mirror the historical, socioeconomic dis-parities which are a result of implicit and explicit social stereotypes,such as about race. In the AI development lifecycle, algorithmic inequity can arise as a result of bias in the data used to train the algorithm and choices about what and how data is collected as well as algorithmic choices. If the training data is not representative of the population it will be used on, the model may make inaccurate or unfair predictions; for example, if a model is trained on data about primarily white pa-tients it may not accurately predict outcomes for patients of Black,Asian and minority ethnic races. At the same time, Viljoen [81]explains that socially advantaged groups tend to collect data that benefits them, which results in collections that produce great risks for marginalized people. For instance, data collected from racially diverse patients that include characteristics that are more prevalent in economically fortunate populations but exclude characteristics that are more prevalent in economically disadvantaged populations will result in models that make inegalitarian predictions. Further, AIalgorithms represent the world as a collection of simplified metrics that ignore important parts of personal experience; for instance, Pendse et al. [65] explain that standardised medical categories used in mental health care do not include local forms of distress marginalize minority forms by design. In advocating against limiting data to dominant clinical constructs and scales, and for representing a rich variety of relations, they refer to how traditional healers in Zambia integrate complex but localised socio-psychological rela-tions in successfully treating bodily symptoms. This illustrates the importance of widely inclusive data if the benefits of AI systems in healthcare, such as personalised treatment regimes, are available for every patient [65]. The ethic of fairness in AI is, however, not only about the data used to train models but also about data governance; thus, Viljoen argues that societal harm is caused because data relations will inherently enact or amplify social relations [81]. One way that Ubuntu ethics can improve fairness in AI in health-care is by emphasising the need to collect diverse data from different regions, races and socioeconomic groups and use that data in ways that are sensitive to their diverse needs and experiences. This data should be collected through community engagement, involve diverse communities and local expert clinicians in assessing how data represent local needs and experiences and determining how data is used and reused, and involve local populations in supervising the machine learning. Recognising that people are interconnected and a person’s well-being is tied to the well-being of others, regardless of their economic or societal situation, means that decision-making underpinned byUbuntu-informed ethics actively seeks the perspectives and experiences of marginalised individuals and groups. By promoting the virtues of empathy, inclusivity and mutual support, the impor-tance of balance and harmony [52] and obligations to be friendly with others, Ubuntu ethics can foster active listening and mutual  understanding between groups which can shape both the ongo-ing evolution of ethical AI for healthcare and more inclusive data governance. Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group. As such, it is important for developers of visual-based applications to prioritise the inclusion of visually impaired individuals in the development process of such apps.Finally, a recent study highlighted that fairness needs to account for empowerment [65]. We believe that as a philosophy with an im-plied obligation for people to be friendly or harmonious with others,Ubuntu-inspired values will ensure that AI systems in healthcare account for empowerment and, therefore, make their beneficiaries well-off, such as by improving personalised care for every patient.",587-8,"Consider, for example, a recent review that shows the increasing number of visual-based applications for food recognition to support healthy eating [2]. Visually impaired individuals are of-ten overlooked in the development of these applications and, thus,in the context of Ubuntu, they are part of the marginalised group."
