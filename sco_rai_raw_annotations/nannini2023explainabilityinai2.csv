,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{nannini2023explainabilityinai,
    author = {Luca Nannini and Agathe Balayn and Adam Leon Smith},
    title = {Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-08, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,EuropeanComission,Agent,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EC,
10,DraftEUAIAct,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EU AI Act draft,
11,AIRegulation,Perceived_Problem,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,a proportional risk-based approach to regulating AI systems,
12,CategoriesOfRisk,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms",
13,DefinitionsOfConcepts,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"definitions of risks assessments, transparency, and human oversight",
14,TaxonomyOfAIActors,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,taxonomy of AI actors,
15,DataQualityRequirements,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,data quality requirements,
16,TechnicalDocumentation,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,technical documentation,
17,InstructionsforUse,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems.",
18,IMCOLIBECommittee,Agent,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,IMCO-LIBE committee,
19,ChangesToExplainabilityRegulations,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,initial changes to the regulations surrounding AI explainability during inspections,
20,JURICommittee,Agent,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,JURI committee,
21,EmphasisOnExplanability,Strategy,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,stressed significant explainability mentions,
22,Opinion,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"an Opinion [103] addressing risk mitigation, user interaction and rights.",
23,RejectionOfOpinions,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,not substantiated in the final draft,
24,PermRepsCommittee,Agent,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,Permanent Representatives Committee,
25,RevisionsAimedAtTraceability,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems.,
26,DocumentationRequirements,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,strengthened technical documentation requirements,
27,TransparencyWeakneingAmendments,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,final amendments weakened concepts of transparency and explainability,
28,ShiftToOversight,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands,
29,ProposalForLiabilityDirective,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems.,
30,AIDeployer,Agent,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,AI deployer,
31,EUCourt,Agent,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EU Court,
32,OrderToDisclose,Artifact,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,order to disclose proportional evidence necessary,
33,OrderToDisclose,Perceived_Need,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,order to disclose proportional evidence necessary,
34,TradeSecretPreservation,Goal,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,preserve the confidentiality of trade secrets,
35, , , , , ,
36,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
37,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
38,EuropeanComission,hasProducedArtifact,DraftEUAIAct,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EC’s committees and their intense work on reinforcing the EU AI Act draft
39,EuropeanComission,hasProducedArtifact,CategoriesOfRisk,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms."
40,EuropeanComission,hasProducedArtifact,DefinitionsOfConcepts,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EC’s committees and their intense work [...] Alongside definitions of risks assessments, transparency, and human oversight"
41,EuropeanComission,hasProducedArtifact,TaxonomyOfAIActors,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EC’s committees and their intense work [...] the approach develops a taxonomy of AI actors
42,EuropeanComission,hasProducedArtifact,DataQualityRequirements,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EC’s committees and their intense work [...] the approach develops [...]  data quality requirements
43,EuropeanComission,hasProducedArtifact,TechnicalDocumentation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EC’s committees and their intense work [...] The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems"
44,EuropeanComission,hasProducedArtifact,InstructionsforUse,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EC’s committees and their intense work [...] the approach develops [...] technical documentation
45,IMCOLIBECommittee,hasProducedArtifact,ChangesToExplainabilityRegulations,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections
46,JURICommittee,hasProducedArtifact,Opinion,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights"
47,PermRepsCommittee,hasProducedArtifact,RevisionsAimedAtTraceability,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,the final draft as presented by the Permanent Representatives Committee  [...] The changes include the introduction of indirect measures such as strengthened technical documentation requirements
48,PermRepsCommittee,hasProducedArtifact,DocumentationRequirements,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"the final draft as presented by the Permanent Representatives Committee  [...] final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AI systems "
49,EUCourt,hasProducedArtifact,OrderToDisclose,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary "
50,AIRegulation,constrainsAgent,EuropeanComission,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems
51,EmphasisOnExplanability,constrainsAgent,JURICommittee,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,The JURI committee stressed significant explainability
52,OrderToDisclose,constrainsAgent,AIDeployer,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,the AI deployer through an order to disclose proportional evidence necessary
53,TradeSecretPreservation,constrainsAgent,AIDeployer,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets"
54,DraftEUAIAct,reflectsPrecept,AIRegulation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems
55,CategoriesOfRisk,reflectsPrecept,AIRegulation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms"
56,DefinitionsOfConcepts,reflectsPrecept,AIRegulation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight"
57,TaxonomyOfAIActors,reflectsPrecept,AIRegulation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors "
58,DataQualityRequirements,reflectsPrecept,AIRegulation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements"
59,TechnicalDocumentation,reflectsPrecept,AIRegulation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation"
60,InstructionsforUse,reflectsPrecept,AIRegulation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems."
61,Opinion,reflectsPrecept,EmphasisOnExplanability,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"Initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions "
62,OrderToDisclose,reflectsPrecept,TradeSecretPreservation,"The path inaugurated by the HLEG and the White Paper informed EC’s committees and their intense work on reinforcing the EU AI Act draft [120] proposed in April 2021. TheAct adopts a proportional risk-based approach to regulating AI systems, dividing them into three categories of risk with corresponding limitations, transparency requirements, and oversight mechanisms. Alongside definitions of risks assessments, transparency, and human oversight, the approach develops a taxonomy of AI actors within data quality requirements and technical documentation. The original draft included provisions for interpretability, specifically Article 13, requiring a sufficient degree of transparency mechanisms for high-risk systems and the attachment of instructions for use containing relevant, accessible, and understandable information to users regarding the characteristics, capabilities, and limitations of performance of these systems. The Act has undergone several revisions as a result of the EC work, prior to inter-institutional negotiations being held in 2023. In April 2022, the IMCO-LIBE committee proposed initial changes to the regulations surrounding AI explainability during inspections [102]. In September 2022, the JURI committee stressed significant explainability mentions with an Opinion [103] addressing risk mitigation, user interaction and rights. Yet, those revisions were not substantiated in the final draft as presented by the Permanent Representatives Committee on November 25, 2022 [122], including several revisions aimed at ensuring the traceability and interpretability of high-risk AI systems. The changes include the introduction of indirect measures such as strengthened technical documentation requirements and instructions for use with illustrative examples, as well as guidelines for collecting and interpreting system logs. Article 13, originally stipulating a sufficient and appropriate degree of transparency for high-risk AI systems, was altered to include specific instructions for use and metrics related to the behavior of the system on cer-tain groups of people and in relation to the sociotechnical context of adoption. Article 14(4)(c) was removed from the provision to have end users interpreting the characteristics of a system, intro-ducing it not as a requirement but as a possibility of consulting them through interpretation methods. Overall, final amendments weakened concepts of transparency and explainability, shifting the focus on ensuring the traceability and oversight of high-risk AIsystems rather than empowering end-users’ explanation demands. As a final mention, in October 2022, the EC advanced a Proposal for an AI liability directive [121] to define accountability alloca-tion within non-contractual civil liability for damage involving AIsystems. Whenever an allegedly damaged claimant suspects non-compliance of an AI system output, then it is required to establish a causal link as a burden of proof. Art. 4(2)(b) details that if an AIsystem is considered high-risk, opaque, and complex (i.e., not al-lowing transparency requirements of the AI Act’s Art.13), therefore explainability is mandated from an EU court not within the system (e.g., through XAI methods) but to the AI deployer through an order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets, as in Recital 16, it is said that the AI Act does not specify any right for an injured person to access that information",1201-2,"order to disclose proportional evidence necessary (e.g., logs, documen-tation, and datasets). This is done to preserve the confidentiality of trade secrets"
