,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{radiya-dixit2023asociotechnicalaudit, author = {Evani Radiya-Dixit and Gina Neff}, title = {A Sociotechnical Audit: Assessing Police Use of Facial Recognition}, year = 2023 }", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-07, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,LFRDeployment,Artifact,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,operational trial deployments of live facial recognition (LFR),
10,MPS,Agent,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,Metropolitan Police Service (MPS),
11,EssexResearchers,Agent,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,University of Essex researchers,
12,Report,Artifact,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,Their report ,
13,ConclusionOfUnlawfulness,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,"concludes that the trials would likely ""be held unlawful if challenged before the courts",
14,LackOfGuidance,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,absence of clear guidance,
15,Researchers,Agent,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,our,
16,AuditRevealingConcerns,Artifact,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,sociotechnical audit revealed additional concerns,
17,LackOfDemographicData,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,"they did not record the demographic breakdown for engagements,stop and searches, and arrests",
18,InabilityToEvaluate,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,This makes it hard to evaluate whether LFR perpetuates racial profiling,
19,InternalEvaluation,Artifact,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,internal evaluation,
20,LackOfDisclosure,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,did not disclose the results,
21,LackOfTransparency,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,lack of transparency,
22,OutsideEntities,Agent,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,outside entities,
23,ConcentrationOfPower,Strategy,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,power is concentrated,
24,Evaluation,Artifact,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,evaluation,
25,NIST,Agent,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,National Institute of Standards & Technology,
26,PoliceEthicsPanel,Agent,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,London Policing Ethics Panel,
27,DelayedOversight,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,transparent oversight did not begin until several deployments,
28,AdvisoryStatus,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,advisory,
29,LackOfExperts,Perceived_Problem,"The next case is the operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS) from August 2016 to February 2019 [69]. We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts""given the absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary ina democratic society"" as required by human rights law [45]. Our sociotechnical audit revealed additional concerns related to discrim-ination and oversight, as illustrated in Table 2.While MPS published some demographic data in their results,they did not record the demographic breakdown for engagements,stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling.There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency makes it hard for outside entities to assess the comprehensiveness of the evaluation.As we discuss in Section 7.3, this obscurity reveals how power is concentrated in the police and where change needs to be made.Since the LFR trial has ended, MPS has pointed to an evalua-tion undertaken by the National Institute of Standards & Technol-ogy [49]. However, citing this evaluation can be misleading: the evaluation shows high accuracy, but it was conducted with high-quality standardized images rather than wild images on which LFRwas used. In fact, for MPS’ trial, only 19% of LFR matches were verifiably correct. This performance is especially concerning given that the same technology used by MPS misidentified and led to wrongful arrests of Black men in the U.S. [2, 32, 51].Regarding oversight, MPS engaged with the London PolicingEthics Panel. However, transparent oversight did not begin until several deployments rather than starting from the concept stage of the trial. Even though MPS responded to the panel’s recommen-dations, the panel was advisory and MPS was not required to act upon the recommendations. There were also no experts in human rights, equality, or data protection on the panel, even though this is crucial for the oversight of technologies such as LFR.",1340-41,"no experts in human rights, equality, or data protection on the panel",
30, , , , , ,
31,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
32,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
33,MPS,hasProducedArtifact,LFRDeployment,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,operational trial deployments of live facial recognition (LFR) conducted by the Metropolitan Police Service (MPS)
34,MPS,hasProducedArtifact,InternalEvaluation,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,MPS conducted an internal evaluation
35,EssexResearchers,hasProducedArtifact,Report,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report
36,Researchers,hasProducedArtifact,AuditRevealingConcerns,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,Our sociotechnical audit revealed additional concerns related to discrimination and oversight
37,NIST,hasProducedArtifact,Evaluation,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,evaluation undertaken by the National Institute of Standards & Technology
38,ConclusionOfUnlawfulness,constrainsAgent,Researchers,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts"
39,LackOfGuidance,constrainsAgent,MPS,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"We built upon a study conducted by University of Essex researchers on the human rights compliance of these trials. Their report concludes that the trials would likely ""be held unlawful if challenged before the courts"" given the absence of clear guidance"
40,LackOfDemographicData,constrainsAgent,MPS,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"they [MPS] did not record the demographic breakdown for engagements, stop and searches, and arrests resulting from the use of LFR"
41,InabilityToEvaluate,constrainsAgent,MPS,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,This makes it hard to evaluate whether LFR perpetuates racial profiling
42,LackOfDisclosure,constrainsAgent,MPS,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,MPS conducted an internal evaluation but did not disclose the results
43,LackOfTransparency,constrainsAgent,MPS,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,MPS conducted an internal evaluation but did not disclose the results. This lack of transparency
44,ConcentrationOfPower,constrainsAgent,MPS,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,power is concentrated in the police
45,DelayedOversight,constrainsAgent,PoliceEthicsPanel,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"MPS engaged with the London Policing Ethics Panel. However, transparent oversight did not begin until several deployments"
46,AdvisoryStatus,constrainsAgent,PoliceEthicsPanel,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"Even though MPS responded to the panel’s recommen-dations, the panel was advisory"
47,LackOfExperts,constrainsAgent,PoliceEthicsPanel,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"There were also no experts in human rights, equality, or data protection on the panel"
48,LFRDeployment,influencesPrecept,LackOfGuidance,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"the trials would likely ""be held unlawful if challenged before the courts"" given the absence of clear guidance"
49,LFRDeployment,influencesPrecept,LackOfDemographicData,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"they did not record the demographic breakdown for engagements, stop and searches, and arrests resulting from the use of LFR"
50,LFRDeployment,influencesPrecept,LackOfDisclosure,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results.
51,LFRDeployment,influencesPrecept,LackOfTransparency,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,There was also no published evaluation of racial or gender bias in the LFR software. MPS conducted an internal evaluation but did not disclose the results. This lack of transparency
52,Report,reflectsPrecept,LackOfGuidance,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"absence of clear guidance on who was included in a watchlist and the failure to establish that LFR was ""necessary in a democratic society"""
53,AuditRevealingConcerns,reflectsPrecept,LackOfDemographicData,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"Our sociotechnical audit revealed additional concerns related to discrimination and oversight, as illustrated in Table 2. While MPS published some demographic data in their results, they did not record the demographic breakdown for engagements, stop and searches, and arrests"
54,AuditRevealingConcerns,reflectsPrecept,InabilityToEvaluate,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-41,"Our sociotechnical audit revealed additional concerns related to discrimination and oversight, as illustrated in Table 2. While MPS published some demographic data in their results, they did not record the demographic breakdown for engagements, stop and searches, and arrests resulting from the use of LFR. This makes it hard to evaluate whether LFR perpetuates racial profiling."
55,InternalEvaluation,reflectsPrecept,LackOfDisclosure,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,MPS conducted an internal evaluation but did not disclose the resultS
56,InternalEvaluation,reflectsPrecept,LackOfTransparency,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-41,MPS conducted an internal evaluation but did not disclose the results. This lack of transparency
