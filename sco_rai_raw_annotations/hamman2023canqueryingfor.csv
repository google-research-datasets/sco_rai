,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{hamman2023canqueryingfor,
    author = {Faisal Hamman and Jiahao Chen and Sanghamitra Dutta},
    title = {Can Querying for Bias Leak Protected Attributes?},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Regulations,Perceived_Problem,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",Existing regulations,
10,DeveloperComplainceSeparation,Strategy,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes).,
11,ModelDevelopers,Agent,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",model developers,
12,QueriesToCompliance,Artifact,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367","simply querying for fairness metrics, such as, statistical parity and equalized odds",
13,SingleQueryAttributeLearning,Strategy,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",trategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query.,
14,Researchers,Agent,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",we,
15,LoanApplicants,Agent,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",48842 loan applicants.,
16,Institutions,Agent,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",institutions,
17,ProtectedAttributeLeakage,Perceived_Problem,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",can leak the protected attributes of individuals to the model developers.,
18,AttributeConceal,Artifact,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367","Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism.",
19,AdultDataset,Artifact,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",The Adult dataset,
20, , , , , ,
21, , , , , ,
22, , , , , ,
23, , , , , ,
24,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
25,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
26,Regulations,constrainsAgent,ModelDevelopers,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367","Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. "
27,DeveloperComplainceSeparation,constrainsAgent,Institutions,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes).
28,ModelDevelopers,hasProducedArtifact,QueriesToCompliance,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics.
29,QueriesToCompliance,influencesPrecept,ProtectedAttributeLeakage,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367","simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers."
30,SingleQueryAttributeLearning,constrainsAgent,ModelDevelopers,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query.
31,Researchers,hasProducedArtifact,AttributeConceal,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367","we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism."
32,LoanApplicants,hasProducedArtifact,AdultDataset,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from O (ğ‘ğ‘˜ log(ğ‘›/ğ‘ğ‘˜ )) queries when ğ‘ğ‘˜ â‰ª ğ‘› using techniques from compressed sensing (ğ‘› is the size of the test dataset and ğ‘ğ‘˜ is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individu-als? To address this supposed violation of regulations and privacy,we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters). [...] The Adult dataset has 14 attributes for 48842 loan applicants. The Classification task is to predict whether an individualâ€™s income is more or less that 50K [25]. The feature ""race"" is chosen as the protected attribute. This feature is excluded from training and only used for statistical parity evaluation. We restrict ourselves to onlyWhite and Black (binary) with the latter being relatively sparse(10.4%). We compare Attribute-Conceal with a naive differential privacy technique, Laplace mechanism. We experiment with different test sizes and show our results in Figure 2 and Table 1.Given an input, our base model â„0 (Â·) outputs a probability value between 0 and 1. For the other ğ‘š models, we add a small noise samples from Uniform(âˆ’0.1, 0.1) distribution to each output of the base model.We observe that the accuracy of the other models is quite close to the original. We created 40 models from the base model: they had a mean accuracy of 86.23% and a standard deviation of 0.2583.Interestingly, our experiments demonstrate that with the uniform noise, we can still recover the protected attributes with far fewer models than the full-rank case. As shown in Figure 2, we are able to recover all the protected attributes using ğ‘š = 40 models.Notice that, this is roughly O (ğ‘0 log(ğ‘›/ğ‘0)).Our recovery of protected attributes is based on the values of the Ì„ğ‘  vector in Algorithm 2. Ideally, it should be 0 if ğ‘ğ‘— = 1 and 1/ğ‘1 +1/ğ‘0 if ğ‘ğ‘— = 0. In our practical implementation, the compressed sensing solution is not always exact but still good enough to inferthe protected attribute. Due to this, we use a threshold between 0 and 1/ğ‘1 + 1/ğ‘0 to identify the protected attribute [...]  This work highlights a major concern with fairness assessments in scenarios where protected attributes such as gender or race cannot be accessed during model training. Showing that simply querying for fairness metrics can leak sensitive information to model developers raises important questions about the ethical implications of these assessments. As a remedy, we also propose a novel tech-nique, Attribute-Conceal, which achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query.The results of this study have important implications for regula-tions and privacy in the field of algorithmic fairness and provide a new approach to protect the sensitive information of individuals in fairness assessments. This also provides a potential resolution to the continuing debate about whether protected attributes should be used in training. Future research could look into expanding the framework to include other fairness metrics or incorporating these techniques into training or post-processing to directly reduce bias without leaking protected attributes.Our current approach assumes that both model developers and the compliance (or auditing) team work with the same test set.However, this might not hold true in every context. The compli-ance/auditing team may choose to use a different test set. However,note that a different test set may not adequately represent the true training distribution, which could potentially affect generalization.","1358, 1366, 1367",The Adult dataset has 14 attributes for 48842 loan applicants.
