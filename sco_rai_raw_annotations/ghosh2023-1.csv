,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{ghosh2023, author = {Ghosh, Bishwamittra and Basu, Debabrota and Meel, Kuldeep S.}, title = {â€œHow Biased Are Your Features?â€: Computing Fairness Influence Functions with Global Sensitivity Analysis}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,FairXplainer,Agent,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,FairXplainer ,
10,LowerEstimationError,Artifact,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,less estimation error than SHAPE while approximating statistical parity using FIFs,
11,LowerExecutionTime,Artifact,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,"incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers.",
12,BetterFeatureIdentification,Artifact,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,demonstrates to be a better choice for identifying features influencing group fairness metrics than SHAP.,
13,ComparisonWithSHAP,Perceived_Problem,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,"We compare [...] with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred as SHAP",
14,MoreAccurateInterpretation,Artifact,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,"a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP",
15,Intersectionality,Causal_Theory,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-7,intersectional influences,
16, , , , , ,
17, , , , , ,
18, , , , , ,
19, , , , , ,
20,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
21,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
22,ComparisonWithSHAP,constrainsAgent,FairXplainer,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,"We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred as SHAP"
23,FairXplainer,hasProducedArtifact,LowerEstimationError,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,FairXplainer yields less estimation error than SHAPE while approximating statistical parity using FIFs
24,FairXplainer,hasProducedArtifact,LowerExecutionTime,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,"FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers."
25,FairXplainer,hasProducedArtifact,BetterFeatureIdentification,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics than SHAP.
26,Intersectionality,constrainsAgent,FairXplainer,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,"By quantifying both individual and intersectional influences of features, FairXplainer"
27,FairXplainer,hasProducedArtifact,MoreAccurateInterpretation,"In this section, we perform an empirical evaluation of FairXplainer. Particularly, we discuss the experimental setup, the objectives of experiments, and experimental results.Experimental Setup. We implement a prototype of FairXplainerin Python (version 3.7.6). To estimate FIFs, we leverage and modifythe â€˜HDMRâ€™ module in SALib library [22] based on global sensitivityanalysis. In experiments, we consider four widely studied datasets from fairness literature, namely German-credit [11], Titanic (https://www.kaggle.com/c/titanic), COMPAS [2], and Adult [12]. We de-ploy Scikit-learn [38] to learn different classifiers: logistic regression classifier, support vector machine (SVM), neural network, and deci-sion tree with 5-fold cross-validation. In experiments, we specifyFairXplainer to compute intersectional influences up to the secondorder (ğœ† = 2). While applying cubic-spline based local regressionin FairXplainer, we set ğœ, the number of spline intervals to 6. We compare FairXplainer with the existing Shapley-valued based FIF computational framework (https://shorturl.at/iqtuX), referred asSHAP [32]. For both FairXplainer and SHAP, we set a timeout of300 seconds for estimating FIFs. In addition, we deploy FairXplaineralong with a fairness-enhancing algorithm [25] and a fairness at-tack [48] algorithm, and analyze the effect of these algorithms on8Typically, the runtime complexity of smoothing oracles, particularly of cubic splines,is linear with respect to ğ‘›, i.e. the number of samples [50].the FIFs and the resultant fairness metric. In the following, wediscuss the objectives of our empirical study.(1) Performance: How accurate and computationally effi-cient FairXplainer and SHAP are in approximating the biasof a classifier based on estimated FIFs?(2) Functionality: How do FIFs estimated by FairXplainer andSHAP correlate with the impact a fairness interventionstrategy on features?(3) Granularity of explanation: How effective are the intersec-tional FIFs in comparison with the individual FIFs while tracing the sources of bias?(4) Application: How do FIFs quantify the impact of applying different fairness enhancing algorithms, i.e. affirmative ac-tions, and fairness attacks, i.e. punitive actions?In summary, (1) we observe that FairXplainer yields less estimation error than SHAPE while approximating statistical parity usingFIFs. FairXplainer incurs lower execution time, i.e. better efficiency in computing individual FIFs than SHAPE while also enabling compu-tation of intersectional FIFs for real-world datasets and classifiers. (2) While considering a fairness intervention, i.e. change in bias due to the omission of a feature, FIFs estimated by FairXplainerhave higher correlation with increase/decrease in bias due to the intervention than SHAPE. Thus, FairXplainer demonstrates to be a better choice for identifying features influencing group fairness metrics thanS HAP. (3) By quantifying both individual and intersectional influ-ences of features, FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP. (4) Finally, as an application of the FIFformulation, FairXplainer detects the effects of the affirmative and punitive actions on the bias of a classifier and the corresponding tensions between different subsets of features. Here, we elaborate on experimental results, and defer additional experiments such as applying FairXplainer on other fairness metrics: equalized odds and predictive parity, and an ablation study of hyper-parameters: maxi-mum order of intersectionality ğœ† and spline intervals ğœ to Appendix Of the extended paper accurate Approximation of Bias with FIFs. We compare Fair eXplainer with SHAPE in estimating statistical parity by summing all FIFs, as dictated by the decomposability property (Theorem 2). To our best knowledge, the ground truth of FIF is not known for real-world datasets and classifiers. As such, we cannot compare the accuracy of FairXplainer and SHAP directly on the estimated FIFs. Since both methods follow the decomposability property, one way to comparethem is to test the accuracy of the sum of estimated FIFs yieldingbias/unfairness, as the ground truth of bias of a classifier can be exactly calculated for a given dataset [4]. We compute estimated error by taking the absolute difference between the exact and es-timated values of statistical parity, and present median results inTable 1.In general, FairXplainer achieves less estimation with ğœ† = 2 thanwith ğœ† = 1 in all datasets and classifiers. This implies that combining intersectional FIFs with individual FIFs captures bias more accu-rately than the individual FIFs alone. In each dataset, FairXplainer (ğœ† = 2) demonstrates less estimation error than SHAP in all classi-fiers except in decision trees, denoting that GSA based approachFairXplainer is more accurate in approximating group fairness met-rics through FIFs than the local explainer SHAP. In decision trees,FairXplainerâ€”which is model-agnostic in methodologyâ€”with ğœ† = 2 often demonstrates a comparable accuracy with SHAP, especially the optimized tree-based explanation counterpart of SHAP [33]. In the context of neural networks, SHAP, particularly Kernel-SHAP,often fails to estimate FIFs within the provided time-limit, whileFairXplainer with ğœ† = 2 yields highly accurate estimates (median estimation error between 0 to 0.053). Therefore, we conclude thatFairXplainer is more accurate in estimating statistical parity usingFIFs than SHAP. Execution Time: FairXplainer vs. SHAP. We compare the execu-tion time of FairXplainer vs. SHAP in a cactus plot in Figure 2,where a point (ğ‘¥, ğ‘¦) denotes that a method computes FIFs of ğ‘¦many fairness instances within ğ‘¥ seconds. We consider 480 fairnessinstances from 4 datasets constituting 24 distinct combinations of sensitive features, 4 classifiers, and 5 cross-validation folds. In Fig-ure 2, FairXplainer with ğœ† = 1 is faster than ğœ† = 2. For example,within 10 seconds, FairXplainer with ğœ† = 1 solves 443 instances vs. 41 instances with ğœ† = 2. In addition, FairXplainer with ğœ† = 1 solves all 480 instances compared to 360 instances solved by SHAP.Thus, FairXplainer with ğœ† = 1 demonstrates higher efficiency in estimating individual FIFs compared to SHAP. While estimating intersectional FIFs, FairXplainer also demonstrates its practical ap-plicability by solving 350 instances within 160 seconds. Therefore,FairXplainer demonstrates computational efficiency in explaining group fairness metrics of real-world datasets and classifiers.FIFs under Fairness Intervention. We consider a fairness intervention strategy to hide the impact of an individual feature on a classifier and record the correlation between fairness improve-ment/reduction of the intervened classifier with the FIF of the feature. Our intervention strategy of modifying the classifier is dif-ferent than [10], where the dataset is modified by replacing features with random values. In particular, we intervene a logistic regression classifier by setting the coefficient to zero for the corresponding feature. Intuitively, when the coefficient becomes zero for a feature,the prediction of the classifier may become independent on the feature; thereby, the bias of the classifier may also be independent on the conditional variances of the feature for different sensitive groups. As a result, a feature with a positive FIF value (i.e. increases bias) is likely to decrease bias under the intervention, and vice versa. n Figure 3, we report Pearsonâ€™s correlation coefficient between the difference in bias (statistical parity) due to intervention and the FIFs of features in COMPAS and Adult datasets, where features are sorted in descending order of their absolute FIFs estimated byFairXplainer and SHAP. In Fair eXplainer, the correlation coefficient generally decreases with an increase of top influential features,denoting that features with higher absolute FIFs highly correlatewith bias-differences. SHAP, in contrast, demonstrates less correla-tion, specially for the top most influential features. Therefore, FIF sestimated by FairXplainer shows the potential of being deployed to design improved fairness algorithms in future.",144-6,"FairXplainer leads to a more accurate and granular interpretation of the sources of bias, which is absent in earlier bias explaining methods like SHAP"
