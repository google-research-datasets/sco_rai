,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{liu2023groupfairnesswithout,
    author = {David Liu and Virginie Do and Nicolas Usunier and Maximilian Nickel},
    title = {Group fairness without demographics using social networks},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-10, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,UnfavorableTreatment,Perceived_Problem,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",unfavorable treatment ,
10,Privacy,Perceived_Need,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",privacy,
11,Intersectionality,Perceived_Need,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",intersectionality,
12,Bias,Perceived_Problem,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",unforseen bias,
13,Researchers,Agent,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",we,
14,LastFMAsia,Artifact,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",the Lastfm-Asia dataset,
15,Individuals,Agent,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",individuals,
16,GroupFairnessMethods,Agent,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",group fairness,
17,GroupFreeFairnessMeasure,Artifact,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440","“group-free"" measure of fairness ",
18,CountryHomophily,Causal_Theory,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",the network exhibits strong homophily on country,
19, , , , , ,
20, , , , , ,
21,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
22,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
23,UnfavorableTreatment,constrainsAgent,Individuals,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440","unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability."
24,Privacy,constrainsAgent,GroupFairnessMethods,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440","the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. "
25,Intersectionality,constrainsAgent,GroupFairnessMethods,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440","the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. "
26,Bias,constrainsAgent,GroupFairnessMethods,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440","the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. '"
27,Researchers,hasProducedArtifact,GroupFreeFairnessMeasure,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440","we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks"
28,LastFMAsia,reflectsPrecept,CountryHomophily,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on ac-cess to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, an unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering anyform of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. [...] Our proposed group-free group fairness measure is adaptable fora variety of tasks as an objective or a constraint. To demonstrate its versatility, we apply the measure to three diverse tasks: node classification, maximizing information access, and recommender systems. In all experiments, we leverage datasets that provide both asocial network and individual sensitive attribute labels, henceforth referred to as “ground-truth"" labels. We use the ground-truth labels to show that our kernel-based approach, which utilizes only the network, does indeed lower inequality among these classes. Further,for the maximizing information access and recommender system tasks, we compare our results against the baseline of inferring group memberships via community detection instead of our group-free approach. [...] On the Lastfm-Asia dataset, our GroupFreeKernel method achieves trade-offs in-between the GroupAware approach, which requires group labels, and the Individual fairness approach. As expected, enforcing equality across all individual users in-stead of groups of users incurs a large cost for total user utility. Our kernel approach is able to mitigate this cost while reducing between-group inequality, without the need for group labels. OurGroupFreeKernel obtains similar results to the Louvain commu-nity detection baseline. On this particular social network, the Lou-vain algorithm is very good at identifying the ground-truth groups because the network exhibits strong homophily on country. Al-though our continuous kernel does not perform a hard assignment from users to communities, and hence remains group-free, it still obtains results that are on par with Louvain. On the Deezer-Europe dataset, both methods based on the social network, i.e. the Louvain community detection baseline and our GroupFreeKernel approach, obtain poor trade-offs between user utility and inequality. They are no better than simply min-imizing individual-level inequalities (Individual) and very far below the results obtained when knowing the gender group la-bels (GroupAware). This is because the underlying network is too sparse to extract any information from it: users make very few connections on Deezer and mostly use it to listen to music. This Stresses the importance of network structure for our method (and any network-based method). Nonetheless, given this disassortative network, our method does also not degrade performance compared to the individual-level baseline.","1432, 1436-7, 1439, 1440",the Lastfm-Asia dataset [...]  the network exhibits strong homophily on country
