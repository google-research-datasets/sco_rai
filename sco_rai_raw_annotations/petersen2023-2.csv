,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{petersen2023, author = {Petersen, Eike and Ganz, Melanie and Holm, Sune and Feragen, Aasa},
title = {On (Assessing) the Fairness of Risk Score Models},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,MajorDepressiveDisorder,Perceived_Problem,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,Major Depressive Disorder (MDD),
10,HealthPlayers,Agent,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,public authorities and health insurance companies,
11,DanishPeople,Agent,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,Danish [...] the national population aged 15 years or older per the 31st of December From 2000–2018,
12,Researchers,Agent,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,We,
13,DanishNationalRegistryDatabase,Artifact,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,national registry database that includes demographic and healthcare status information ,
14,GlobalPeople,Agent,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,globally,
15,MLBasedPrioritizationExploration,Artifact,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,"exploring ML-based healthcare resource prioritization,in mental health and other domains",
16,MDDDiagnosisModel,Artifact,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,model to predict MDD diagnosis from demographic information,
17,GreaterReliability,Goal,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,more reliable statements regarding potential disparities can be made.,
18, , , , , ,
19,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
20,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
21,MajorDepressiveDisorder,constrainsAgent,GlobalPeople,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally
22,HealthPlayers,hasProducedArtifact,MLBasedPrioritizationExploration,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,"public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains"
23,DanishPeople,hasProducedArtifact,DanishNationalRegistryDatabase,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,data from a Danish national registry database that includes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018
24,Researchers,hasProducedArtifact,MDDDiagnosisModel,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,We train a model to predict MDD diagnosis from demographic information
25,DanishNationalRegistryDatabase,influencesPrecept,GreaterReliability,"Major Depressive Disorder (MDD) is considered the most burden-some disease in Europe [96] and the leading cause of disability globally [97]. To address this challenge, depression risk assessments have been proposed for early detection and prevention [28, 67, 94]. At the same time, public authorities and health insurance companies are actively exploring ML-based healthcare resource prioritization,in mental health and other domains [45, 74, 81]. In this case study,we aim to investigate potential algorithmic fairness challenges that arise when using a depression risk assessment to decide who gets access to limited healthcare resources, and who does not. We use data from a Danish national registry database that in-cludes demographic and healthcare status information of the na-tional population aged 15 years or older per the 31st of December From 2000–2018. For each year, we include all subjects diagnosed by a medical practitioner with MDD (ICD codes DF32 and DF33) within that year as cases. Control subjects were sampled without replacement from those not diagnosed with MDD that year, alive at year end, and not previously selected as cases. Two equally sized cohorts without subject overlap were constructed this way, one for training, and one for validation and testing, both balanced between cases and controls and containing approximately 240,000 subjectseach. The latter set was split into validation (1/3) and test (2/3)sets. We train a model to predict MDD diagnosis from demographic information; a complete list of predictors and sensitive variables can be found in the supplementary material. We analyze model performance in groups with a test set size of at least 2,000 subjects.Figure 4 shows the results of our analyses. Overall model per-formance is again reasonable (AUROC > 0.7) and especially overall calibration is very good (DRMSCE < 0.01). Moreover, since this dataset is much larger, more reliable statements regarding potential disparities can be made. Some groups are clearly better calibrated than others; the groups that are poorer calibrated tend to be small.There are also large differences in discriminative ability, with AU-ROCs ranging from 0.6 to 0.75 in different groups. (We did not con-sider UPRG here since the dataset is perfectly outcome-balanced.)This is also reflected in measures of under-representation in the selected set: subjects from groups with higher AUROCs tend to receive more extreme risk estimates, resulting in them being selected earlier compared to subjects in groups with lower AUROCs.",822,"since this dataset is much larger, more reliable statements regarding potential disparities can be made."
