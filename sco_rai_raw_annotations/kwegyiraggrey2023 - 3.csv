,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{kwegyireaggrey2023, author = {Kwegyir-Aggrey, Kweku and Gerchick, Marissa and Mohan, Malika and Horowitz, Aaron and Venkatasubramanian, Suresh},
title = {The Misuse of AUC: What High Impact Risk Assessment Gets Wrong},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,SchoolDistricts,Agent,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,schools and school districts,
10,EarlyWarningSystems,Agent,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,“early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future.,
11,AreaUnderCurve,Strategy,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,AUC,
12,ImbalancedSample,Artifact,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,the sample used [...] for several of the outcomes is imbalanced ,
13,ErrorWeighing,Strategy,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,the importance of weighing the costs of different kinds of errors based on the context in question,
14,AUCForModelSelection,Strategy,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,AUC was used for model selection ,
15,RacialBias,Perceived_Problem,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,much higher false positive rate for Black and Latino students compared to white students,
16,LackOfTraining,Perceived_Need,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,little training about the tool ,
17,EarlyWarningSystemDeployment,Artifact,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future.,
18,StudentRiskCategorization,Artifact,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,"students into “risk categories” accordingly, which are shown to school administrators",
19,Wisconsin,Agent,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) ,
20,Stigmatization,Artifact,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,(often false) “high-risk” labels stigmatized students,
21,Educators,Agent,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,educators,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,SchoolDistricts,hasProducedArtifact,EarlyWarningSystemDeployment,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future."
26,EarlyWarningSystems,hasProducedArtifact,StudentRiskCategorization,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,"These systems place students into “risk categories” accordingly, which are shown to school administrators"
27,AreaUnderCurve,constrainsAgent,EarlyWarningSystems,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,AUC is commonly used in developing and validating EWS
28,ImbalancedSample,influencesPrecept,AreaUnderCurve,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,"Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC"
29,ErrorWeighing,constrainsAgent,Wisconsin,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,"Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question"
30,AUCForModelSelection,constrainsAgent,Wisconsin,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,AUC was used for model selection for DEWS
31,RacialBias,constrainsAgent,EarlyWarningSystems,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,DEWS had a much higher false positive rate for Black and Latino students compared to white students
32,EarlyWarningSystems,hasProducedArtifact,Stigmatization,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,tool’s (often false) “high-risk” labels stigmatized students
33,LackOfTraining,constrainsAgent,Educators,"Around the country, schools and school districts use “early warning systems” (EWS) to analyze student data and attempt to make predictions about how students will perform academically or whether they will drop out of school in the future. These systems place stu-dents into “risk categories” accordingly, which are shown to school administrators [6, 10, 11, 22, 97]. In 2016, these systems – which typically use historical student data such as academic performance(e.g., grades, test scores), behavioral and attendance records, and sometimes external data – were estimated to be in use in more than half of all public high schools in the United States [97], and recent reporting indicates at least eight state education agencies in the U.S.have built or are building algorithmic EWS [38]. As with the exam-ples from the child welfare system and the criminal legal system, AUC is commonly used in developing and validating EWS. But this reliance on AUC and emerging evidence about the effects of EWSraise complex questions about real-world performance, the policy choices embedded in threshold-setting, and impacts for students lives. A 2020 report authored by the research organization Mathematica and prepared for the Institute of Education Sciences (IES) – norm of the Department of Education – provides insight into howAUC is used in the creation and deployment of EWS [16]. Usingdata from Allegheny County, Pennsylvania, the report analyzed machine learning approaches for building EWS that predict the likelihood of several different outcomes including: low GPA, course failure, chronic absenteeism, and suspension from school. In se-lecting and comparing potential models, the report relied on AUCas a measure of accuracy, writing that the “predictive model risk scores identify at-risk students with a moderate-to-high level of accuracy,” based on AUC values, which ranged from .75-.92 across the different outcomes and grade levels of students analyzed [19,p. 2]. The report characterizes an AUC of .70 or higher as indicative of “a strong model fit,” referencing Rice and Harris [80], using AUC to validate model performance at the population level, and also asa benchmark to validate performance across different racial groups.One key issue affecting the interpretation of AUC values in this context is class imbalance. Specifically, the sample used to compute AUC for several of the outcomes is imbalanced (for example, in the sample, suspension occurred less than 10% of the time and chronic absenteeism occurred less than 25% of the time) [17, p. 8], raising concerns about interpretations of AUC as discussed in Sections 4.1and 4.3. See [55] for further discussion of issues of class imbalance in EWS.Beyond AUC, the report also considers other performance met-rics in its analysis [16]. Bruch et al. [16] and a related report also written by Mathematica [19] discuss various considerations for finding and setting thresholds to convert model predictions into risk categories for EWS, noting that the determination of cutoff should be a context-dependent inquiry for individual school districts. Cattell and Bruch [19] provides an example guide for use in this decision process [19, p. 12], which highlights the myriad policy implications of determining thresholds, including consideration of costs of false positives and false negatives.In a 2015 research paper, the developer of an EWS currently in use in schools in Wisconsin – the Wisconsin Dropout Early Warning System (DEWS) – made similar recommendations, including about the importance of weighing the costs of different kinds of errors based on the context in question [53]. AUC was used for model selection for DEWS [53], and recent research uses AUC results to characterize the predictive value of the tool [74]. But that same research also demonstrates the limits of AUC and other metrics as meaningful indicators of real world outcomes – Perdomo et al. [74]say they found that even though DEWS was accurate at predict-ing individual outcomes (using AUC and other metrics), it had no impact on graduation rates. Perdomo et al. [74]’s findings suggest that targeting individual students with interventions without ad-dressing the structural issues and inequities that lead students to out is an ineffective approach. Investigative reporting about the tool also found that DEWS had a much higher false positive rate for Black and Latino students compared to white students, that the tool’s (often false) “high-risk” labels stigmatized students, and that educators received little training about the tool [38].The impact of DEWS demonstrates that the metrics and measures used to build and measure models, including AUC, may present an incomplete picture of algorithmic performance in the real world. In deployment, understanding the nature of the services or interventions informed by model predictions, as well as the context in which the tool operates, is essential. For example,in educational contexts, Black students are disproportionately punished and subjected to violence in schools compared to white students [50, 51, 88, 95, 96], students of color are disproportionately pushed into the criminal legal system [48, 95, 96], and surveillance technologies are increasingly being used to track and monitor students, often without their awareness or ability to opt out [54]. As Educators around the country use these kinds of algorithms, contextis a critical part of model evaluation and development (if algorith-mic models are to be used at all). Table 4 includes more examples of EWS, and see [10, 12, 13, 40] for further analysis of these tools.",1578,educators received little training about the tool
