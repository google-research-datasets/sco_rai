,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{radensky2023, author = {Radensky, Marissa and S\'{e}guin, Julie Anne and Lim, Jang Soo and Olson, Kristen and Geiger, Robert}, title = {“I Think You Might Like This”: Exploring Effects of Confidence Signal Patterns on Trust in and Reliance on Conversational Recommender Systems}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,LackOfKnowledgeAboutUserResponseToRecommenderConfidenceStatements,Perceived_Problem,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs.,
10,ConfidenceSignal,Artifact,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",an accurate confidence signal ,
11,Prototype,Agent,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",the prototype,
12,InterestInConfidenceSignal,Perceived_Need,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",interest in having some version of a dynamic confidence signal in their interviews.,
13,DesireForReasons,Perceived_Need,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800","preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation",
14,IgnoranceOfConfidenceLevels,Perceived_Need,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",reported not noticing or paying attention to the confidence levels,
15,IgnoranceOfWording,Perceived_Need,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",don’t really notice the changing in the words at all because it’s one word changing,
16,ResponsivenessToConfidenceLevels,Causal_Theory,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",the confidence levels did or would impact their expectations,
17,LowConfidenceAsSignalOfNovelty,Causal_Theory,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",assumed that recommendations with lower confidence signals should more likely be new to them.,
18,StaticConfidenceAsSignalOfQuality,Causal_Theory,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",the static confidence signals gave them higher expectations,
19,ExpectationOfExploration,Causal_Theory,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations,
20,AnticipationOfBias,Causal_Theory,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",feel like some biases could come out,
21,Researchers,Agent,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",we,
22,IncreaseInTrust,Other_Precept,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",greatest increase in trust-related metrics,
23,Recommendation,Artifact,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800","respond with a recommendation, including its title, artist, and YouTube embedded video",
24,Participants,Agent,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",participants,
25,RecognitionOfSystemAbility,Causal_Theory,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",recognizing the system’s current ability,
26, , , , , ,
27, , , , , ,
28,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
29,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
30,LackOfKnowledgeAboutUserResponseToRecommenderConfidenceStatements,constrainsAgent,Researchers,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800","no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confiden"
31,ConfidenceSignal,reflectsPrecept,IncreaseInTrust,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance
32,Prototype,hasProducedArtifact,Recommendation,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800","the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video."
33,Prototype,hasProducedArtifact,ConfidenceSignal,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",the prototype would first also provide a confidence signal.
34,InterestInConfidenceSignal,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",Twenty-four of the 30 participants expressed interest in having some version of a dynamic confidence signal in their interviews.
35,DesireForReasons,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800","The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation"
36,IgnoranceOfConfidenceLevels,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels
37,IgnoranceOfWording,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure
38,ResponsivenessToConfidenceLevels,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",15 expressed that the confidence levels did or would impact their expectations
39,LowConfidenceAsSignalOfNovelty,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800","Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them."
40,StaticConfidenceAsSignalOfQuality,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal
41,ConfidenceSignal,influencesPrecept,RecognitionOfSystemAbility,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800",the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users.
42,ExpectationOfExploration,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800","given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations"
43,AnticipationOfBias,constrainsAgent,Participants,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like. . . ” or “I think you will like. . . ,”but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard Of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence,which is accurate, random, always low, or always high. Through Semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and per-ceived anthropomorphism. [...] Thirty English-speaking adults age 18 to 24 years (17 women, 12 men, 1 non-binary) were recruited from a US-wide participant pool by L&E Research and received $75 for one hour of their time. We controlled for age given its potential influence on the set of cold-start recommendations provided to participants (see Section 3.3). Participants did not work in human-computer interaction,linguistics, technology, psychology, marketing, or the government.All stream music, listen to music at least a few times per week, and seek out music for playlists at least a few times per month. A summary of the wording and color-coding of confidence signals for each condition and recommendation batch is shown in Figure1. The low-confidence and high-confidence conditions used static wording and no color-coding, while the random-confidence and accurate-confidence conditions used dynamic wording and color-coding to convey three confidence levels. The confidence signals in the random condition were randomized across the 18 recommenda-tions so that there were 6 of each confidence level. The accurate confidence signals were the only ones aligned with actual recom-mendation confidence. How the actual confidence was determined as described in Section 3.3.Although the differences among “might,” “would,” and “will” in conveying confidence are supported by prior work [20], we also ran two online surveys using Google Surveys to confirm the semantic hierarchy (without color-coding). Comparing “might” and “would”,59.5% of 401 respondents thought that “would” was more confi-dent, and comparing “would” and “will”, 57.7% of 403 respondents thought that “will” was more confident. These results are weighted according to age, gender, and geography for the United States Internet population. Though the surveys supported the semantic hierarchy, the differences were somewhat narrow. Furthermore, i pilot study sessions without color-coding, we found that partic-ipants often did not notice the confidence levels in the dynamic confidence conditions. We thus decided to add traffic-light color-coding to emphasize the confidence levels: red for low confidence,yellow for medium, and green for high [34]. The specific color hues were selected to be accessible for people who are color-blind. We used YouTube Music to obtain personalized recommendations of low, medium, and high confidence for each participant prior to their study session. In their screening, participants selected the most common situation in which they would listen to a playlist for30 minutes or more. In addition, they provided one genre, three artists, and two songs per artist that they would want to hear in that situation. For each participant, we made a new Gmail account with which to access YouTube Music. For the low-confidence songs, we used songs recommended before the user had interacted with the system. In order for these songs to be the same for all participants,participants needed to be around the same age because Gmail requires a birth date, which could influence recommendations. Wetherefore recruited participants in the age range of 18 to 24 and set Gmail’s field of birth date to the average birth date of that age range. To obtain the medium-confidence songs, we utilized partici-pants’ preferred genre, and to acquire the high-confidence songs,we utilized their preferred artists and songs. For more detail on the recommendation selection, see Appendix A.After the study, we ran a two-sample paired sign test (rather thana Wilcoxon signed-rank test due to the violation of the symmetric distribution assumption) and found a significant difference between participants' average final ratings for the low-confidence recom-mendations versus the high-confidence recommendations (p<.05).Wilcoxon signed-rank tests also showed a significant difference in average final ratings between the medium- and high-confidence recommendations (p<.05) and a trend in the difference between the low- and medium-confidence recommendations’ average final rat-ings (p<.1). Thus, there appears to have been a substantial difference in recommendation relevance across the confidence levels. Also,per non-baseline condition, we did not observe any trend in the overall difference between participants’ final ratings for their base-line versus non-baseline condition. This suggests that the groundtruth relevance of the recommendations was relatively consistent across participants’ two conditions. The study procedure is outlined in Figure 2. Each one-hour study session was conducted and recorded over Google Meet. To start, participants were reminded of the contents of their consent form and guided through a tutorial. They were told that they may or may not receive a comment from the system alongside recommendations.Those assigned to the accurate or random condition were also shown the 3-level confidence scale associated with the comment.Participants were not informed as to whether or not the system would update based on feedback.Participants then received the baseline and one other condition in randomized order. The non-baseline condition’s confidence was always high (5 participants), always low (5 participants), accurate(10 participants), or random (10 participants). The Wizard of Oz CRSprototype for each condition was presented through Google Slides.Each condition consisted of 18 recommendation chats. In every condition, the actual confidence was low for the first 6 recommen-dations, medium for the next 6, and high for the last 6. (We note that,due to time constraints, P2 skipped 2 chats per recommendation batch in the baseline condition.) For each recommendation, a chart within a smartphone outline appeared (Figure 1). The chat always started with a pre-filled request from the participant: “What’s a good song to add to my [situation provided in screening] playlist?”In the baseline condition, the prototype would then respond with a recommendation, including its title, artist, and YouTube embedded video. In the other conditions, the prototype would first also provide a confidence signal. Once the recommendation appeared, the first 15 seconds of the song played. Participants then rated their agreement with the 7-point Likert-type statement “I would enjoy listening to this song while [in Situation X].” Next, they rated how familiar they were with the song. Their options were “not at all familiar,” “somewhat familiar,” and “very familiar.” If the song was not very familiar, they heard more and, after hearing at least 15 and up to 45 more seconds of the song, provided a final 7-pointLikert-type recommendation rating. Participants listened to at least30 seconds before providing their final rating because a review ofSpotify music listeners showed that most users decide whether or not to skip a song within this time [29]. We did not collect final ratings for songs with which participants were very familiar, asa prior work indicated that results for a movie recommendation explanation were substantially different when users had already seen the movie [3].After the ninth and eighteenth recommendation chats from each prototype, participants briefly provided thoughts on their experi-ence with the recommender. Once they had provided their thoughts following the eighteenth recommendation, they answered 6 survey questions (Table 1) corresponding to different metrics that may affect users’ trust. As they answered the survey questions, a list of their recommendations and any associated confidence statements were provided for reference. Upon completing both conditions, par-ticipants engaged in a semi-structured interview. The interview questions (Table 2) focused on their experience with the confi-dence signals provided in the non-baseline condition and what they thought of other potential scenarios involving confidence signals,including other non-baseline conditions that they did not encounter For the interview analysis, interviews were open coded using in-ductive thematic analysis [7]. The first author generated a codebook based on a review of the recordings. The first and fourth author then coded 4 transcripts, and the first author iterated on the codebook based on any disagreements, with input from the second author.The first author used the final codebook (Appendix B) to code the transcripts. Quotes are attributed to participants based on their IDnumber and non-baseline condition (A: accurate, L: low, H: high,R: random). To analyze the trust results, for each 7-point Likert-type question corresponding to a trust-related metric (Table 1), we subtracted the participant’s baseline rating from their non-baselinerating to observe their change in response for that metric (Figure 3).For an exploratory comparison of the trust results across conditions,we discuss their medians, first quartiles, and third quartiles. For The reliance analysis, to determine whether a participant over- or under-relied on the system in a particular condition, we first ob-tained the average difference between their final and initial ratings for recommendations in the low-confidence batch. We did the same for the medium- and high-confidence batches. Then, we calculated the average of those averages to determine the average recommen-dation rating update for that condition. We plotted the baseline andnon-baseline average rating updates for each participant in Figure 4.Since any very familiar recommendations did not have final ratings,if a participant encountered a batch of recommendations in which all the songs were very familiar to them, we did not include them in this analysis. Still, for each condition, at least 60% of the participants were included. Given that the quantitative results are based on a small sample size and are therefore preliminary, we focus primarily on the qualitative results in the following sections. In Figure 3a, we see that, for desire to use, the accurate condition performed best with the only positive median (1.00), the highest first quartile (-0.50), and the highest third quartile (1.75). Meanwhile, the high condition had the worst performance with the lowest median shared with the low condition (-1.00), first quartile shared with the low and random conditions (-2.00), and third quartile (0.00).4.2.1 Desire for Dynamic Confidence Signals. Twenty-four of the30 participants expressed interest in having some version of a dynamic confidence signal in their interviews. P23-A shared, “I like the red, yellow, and what will eventually be green. . . . I like this[recommender] more than the last one, because I feel like it’s learningI guess and more tailored.” It should be noted that 5 of the partici-pants in favor of confidence levels desired occasional rather than constant confidence signals. Over the course of the interviews, we realized that the idea of occasional confidence signals may be interesting to investigate, so we asked 22 participants for their thoughts on such signals for a category of recommendations, such as exploratory or high-confidence recommendations. P28-R said of an occasional high-confidence signal, “I like that because I would expect that it would only send that when it’s very sure that you would like it.” Discussing an occasional exploratory confidence signal, P26-Hcommented, “I think that every once in a while if it checked in on you and then used that feature in order to build your music profile for future song recommendations, I think that that would be a good idea.” Furthermore, 4 of the participants who showed interest inconfidence signals indicated that they would also be okay without them. The factors that may have contributed to the conditions disparate effects on desire to use the system include perceived transparency, ability, benevolence, and anthropomorphism. These Factors are discussed in the next sections. 4.2.2 Desire for Reasons Combined with Dynamic Confidence Sig-nals. Of the 6 participants who did not express interest in dynamic confidence signals, one simply did not get a chance to voice their opinion. The rest preferred only to know the reason (e.g.,“because you like Artist X”) behind a recommendation [48]. P4-R asserted, “If It's not going to tell me why, then. . . I don’t need a blurb.” This pointarose because we asked 17 participants whether confidence alone,reasons alone, or both would be most favorable, after realizing it was an interesting point to explore. Twelve preferred the combina-tion. P30-L observed, “I feel like having both is helpful, and it would make me more excited to listen to the recommendations, getting more of a full feedback.”4.2.3 Wording Changes vs Color-Coding for Dynamic ConfidenceSignals. In order for the confidence levels to make an impact on users' trust and reliance, users must be aware of them. Despite the confidence levels being explained to them in the tutorial, 4 random-condition and two accurate-condition participants reported not noticing or paying attention to the confidence levels. Four of these6 participants did not like the idea of using a full confidence scale,so it may be that they simply did not find it useful. That said, onesolution for increasing awareness of changes in confidence may be to emphasize color-coding. Five participants who received the accurate or random condition noted that the color-coding was more helpful to their understanding than the wording changes. P18-Revealed, “I think it was more about the color than it was about this statement. What were actually the three different [statements]. . . ?” P8-A argued that color-coding alone works because “I mean personally I’m lazy. I don’t really love reading so if I know what [the color-coding] means. . . .” However, if the wording changes had been more drastic, it is possible that they would have had more of an effect.P8-A noted, “I don’t really notice the changing in the words at all because it’s one word changing. . . . When it says ‘I think’ it’s already implying that it’s not 100% sure.” Design Guideline 1: Limited Wording changes may not be sufficient to convey change in confidence, so consider additional means of confidence communication such as color-coding. For perceived transparency (Figures 3b and 3c), the accurate condi-tion performed best. For perceived transparency I, it had the onlypositive median (0.50), had the highest first quartile (-0.75), andshared the highest third quartile with the low condition (1.00). Forperceived transparency II, it had the only positive median (0.50),the highest first quartile (-0.75), and the highest third quartile (1.75).Meanwhile, the poorest performance came from the high condi-tion. For perceived transparency I, it had the lowest median (-2.00),first quartile shared with the low condition (-2.00), and third quar-tile (0.00). For perceived transparency II, it had the lowest medianshared with the low condition (-1.00), first quartile (-2.00), and thirdquartile shared with the low condition (0.00).4.3.1 Quality Expectations with Dynamic Confidence Signals. Ofthe 24 participants who desired some version of confidence levels,15 expressed that the confidence levels did or would impact theirexpectations. Six of the 15 had received and taken note of a dynamicconfidence signal. Intuitively, participants generally expected thatrecommendations associated with higher confidence would havehigher quality than those associated with lower confidence. Toillustrate, when P18-A encountered a high-confidence signal, they“felt there was a high probability that I was going to add [the] song tomy playlist or that it would be something that I would at least likeeven if I don’t add it.” Describing their experience with a medium-confidence recommendation, P13-A said, “. . . that was a good jobbecause I do like the song but. . . just not for that specific playlist. SoI thought it was like a forewarning.” Discussing their expectations for low-confidence recommendations, P29-A relayed, “I felt like it would be more random, and it kind of was random.” Given that participants in the random and static confidence conditions were not provided confidence signals that help them set expectations, it makes sense that those conditions (particularly the high one which could not signal a low-confidence recommendation) fared worse in terms of perceived transparency.4.3.2 Novelty Expectations with Dynamic Confidence Signals. Interestingly, 5 participants assumed that recommendations with lower confidence signals should more likely be new to them. When faced with a low-confidence recommendation, P22-R “figured I might not be as familiar so it might be a newer song to try.” Similarly, P19-Explained, “I would expect [a low-confidence recommendation] to be a little bit different, and it’s like a new song. So [I’d be] exposed to new things.” This may be an important expectation to factor into providing low-confidence signals with recommendations, given that low-confidence recommendations may not necessarily be novel by default and may be, for example, generically popular recom-mendations. For those who do not automatically assume that low confidence implies novelty, low-confidence recommendations may elicit reduced interest, as noted in prior work [47]. P26-H raised this concern: “I think [a confidence scale is] probably a good idea, but you have to wonder at that point what the usefulness is of it in the first place? Like why would it even recommend me a song that’s like ‘Ithink you might like the song.’ ” On a related note, two participants stated that confidence levels would not adjust their expectations much. P3-A explained, “. . . If the confidence level is high, I expect it to mesh well, but [if] the confidence level is low. . . you know, either way, I expect it to mesh well with the rest of the playlist.” In order to help users understand the benefits of low-confidence recommenda-tions, it may be helpful to lean into setting a novelty expectation.By framing low-confidence recommendations as opportunities for exploring the users’ preferences, users may be more willing to en-gage with such recommendations. Though P9-R did not think the presented confidence levels would affect their expectations, when they heard the idea of an occasional exploratory confidence signal,they reacted more positively: “. . . if you begin it with ‘hey, you could hate this, but I just want to see what you think,’ I feel like that’s better than just ‘you might like this’ because. . . there was a few on there that said, ‘I think you might like the song,’ and I absolutely wouldn't. So I was like, ‘where are they getting that idea from?’ ” DesignGuideline 2: Consider both quality and novelty expectations when designing confidence levels.4.3.3 Quality Expectations with Static Confidence Signals. Threeparticipants in the static conditions noted that the static confidence signals gave them higher expectations than no signal or that they would be more willing to listen to a recommendation with a confidence signal. P30-L explained, “I feel like it does kind of prep you to think, ‘oh, I think I might like this song,’ even though it’s a small thing.’” Two participants thought that the high-confidence wording would lead to unreasonably high expectations. P7-L remarked, “If They're like ‘you will like this song’ immediately, well, do you know me like that?” This aligns with the fact that the high condition had the lowest performance for perceived transparency. For perceived ability (Figure 3d), the accurate condition had the only positive median (1.00), shared the highest first quartile with the high condition (0.00), and shared the highest third quartile with the low condition (1.00). The low condition performed worst with the lowest median (-1.00) and the lowest first quartile (-1.00),although its third quartile was higher than that of the high and random conditions (1.00).4.4.1 Confidence may be confused with reliability. Four partici-pants, three of whom had received a static confidence signal, mentioned that the confidence levels would be useful for recognizing the system’s current ability in terms of how well it understood them as users. Participants who received a static condition did not have the opportunity to experience this benefit that could improve the system's perceived ability, while the participants who received the random condition were unable to gain proper insight into the sys-tem’s improving ability. For example, P17-H stated, “I think [having confidence levels] kind of gives you a better idea of where it’s at, espe-cially because it’s always a learning process for something like this to get to know you and your taste. . . But I think once it was something I Was very familiar with, it wouldn’t really matter.” The second half of P17-H’s statement illustrates an issue with viewing the confidence signal as a reliability metric. Reliability is similar to confidence in that they both describe how well a system is likely to perform. However, whereas confidence communicates the predicted accuracy of an individual output, reliability communicates the system’s overall predicted performance [12, 32, 52]. As a recommender improves understanding a user (which this study mimicked), its confidence signals could resemble a reliability metric, as they should give an indication of how much the system has learned. Nevertheless, the user or system may regularly explore new potential areas of interest. The system would not be confident about recommendations in these new areas, but the system’s reliability would not necessarily have decreased.After determining it would be interesting to investigate, we asked 12 participants what they would expect or prefer the confi-dence trajectory to look like and gave various examples such as the confidence increasing or oscillating. The largest plurality (5) of those participants expected or preferred the confidence to increase over time. P24-R explained, “I would prefer it to be. . . a higher con-fidence as you listen to more music. . . . having it go on-and-off I feel like doesn’t really do anything, like you could listen to a radio and it could do that too. . . .” The next most common response, given by 3 participants, was an expectation or preference that the confidence would generally increase but occasionally be lower when providing exploratory recommendations. It is possible that, had we always directly proposed the idea of such a confidence trajectory, which we did with the last participant, more participants would have agreed that that made sense. However, because it was not their initial assumption, communicating to users what a confidence signal is and how it is different from a reliability metric appears important for its appropriate utilization. Design Guideline 3: Differentiate Between confidence and reliability signals. The high condition outperformed the accurate one with respect to perceived benevolence (Figure 3e). The two conditions shared the highest first (0.00) and third (1.00) quartiles. However, the median of the accurate condition (0.50) was below the median of the high condition (1.00). The low and random conditions performed worst.They shared the lowest median (0.00), the random condition had the lowest first quartile (-2.75), and the low condition had the lowest third quartile (0.00).4.5.1 Bias Concerns around Dynamic Confidence Signals. Two par-ticipants under the random condition made the important obser-vation that the confidence levels may influence their music pref-erences. P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out. . . almost like trick me into thinking I’d like it more if it says I’m gonna like it.” Similarly, P24-R said of the op-tion for a static confidence signal instead, “I feel like that could be a good thing because sometimes if a particular system thinks that you like this song, right? And they show more songs similar to that type of song. . . it doesn’t really give you much variety. So maybe if it was worded like that, I would be more open to trying other songs as well.” Just as with the act of recommendation itself, it is crit-ical to consider how confidence signals may lead to a feedback loop in which users see narrower recommendation selections over time [11]. Nonetheless, the high and accurate conditions performed best in terms of perceived benevolence, suggesting that the presence of high-confidence signals alone did not generally beget more concerns of malevolence. As the two participants who raised bias concerns received the random condition, they may have been more aware of the potential for bias, given the combination of a dynamic and inaccurate confidence signal. Neither participant had increased perceived benevolence from the baseline to the random condition.Nine participants shared that they would be more likely to listen to a recommendation or listen for longer if its associated confi-dence were higher, indicating some bias towards higher-confidence recommendations. As an example, if faced with a high-confidence signal, P23-A was “more willing to listen to more of a song. . . because it's hard to tell in just the first couple of seconds.” On the other hand,P22-R explained that “if I saw [a recommendation] of the lowest level[of confidence] then, depending on my mood, I might just not listen to it.” Even the general presence of consistent confidence signals led 4 participants to be more willing to listen to recommendations.P30-L observed, “I feel like somewhere subconsciously [the static low-confidence signal] does kind of make you more like. . . ‘oh, I want to listen to this song now.’ ”To mitigate concerns around biases towards high-confidence recommendations and encourage a more open-minded view of low-confidence ones, we may present low-confidence signals as exploratory. As discussed in Section 4.3.2, the idea of exploratory recommendations seemed to spark interest in participants. Reasons Behind recommendations were noted in Section 4.2.2 as another piece of information that many participants desire in combination with confidence. Reasons may improve perceived benevolence by clarifying how recommendations are relevant to users and demon-strating that they were not selected to unwarrantedly steer the user towards a particular artist or album. Design Guideline 4: Mitigate Concerns about biasing towards high-confidence recommendations by showcasing the exploratory benefit of engaging with lower-confidence recommendations. For perceived anthropomorphism (Figure 3f), the accurate condition had the only positive median (1.00) and the highest first quartile(0.25), but it lost the highest third quartile to the random condition(1.00 versus 1.75). The high condition performed worst with the lowest median shared with the low and random conditions (0.00),first quartile (-1.00), and third quartile shared with the low condition(0.00).4.6.1 Dynamic Confidence Signals Evoke Friendly but not Infalli-ble Image. Six participants noted that, compared to no confidence signals, dynamic confidence signals did or would make the recom-mender appear more friendly or human-like. P18-A shared, “Even If it’s completely algorithmically generated, it feels like it’s more akin to. . . like your friend showing up and being like. . . ‘you should check this artist out.”’ Despite no quantitative evidence of an increase in perceived anthropomorphism under the static conditions, the interview responses suggest that the static signals increased a sense of friendliness as well. Five participants mentioned feeling that static confidence signals would make the experience more human-like.P15-L recounted, “When I read that [static low confidence signal], I Was like, ’oh, that makes me want to go and listen to [the song] like t’s recommended to me personally.” That said, prior work has high-lighted how anthropomorphism can be interpreted as setting users up for disappointment [17]. As described in Section 4.3, our results indicate that static as opposed to dynamic confidence signals do not provide the same sense of transparency that can help users to avoid disappointment with bad recommendations. Dynamic confidence signals, if presented accurately, may make participants’ interaction with the recommender more natural without causing over-reliance,which we discuss further in Section 4.7. P29-A summed up this idea in the following opinion: “I feel like [the confidence levels feature]gives it more of a sense of being human because it’s. . . not always perfect, and it makes you know that it’s not always perfect.” DesignGuideline 5: Provide accurate confidence signals for natural interaction that does not downplay system limitations. Given the small sample size, we caution that the following quantitative reliance results are preliminary. Looking at Figure 4, we can see little indication that any condition led to more over-reliance than the baseline. Under the accurate condition, only one partici-pant moved in the direction of more over-reliance, as opposed to more appropriate or under-reliance, and this movement was slight.Meanwhile, 4 of the 6 participants in the accurate condition became more under-reliant in comparison to the baseline. The accurate condition may have increased participants’ under-reliance com-pared to the baseline because they were more aware of the fact that the system was imperfect and still learning about them. However,as we saw in Section 4.2, participants’ trust in and desire to use the system appeared not to suffer but rather to improve under the accurate condition. With the willingness to continue to use the system, users may have the chance to build their mental models of the recommender and develop their appropriate reliance over time.With the exception of one participant under the high condition,the participants in the random and static conditions also appeared to only develop more appropriate or under-reliance compared to the baseline. For these conditions, the lack of over-reliance could have been due to participants noticing that the received confidence signals did not align with their experience. As an example, P19-Rcommented, “I like the idea of it saying the different levels of howell you’ll like it, but it didn’t seem accurate.” Despite avoiding over-reliance, these conditions were weaker than the accurate one in terms of promoting trust and desire to use the system. Therefore,users may not be as encouraged to use the system long-term, and benefits of avoiding over-reliance or increasing appropriate reliance may be more difficult to realize.4.7.1 Dynamic Confidence Signals Support Anticipation of Mistakes.Eight participants commented that having a lower confidence level did or would allow them to be more understanding of bad recom-mendations. Remembering their encounters with low-confidence mistakes, P21-A related, “It would be like, ’You might like this song,’and I was like ‘yeah, you tried.’.” P8-A likewise shared, “If it was super confident then I was like, ‘yeah, it’s going to be good,’ but if it was not that confident, I was like, ‘okay, that makes sense.’” Giventhat the accurate condition performed better than the low condi-tion in terms of perceived anthropomorphism, simply providing alow-confidence signal does not appear to adequately prepare users for mistakes in the same way as the accurate confidence levels.For instance, P15-L, who liked the idea of confidence levels, com-mented that the presence of static low-confidence signals still left them disappointed in bad recommendations. They remarked that the signal made them “think I’m gonna actually like [each recom-mendation], and then I don’t. . . . I’m like, ‘okay, why did you put that[confidence signal] in there?’” This statement also reiterates howmore exploratory framing for the low-confidence signal may be more helpful to users’ appropriate reliance. ","792, 794-800","P16-R commented, “I think it’s better without a comment saying whether I’d like it or not almost because I feel like some biases could come out"
