,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{solaiman2023thegradientof,
    author = {Irene Solaiman},
    title = {The Gradient of Generative AI Release: Methods and Considerations},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-12, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,SystemRisks,Artifact,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,Risks and threats from increasingly powerful systems ,
10,AccessToSystems,Artifact,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,providing access to systems,
11,LargeTechCompanies,Agent,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,Large technology companies,
12,AccessToResources,Perceived_Need,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system.",
13,WesternLocation,Other_Precept,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,geographically concentrated in Western countries ,
14,OutsiderStatus,Perceived_Problem,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,rarely found in large technology companies.,
15,AISystems,Agent,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,AI systems,
16,RiskMeasurements,Artifact,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,Means of measuring and mitigating risk ,
17,MaliciousUser,Agent,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"Malicious uses such as the creation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88]",
18,NeedForTechnicalSkill,Perceived_Need,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,will likely require some level of technical skill even when numerous-code tools are built,
19,UnclearAccountability,Perceived_Problem,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,who or what is to be held accountable is unclear.,
20,TechnicalFilters,Artifact,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,Technical filters,
21,DifficultyOfAssessment,Perceived_Problem,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"are difficult to enumerate and assess, especially since malicious actors and their incentives are constantly evolving [44].",
22,LimitedConcentrationOfPower,Perceived_Need,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,to avoid concentrating the level of power ,
23,PeopleMostEffected,Agent,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,The People most affected and exploited by AI systems ,
24,Harms,Artifact,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"harms such as exacerbating social inequity [7, 41, 64] and harmful biases",
25,CulturalContext,Causal_Theory,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,largely cultural and context-dependent [94],
26,Auditors,Agent,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,The actors conducting and capable of conducting audits ,
27,InsufficientSubtlety,Perceived_Need,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. ",
28, , , , , ,
29,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
30,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
31,SystemRisks,reflectsPrecept,DifficultyOfAssessment,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since malicious actors and their incentives are constantly evolving [44]."
32,AccessToSystems,reflectsPrecept,LimitedConcentrationOfPower,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,One of the most prominent arguments for providing access to systems is to avoid concentrating the level of power
33,LargeTechCompanies,hasProducedArtifact,AISystems,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,Large technology companies are able to create powerful AI systems
34,AccessToResources,constrainsAgent,LargeTechCompanies,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field"
35,WesternLocation,constrainsAgent,LargeTechCompanies,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59]."
36,OutsiderStatus,constrainsAgent,PeopleMostEffected,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,The People most affected and exploited by AI systems are rarely found in large technology companies.
37,AISystems,hasProducedArtifact,Harms,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases"
38,RiskMeasurements,reflectsPrecept,CulturalContext,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,Means of measuring and mitigating risk in these systems are largely cultural and context-dependent [94]
39,MaliciousUser,hasProducedArtifact,Harms,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"Malicious uses such as the creation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level."
40,NeedForTechnicalSkill,constrainsAgent,Auditors,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built
41,UnclearAccountability,constrainsAgent,AISystems,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear."
42,TechnicalFilters,reflectsPrecept,InsufficientSubtlety,"Deployers should weigh the following considerations when mak-ing release decisions. Risks and threats from increasingly powerful systems are difficult to enumerate and assess, especially since ma-licious actors and their incentives are constantly evolving [44].Taxonomies of ethics and risks of specific systems [100] can serve as a framework for potential harms. Specific considerations across all generative systems are listed below.4.1 Concentration of Power One of the most prominent arguments for providing access to sys-tems is to avoid concentrating the level of power that high-resource organizations are collecting as one of the few groups capable of developing and deploying these systems. Large technology compa-nies are able to create powerful AI systems because of their access to training data, computing infrastructure, and commercial capa-bilities for deploying that system. This monopolization also gives these high-resource institutions more influence in AI development,the behavior of these systems, and the narrative and direction of the field [8]. Although these companies may provide access or even open-source their systems, contributions to system development are limited to people and resources working towards that com-pany’s interests [101]. Large companies are often geographically concentrated in Western countries whereas systems are deployed globally, which can asymmetrically impose cultural values [59].These companies can also punish push-back or dissent [28]. The People most affected and exploited by AI systems are rarely found in large technology companies. They must be empowered to shape systems that also benefit them, or to opt out of interaction with entirely [43].4.2 Exacerbating Disparate Performance and Harmful Social Impacts The fewer perspectives that are incorporated into the system development process leads to higher likelihood the system performs disparately for different groups. AI systems can propagate harms such as exacerbating social inequity [7, 41, 64] and harmful biases[13, 20], which can be further amplified in larger systems as scale increases [6]. Means of measuring and mitigating risk in these sys-tems are largely cultural and context-dependent [94]. The many technical and social aspects of AI systems [15] require robust re-search [74] conducted with communities affected [11] to ensure these systems benefit and do not exploit marginalized groups, if the systems are to be deployed among these groups.4.3 Malicious Use and Unintentional MisuseWith more modalities of AI generation improving in output quality,from high quality text to high quality images, the potential for harmful use cases also increases. Malicious uses such as the cre-ation of deep fake imagery [63], AI-generated disinformation [37],and illegal and disturbing material [88], can cause severe emotional harm at the individual level and destructive institutional harm at the societal level. Furthermore, malicious actors [17] have histori-cally worked to circumvent safety controls. Threat modeling will necessarily differ by modality, but as systems improve in types of outputs such as code generation, potential harms can also broaden[45]. While limiting access can prevent some malicious uses and is often a suggested action to minimize misuse [71], systems can still be vulnerable to attacks with only querying functionality available[21]. 4.4 AuditabilityThe question of auditability addresses who is conducting audits and the level of access required to effectively examine an AI system. Auditing must be considered both pre- and post-deployment as impacts from a system may not be detectable pre-deployment and when deployed, impacts may be difficult to trace back to a specific system [75]. The actors conducting and capable of conducting audits will likely require some level of technical skill even when numerous-code tools are built. The size of the system and its components also determine auditability; the datasets that large generative AIsystems are trained on are not only difficult to analyze at scale, but few tools exist to analyze large static datasets [19]. Formal audits alone cannot be the only insight or governance of a system [76].4.5 Accountability in Case of Harm In the case that an AI system harms or is connected to harming people, who or what is to be held accountable is unclear. More open and deployed systems have a higher likelihood of a broader reach and therefore a higher chance of harm. Since harm is not explicitly defined and not always physical, what constitutes harm can have a large range. The range may include encouraging physical harm,propagating social harms such as identity stereotypes, and more abstract harms such as lack of access to a system lowering oppor-tunities for a specific group. Work to characterize sociotechnical harms can narrow the scope [85].4.6 Value judgments for gating and limiting access base generative AI system is capable of many types of content,making content moderation complex [36]. What constitutes appropriate outputs is influenced by religion, cultural, and personal beliefs. What content can and should be limited, filtered, and gate is also vague. For example, sexual content may not be inherently unsafe to generate in some cultures, but may be subject to local laws. Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. While most diffi-cult without specific use cases or context, specific applications face the same challenges.",112-3,"Technical filters may not be able to distinguish between sexualcontent and nonsexual nudity, and may not be able to distinguish between consensual and non-consensual content. "
