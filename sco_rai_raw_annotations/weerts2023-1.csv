,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{weerts2023, author = {Weerts, Hilde and Xenidis, Rapha\""{e}le and Tarissan, Fabien and Olsen, Henrik Palmer and Pechenizkiy, Mykola},
title = {Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is Not a Decision Tree},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,DutchTaxAdministration,Agent,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,the Dutch government [...] Tax and Customs Administration,
10,RiskAssessmentAlgorithm,Artifact,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,a risk assessment algorithm,
11,DutchDataProtectionAuthority,Agent,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,Dutch Data Protection Authority (DPA) ,
12,UnintelligibleAlgorithms,Artifact,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,unintelligible or inaccessible algorithms.,
13,Opacity,Perceived_Problem,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,opaque ,
14,FalseFraudAllegations,Artifact,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,false fraud allegations [...] in the distribution of childcare benefits.,
15,CitizenshipDiscrimination,Causal_Theory,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,explicitly included Dutch Citizenship as one of the risk factors,
16,FindingOfDiscrimination,Artifact,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,"established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].",
17,DifficultyOfEstablishingDiscrimination,Perceived_Problem,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,establishing prima facie evidence can prove very difficult,
18,Individuals,Agent,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,individuals,
19, , , , , ,
20, , , , , ,
21,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
22,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
23,DutchTaxAdministration,hasProducedArtifact,FalseFraudAllegations,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits.
24,DutchTaxAdministration,hasProducedArtifact,RiskAssessmentAlgorithm,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,"over the course of several years, the administration had used a risk assessment algorithm"
25,RiskAssessmentAlgorithm,reflectsPrecept,CitizenshipDiscrimination,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.
26,DutchDataProtectionAuthority,hasProducedArtifact,FindingOfDiscrimination,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,"Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74]."
27,UnintelligibleAlgorithms,influencesPrecept,DifficultyOfEstablishingDiscrimination,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,"However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms."
28,Opacity,constrainsAgent,Individuals,"We start our analysis with a case related to the explicit use of a sen-sitive feature in a machine learning model, which is often assumed to be unlawful. In January 2021, the Dutch government resigns over a scandal involving false fraud allegations made by the Tax and Customs Administration in the distribution of childcare benefits. In particular, over the course of several years, the administration had used a risk assessment algorithm that explicitly included Dutch Citizenship as one of the risk factors.15 To determine whether this is a case of unlawful discrimination under EU law, we first need to determine whether it falls within the material and personal scope of EU non-discrimination law. This particular case involved a public body and, if the case fell within the scope of EU law, Article 21(1)of the Charter, which prohibits discrimination on a non-exhaustive list of grounds including membership of a national minority, couldapply. Indeed, the Dutch Data Protection Authority (DPA) established that the use of nationality as a factor in the risk classification model is considered discriminatory processing of data on the basis of, amongst others, Art. 21 of the Charter, and therefore illegitimate given the principle of fairness in Article 5 of the GDPR [74].16 In Particular, the DPA explained that incorporating nationality as a factor in the risk classification model could result in higher risk scores for applicants who are not Dutch citizens compared to appli-cants with a Dutch nationality [75]. This increased the probability of higher scrutiny through manual processing of the application by an employee of the tax administration, which the DPA considered a particular disadvantage.17 However, even in cases of (in hindsight) obvious potential for discriminatory treatment, establishing prima facie evidence can prove very difficult – especially in the context of unintelligible or inaccessible algorithms. In case of the childcare benefits scandal,parents were wrongly accused over the course of a decade and the full scale of the scandal only became clear after several years of investigation. Notoriously, parents who requested access to their files received documents with pages and pages of redacted text [76].In a situation like this, the case law of the CJEU shows that the ab-sence of transparency or information can contribute to contextual evidence with a view to triggering a shift of the burden of proof [10]. Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all. Therefore, legal claims of dis-crimination might not even arise without adequate support. Thisraises questions regarding the protection that equality law, which is designed to protect against discrimination by humans, offers incases of algorithmic discrimination.",808-9,"Yet, when algorithmic systems are embedded into opaque decision-making processes, an individual is unlikely to become aware that discrimination has occurred at all."
