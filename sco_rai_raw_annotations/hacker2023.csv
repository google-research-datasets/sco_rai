,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{hacker2023, author = {Hacker, Philipp and Engel, Andreas and Mauer, Marco}, title = {Regulating ChatGPT and Other Large Generative AI Models}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,FrenchCouncilPresidency,Agent,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,the French Council presidency,
10,AmmendmentOnGPAIS,Artifact,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS)",
11,LiabilityRisks,Perceived_Problem,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,subject to the same liability risks under the new product liability framework,
12,InabilityToPayForCompliance,Perceived_Need,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,compliance will likely be prohibitively costly,
13,AbilityToPayForCompliance,Strategy,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,may field the costs to release an approximately AI Act-compliant LGAIM,
14,DataGovernanceMeasures,Strategy,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4).",
15,NeedForRiskCompliance,Perceived_Need,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, '",
16,ComplianceObligations,Perceived_Need,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version),
17,AIActExemption,Other_Precept,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,the AI Act largely exempts ,
18,CompensationInterests,Perceived_Need,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,the compensation interests ,
19,SecrecyInterests,Perceived_Need,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,the secrecy interests,
20,OverlyBroadDefinitionOfGPAIS,Perceived_Problem,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,broad definition of GPAIS,
21,ImpossibilityOfRiskManagement,Perceived_Problem,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible",
22,AIDevelopingEntities,Agent,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,all entities–large or small–developing LGAIMs,
23,LargePlayers,Agent,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"only large, deep-pocketed players ",
24,SmallPlayers,Agent,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,open source developers and many SME,
25,FoundationModels,Agent,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,all foundation models,
26,NewProviders,Agent,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,“new providers” which significantly modify the AI system,
27,AIRiskFromUse,Causal_Theory,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,risks related from AI systems can stem from their specific use,
28,NonProfessionalUsers,Agent,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,non-professional users,
29,InjuredPersons,Agent,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,injured persons,
30, , , , , ,
31, , , , , ,
32, , , , , ,
33,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
34,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
35,FrenchCouncilPresidency,hasProducedArtifact,AmmendmentOnGPAIS,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS)"
36,AmmendmentOnGPAIS,reflectsPrecept,OverlyBroadDefinitionOfGPAIS,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities"
37,AmmendmentOnGPAIS,influencesPrecept,ImpossibilityOfRiskManagement,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity."
38,LiabilityRisks,constrainsAgent,AIDevelopingEntities,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework"
39,AbilityToPayForCompliance,constrainsAgent,LargePlayers,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"it can be expected that only large, deep-pocketed players (such as Google, Meta, Microsoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM."
40,InabilityToPayForCompliance,constrainsAgent,SmallPlayers,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration"
41,DataGovernanceMeasures,constrainsAgent,FoundationModels,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4)."
42,NeedForRiskCompliance,constrainsAgent,FoundationModels,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, '"
43,ComplianceObligations,constrainsAgent,NewProviders,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version)"
44,AmmendmentOnGPAIS,reflectsPrecept,AIRiskFromUse,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use.
45,AIActExemption,constrainsAgent,NonProfessionalUsers,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,the AI Act largely exempts non-professional users
46,CompensationInterests,constrainsAgent,InjuredPersons,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy interests of AI developers and deployers"
47,SecrecyInterests,constrainsAgent,AIDevelopingEntities,"On May 13, 2022, the French Council presidency circulated an amendment to the draft AI Act, Art. 4a-4c, on what the text calls “general-purpose AI systems” (GPAIS). This novel passage has come to form the nucleus of direct regulation of LGAIMs. It was fiercely contested in the EP [63-65] and will be a key point of debate for the final version of the AI Act. The general approach adopted by theCouncil on December 6, 2022, defines GPAIS as systems “intended by the provider to perform generally applicable functions such as image and speech recognition, audio and video generation, pattern detection, question answering, translation and others; a general purpose AI system may be used in a plurality of contexts and be integrated in a plurality of other AI systems” (Art. 3(1b) AI Act). Under the Council version, GPAIS are subjected to the high-risk obligations (e.g., Art. 8 to 15 AI Act) if they may be used as high-risk systems or as components thereof (Art. 4b(1)(1) and 4b(2) AI Act) The AI Act heroically strives to keep pace with the accelerating dynamics in the AI technology space. However, in our view, the recently introduced rules on GPAIS fail to do justice to the peculiarities of large AI models, and particularly LGAIMs, for three reasons.3.1.1 Toward a Definition of GPAIS. First, the definition in Art.3(1b) AI Act is significantly over-inclusive. Rules on GPA were inspired by the surge in the release of and literature on foundation models and LGAIMs. As seen in Part 2, LGAIMs operate with large numbers of parameters, training data, and compute. Significantly,they generally operate on a wider range of problems than traditional models do [43]. Conceptually, their “generality” may refer to their ability (e.g., language versus vision, or combinations in multimodal models); domain of use cases (e.g., educational versus economic);breadth of tasks covered (e.g., summarizing versus completing text),or versatility of output (e.g., black and white versus multi colored image) [14]. GPAIS, in our view, must necessarily display significant generality in ability, tasks, or outputs, beyond the mere fact that they might be integrated into various use cases (which also holds true for extremely simple algorithms). The broad definition of GPAIS in theAI Act (Council general approach) clashes with this understanding,however. According to that rule, every simple image or speech recognition system seems to qualify, irrespective of the breadth of its capabilities; rightly, this only corresponds to a minority position in the technical GPAIS literature [14, 66].3.1.2 Risk Management for GPAIS. Second, even a narrower definition would not avoid other problems. Precisely because large AI models are so versatile, providers will generally not be able to avail themselves of the exception in Art. 4c(1) AI Act: by excluding all high-risk uses, they would not act in good faith, as they would have to know that the system, once released, may and likely will be used for at least one high-risk application. For example, language models may be used to summarize or rate medical patient files, or student, job, credit or insurance applications (Annexes II, Section A. No.12, 13 and III No. 3-5 AI Act. Unless any misuse can be verifiable technically excluded, LGAIMs will therefore generally count as high-risk systems under the proposed provision. This, however, entails that they have to abide by the high-risk obligations, in particular the establishment of a comprehensive risk management system, according to Art. 9 AI Act. Setting up such a system seems to border on the impossible, given LGAIMs’ versatil-ity. It would compel GAIM providers to identify and analyze all“known and foreseeable risks most likely to occur to health, safety and fundamental rights” concerning all possible high-risk uses of the L'GAIM (Art. 9(2)(a), 4b(6) AI Act). On this basis, mitigation strategies for all these risks have to be developed and implemented(Art. 9(2)(d) and (4) AI Act). Providers of LGAIMs such as GPT-4 would, therefore have to analyze the risks for every single, possible application in every single high-risk case contained in AnnexesII and III concerning health, safety and all possible fundamental rights.Similarly, performance, robustness, and cybersecurity tests will have to be conducted concerning all possible high-risk uses (Art.15(1), 4b(6) AI Act). This seems not only almost prohibitively costly but also hardly feasible. The entire analysis would have to be based on an abstract, hypothetical investigation, and coupled with–again hypothetical–risk mitigation measures that will, in many cases,depend on the concrete deployment, which by definition has not been implemented at the moment of analysis. What is more, many of these possible use cases will, in the end, not even be realized.Hence, such a rule would likely create “much ado about nothing”,in other words: a waste of resources.3.1.3 Adverse Consequences for Competition. Third, the current GPAIS rules would likely have significantly adverse consequences for the competitive environment surrounding LGAIMs. The AI Actdefinition specifically includes open source developers as LGAIM providers, of which there are several.3 Some of these will explore LGAIMs not for commercial, but for philanthropic or research rea-sons. While, according to its Art. 2(7), the AI Act shall not apply to any (scientific, see Recital 12b AI Act) research and development activity regarding AI systems, this research exemption arguably does not apply anymore once the system is released into the wild(cf. Recital 12b AI Act). As a result, all entities–large or small–developing LGAIMs and placing them on the market will have to comply with the same stringent high-risk obligations, and be subject to the same liability risks under the new product liability framework [24]. Given the dif-ficulty to comply with the AI Act’s GPAIS rules, it can be expected that only large, deep-pocketed players (such as Google, Meta, Mi-crosoft/Open AI) may field the costs to release an approximately AI Act-compliant LGAIM. For open source developers and many SMEs, compliance will likely be prohibitively costly. Hence, the AI Act may have the unintended consequence of spurring further anti-competitive concentration in the GAME development market.Similar effects have already been established concerning the GDPR[67]. In this sense, the AI Act threatens to undermine the efforts of the Digital Markets Act to infuse workable competition into the core of the digital and platform economy.3.1.4 Critique of the European Parliament proposal. In the EP, the question of how to regulate large generative AI models significantly delayed the formulation of the EP position on the AI Act. After A lengthy debate, a compromise was reached in late April/earlyMay 2023.4 The compromise foresees three layers of obligations that apply to generative AI systems [65, 68]. The first layer will apply to the providers (=developers) of a subset of GPAIS denominated “foundation models” (Art. 28b(1)-(3) AI Act EP Version) and generative AI (Article 28b(4) AI Act EP Version). Referring to a well-known term in the computer science community [see, e.g., 36,69], the EP version defines foundation models as an AI system “that is trained on broad data at scale, is designed for generality of output,and can be adapted to a wide range of distinctive tasks” (Art. 3(1c)AI Act EP Version) [cf. also 36, at 3]. The focus on generality of output and tasks is indeed better suited to capture the specifics of large generative AI models than the vague definition of GPAIS (seeSection 3.1.1). In line with suggestions made in this paper, the general obligations for all foundation models include data governance measures, particularly with a view to the mitigation of bias (Art.28b(2)(b) AI Act EP Version; see Section 4). Furthermore, appropriate levels of performance, interpretability, corrigibility, safety and cybersecurity must be maintained throughout the model’s life-cycle. These requirements have to be tested for, documented, and verified by independent experts, Art. 28b(2)(c) AI Act EP Version. Crucially, however, all foundation models also need to implement risk assessments, risk mitigation measures, and risk management strategies with a view to reasonably foreseeable risks to health,safety, fundamental rights, the environment, democracy and the rule of law, again with the involvement of independent experts,Art. 28b(2)(a) AI Act EP Version. Effectively, this requirement is tantamount to classifying foundation models as high-risk per se.A crucial element of the minimum standards for generative AIis contained in the “ChatGPT Rule” Art. 28b(4) AI Act EP Version.It contains three main elements. (i) The transparency obligations concerning the use of AI is a step in the right direction. It addresses obligations of providers towards users of AI systems. In our view,additionally, obligations of users towards recipients are warranted in some instances to fight the spread of fake news and misinfor-mation (see Section 6.1). (ii) The rule on preventing a breach of EU law, however, arguably does not go far enough. Here, the com-pliance mechanisms of the DSA should be transferred much more specifically, for example through clear, mandatory notice and action procedures and trusted flaggers (see Section 6.4). (iii) The disclosure4See note 2.of copyrighted material contained in training data may indeed help authors and creators enforce their rights. However, even experts often argue whether certain works are copyrightable at all or not.What must be avoided is that developers who have, e.g., processed 20 million images now have to conduct a full-scale legal due dili-gence on these 20 million images to decide for themselves whether they are copyrightable or not. Hence, it must therefore be sufficient to disclose, even in an over-inclusive manner, works which may be copyrightable, including those for which it is not clear whether they are ultimately copyrightable or not. Otherwise, again, practically prohibitive due diligence costs will arise. The individual author must then decide, when she discovers her work, whether she thinks it is protected by copyright or not. The second level refers to “new providers” which significantly modify the AI system, Art. 28(1)(b) and (ba) AI Act EP Version. This New provider, which is called deployer in our paper (see Section 3.2.1), assumes the obligations of the former provider upon substantial modification; the new provider takes on this role (Art. 28(1)and (2)(1) AI Act EP Version). A third level of requirements related to the AI value chain (Art. 28(2)(2) and (2a) AI Act EP Version), inline with suggestions made below in this paper (see Section 3.2.2). In our view, while containing steps in the right direction, this proposal would be ultimately unconvincing for as it effectively treats foundation models as high-risk applications (cf. Art. 28b(1)(a)and (f) AI Act EP Version). Of course, as noted and discussed in detail below (Part 5), AI output may be misused for harmful speech and acts (as almost any technology). But not only does this seem to be rather the exception than the rule. The argument concern-ing adverse competitive consequences applies equally here. Under The EP version, risk assessment, mitigation, and management still remain focused on the model itself rather than the use-case spe-cific application (Art. 28b(2)(a) and (f) AI Act EP Version), even though Recital 58a acknowledges that risks related from AI systems can stem from their specific use. Again, this leads to the onerous assessment and mitigation of hypothetical risks that may never materialize–instead of managing risks at the application level where the concrete deployment can be considered. This critique does not imply, of course, that LGAIMs should not be regulated at all. However, in our view, a different approach is warranted. Scholars have noted that the regulatory focus should shift [12, 13] and move towards GAME deployers and users, i.e.,those calibrating LGAIMs for and using them in concrete high-risk applications. While some general rules, such as data governance,non-discrimination and cybersecurity provisions, should indeed apply to all foundation models (see Section 4), the bulk of the high-risk obligations of the AI Act should be triggered for specific use cases only and target primarily deployers and professional users.3.2.1 Terminology: Developers, Deployers, Users, and Recipients.Lilian Edwards, for example, has rightly suggested to differentiate between developers of GPAIS, deployers, and end users [12, seealso 24]. In the following, we take this beginning differentiation in the AI value chain one step further. In many scenarios, there will beat least four entities involved, in different roles [cf. 70]. We suggestthat the terminology in the AI Act and other pertaining regulation must be adapted to the evolving AI value chain in the followingway.Developer: this is the entity originally creating and (pre-)training the model. In the AI Act, this entity is called the provider(under some further conditions, see Art. 3(2)). Real-world examples would be OpenAI, Stability, or Google. Deployer: this is the en-tity fine-tuning the model for a specific use case. The AI Act EPVersion also uses the term, albeit in a slightly different manner, cov-ering any person or entity using an AI system under its authority,except where the AI system is used in the course of a personal non-professional activity (Art. 3(4) AI Act EP Version); for the purposes of the AI Act EP Version, a deployer can be a (new) provider, Art.28(2)(1). Note that there could be several deployers (working jointly or consecutively), leading to a true AI value chain similar to OEMvalue chains. Alternatively, the developer could simultaneously act as a deployer (vertical integration)–just as for the purposes of theAI Act EP Version, a deployer can be a (new) provider, Art. 28(2)(1).User: this is the entity actually generating output from an LGAIM,e.g. via prompts, and putting it to use. The user may harness the output in a professional or a non-professional capacity [71, 72].Potential real-world examples of professional users would be the clothing and sportswear manufacturer from the first lead example,or any other entity from the groups of professional users just listed.Note that any individual making professionally motivated com-ments online would also count as a professional user in this respect.Finally, some exceptions from the EU consumer definition are in or-der: for example, employees5 (and students, for that matter) should presumptively count as professional users when applying LGAIMsfor job- or education-related tasks. Particularly concerning negative externalities of AI output, it should not matter whether users are pursuing a dependent or independent professional activity (e.g., Art.29 AI Act). By contrast, the AI Act largely exempts non-professionalusers (cf. Art. 2(8) AI Act; the AI Act EP Version contains no generalexemption, but excludes non-professionals from the definition of deployers, Art. 3(4)). The parent from the lead example using Chat-GPT for birthday party would fall into this category. Recipient:this is the entity consuming the product offered by the user.With this terminology in place, regulatory obligations can be al-located to different types of actors in more nuanced ways. While developers should, to a certain extent, be subject to nondiscrimination law and certain data governance provisions (Section 4), we suggest that the focus of regulatory duties should lie on deployers and users,for example concerning risk management systems (Art. 9 AI Act)or performance and robustness thresholds (Art. 15 AI Act) (see also below, Part 6).3.2.2 The AI Value Chain. Such a shift of the regulatory focus on developers and users, however, entails several follow-up problems that need to be addressed [12]. First, deployers and users may be much smaller and less technologically sophisticated than LGAIMdevelopers. This is not a sufficient reason to exempt them from regulation and liability, but it points to the importance of designing a feasible allocation of responsibilities along the AI value chain.Recent proposals discussed in the EP point in this direction as well(see Section 3.1.4). Obligations must be structured in such a way5But see German Constitutional Court, Order of November 23, 2006, Case 1 BvR1909/06: employees are consumers in the sense of the EU consumer law that deployers and users can reasonably be expected to comply with them, both by implementing the necessary technological ad-justments and by absorbing the compliance costs. Second, many of the AI Act’s high-risk obligations refer to the training and model-ing phase conducted, at least partially, by the GAME developers.Typically, GAME developers will pre-train a large model, which may then be fine-tuned by deployers, potentially in collaboration with developers [73, 74], while users ultimately make the decision what the AI system is used for specifically (e.g. commercial use for design or private use for generating an invitation text). To meet theAI Act requirements concerning training data (Art. 10), documenta-tion and record-keeping (Art. 11 and 12), transparency and human oversight (Art. 13 and 14), performance, robustness and cybersecu-rity (Art. 15), and to establish the comprehensive risk management system (Art. 9), any person responsible will need to have access to the developer’s and deployer’s data and expertise. This unveils a regulatory dilemma: focusing exclusively on developers entails po-tentially excessive and inefficient compliance obligations; focusing on deployers and users risks burdening those who cannot comply due to limited insight or resources. Third, and related to the first and second aspect, individual actors in the AI value chain may simply not have the all-encompassing knowledge and control that would be required if they were the sole addressees of regulatory duties [75]. This more abstract observation also shows that shared and overlapping responsibilities may be needed .In our view, the only way forward are legally mandated col-laborations between LG AIM providers, deployers and users with respect to the fulfillment of regulatory duties. More specifically, we suggest a combination of strategies known from pre-trial discovery,trade secrets law, and the GDPR. Under the current AI Act (Coun-cil general approach), such teamwork is encouraged in Art. 4b(5):providers “shall” cooperate with and provide necessary information to users. A key issue, also mentioned in the Article, is access to information potentially protected as trade secrets or intellectual property (IP) rights [13, 76]. To be workable, this obligation needs further concretization; the same holds true for more recent propos-als by the EP in this direction [77]; Art. 10(6a) AI Act EP Versiononly explicitly addresses a situation where such cooperation does not take place.The problem of balancing collaboration and disclosure with the protection of information is not limited to the AI Act. In our view,it has an internal and external dimension. Internally, i.e., in the relationship between the party requesting and the party granting access, access rights are often countered, by the granting party, by reference to supposedly unsurmountable trade secrets or IP rights[78-80]. The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy inter-ests of AI developers and deployers [24, 81, 82]. Extensive literature and practical experience concerning this problem exists in the realm of the US pretrial discovery system [83-87]. Under this mechanism, partially adopted by the proposed EU evidence disclosure rules [24],injured persons may seek access to documents and information held by the potential defendant before even launching litigation. This, inturn, may lead to non-meritorious access requests by competitors. Similarly, in the AI value chain, developers, deployers and users may indeed not only be business partners but also be (potential) competitors. Hence, deployers’ and users’ access must be limited. Conversely, some flow of information must be rendered possible to operationalize compliance with high-risk obligations by deployers.To guard against abuse, we suggest a range of measures. It may be worthwhile to introduce provisions inspired by the US pretrial discovery system [80, 83, 88] and the proposed EU evidence disclo-sure mechanism (Art. 3(4) AI Liability Directive, protective order).Hence, courts should be empowered to issue protective orders, which endow nondisclosure agreements with further weight and subject them to potential administrative penalties. The order may also exempt certain trade secrets from disclosure or allow access only under certain conditions (see F.R.C.P. Rule 26(c)(1)(G)). Further-more, the appointment of a special master may, ultimately, strike a balance between information access and the undue appropriation of competitive advantage (cf. F.R.C.P. Rule 53(a)) [88]. With these safeguards in place, GAME developers should be compelled, and not merely encouraged, to cooperate with deployers and users con-cerning AI Act compliance if they have authorized the deployment.Concerning the external dimension, the question arises of who should be responsible for fulfilling pertinent duties and be ulti-mately liable, regarding administrative fines and civil damages, if high-risk rules are violated. Here, we may draw inspiration fromArt. 26 GDPR (see also [12]): this mechanism could, mutatis mutan-dis, be transferred to the AI value chain. Collaboration should be documented in writing to facilitate ex post accountability. Disclosing the core parts of the document, sparing trade secrets, should help potential plaintiffs choosing the right party for following dis-closure of evidence requests under the AI liability regime. Finally, joint and several liability ensures collaboration and serves the com-pensation interests of injured persons. Internally, parties held liable by injured persons can then turn around and seek reimbursement from others in the AI value chain. For example, if the developers essentially retain control via an API distribution model, the internal liability burden will often fall on them. Developers’ and employers liability, however, must end where their influence over the deployedmodel ends. Beyond this point, only the users should be the subject of regulation and civil liability (and vice versa, for example incontrol-via-API cases): incentives for action only make sense where the person incentivized is actually in a position to act [89, 90]. In The GDPR setting, this was effectively decided by the CJEU in the Fashion ID case (CJEU, C-40/17, para. 85). The sole responsibility of the users for certain areas should then also be included in the disclosure agreement to inform potential plaintiffs and foreclose non-meritorious claims against the developer and deployer. Sucha system, in our view, would strike an adequate balance of interests and power between GAME developers, deployers, users, andaffected persons.The EP version of the AI Act now rightly contains rules on the value chain [68]. However, these need to be rendered more specific,as laid out in the preceding sections, to function effectively. Ulti-mately, allocating responsibility and liability along the value chain is crucial if the AI Act seeks to maintain its spirit of a technology-specific instrument that does not, however, regulate models per se,but primarily models in concrete use cases. ",1114-7,"The liability directives proposed by the EU Commission, for example, contain elaborate evidence disclosure rules pitting the compensation interests of injured persons against the secrecy interests of AI developers and deployers"
