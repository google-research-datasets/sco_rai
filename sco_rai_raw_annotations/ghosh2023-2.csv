,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{ghosh2023, author = {Ghosh, Bishwamittra and Basu, Debabrota and Meel, Kuldeep S.}, title = {‚ÄúHow Biased Are Your Features?‚Äù: Computing Fairness Influence Functions with Global Sensitivity Analysis}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Intersectionality,Causal_Theory,"Individual vs. Intersectional FIFs. Now, we aim to understand the importance of intersectional FIFs over individual FIFs in Fig-ure 4. We consider COMPAS dataset with race ‚àà {Caucasian, non-Caucasian} as the sensitive feature, and a logistic regression classi-fier to predict whether a person will re-offend crimes within the next two years. Since the classifier only optimizes training error, it demonstrates statistical parity of 0.174, i.e. it suggests that a non-Caucasian has 0.174 higher probability of re-offending crimes thana Caucasian. Next, we investigate the sources of bias and present individual FIFs in Figure 4a, and both individual and intersectional FIGs in Figure 4b. In both figures, we present influential features and their FIFs in the descending order of absolute values. Accordingto FairXplainer, the priors count (FIF = 0.1) dominates in increas-ing statistical parity‚Äîbetween Caucasian and non-Caucasian, their prior count demonstrates the maximum difference in the scaled variance of positive prediction. Other non-sensitive features have almost zero FIFs. However, in Figure 4a, higher-order FIFs (ùúÜ > 1)increases statistical parity by 0.031, denoting that the data is cor-related and presenting only individual FIFs is not sufficient for understanding the sources of bias. For example, while both sex and age individually demonstrate almost zero influence on bias(Figure 4a), their combined effect and intersectional effects which-charge degree, priors count, and juvenile miscellaneous count contribute highly on statistical parity. In contrast to FairXplainer, SHAP only estimates individual FIFs (Figure 4c) and approximate statistical parity with higher error than FairXplainer. Interestingly, FairXplainer yields FIF estimates of the classifier trained on COMPAS dataset that significantly matches with the rule-based classifier extracted from the COMPASS dataset using Certifiably Opti-mal Rule Lists (CORALS) algorithm [41, Figure 3]. From Figure 3in [41], we observe that prior count is the only feature used indi-vidually to predict arrests, while age is paired with sex and prior counts respectively. While considering second-order intersection-ality, FairXplainer (ùúÜ = 2) yields priors count, (sex, age), and (age,priors count) as three of the top four features that explains the ob-servation of [41] better than SHARP and FairXplainer (ùúÜ = 1) consid-ering only individual features. Therefore, FairXplainer demonstrates a clearer understanding on the sources of bias of a classifier by simultaneously quantifying on intersectional influences and individual influences.",146,intersectional influences,
10,FairXplainer,Agent,"Individual vs. Intersectional FIFs. Now, we aim to understand the importance of intersectional FIFs over individual FIFs in Fig-ure 4. We consider COMPAS dataset with race ‚àà {Caucasian, non-Caucasian} as the sensitive feature, and a logistic regression classi-fier to predict whether a person will re-offend crimes within the next two years. Since the classifier only optimizes training error, it demonstrates statistical parity of 0.174, i.e. it suggests that a non-Caucasian has 0.174 higher probability of re-offending crimes thana Caucasian. Next, we investigate the sources of bias and present individual FIFs in Figure 4a, and both individual and intersectional FIGs in Figure 4b. In both figures, we present influential features and their FIFs in the descending order of absolute values. Accordingto FairXplainer, the priors count (FIF = 0.1) dominates in increas-ing statistical parity‚Äîbetween Caucasian and non-Caucasian, their prior count demonstrates the maximum difference in the scaled variance of positive prediction. Other non-sensitive features have almost zero FIFs. However, in Figure 4a, higher-order FIFs (ùúÜ > 1)increases statistical parity by 0.031, denoting that the data is cor-related and presenting only individual FIFs is not sufficient for understanding the sources of bias. For example, while both sex and age individually demonstrate almost zero influence on bias(Figure 4a), their combined effect and intersectional effects which-charge degree, priors count, and juvenile miscellaneous count contribute highly on statistical parity. In contrast to FairXplainer, SHAP only estimates individual FIFs (Figure 4c) and approximate statistical parity with higher error than FairXplainer. Interestingly, FairXplainer yields FIF estimates of the classifier trained on COMPAS dataset that significantly matches with the rule-based classifier extracted from the COMPASS dataset using Certifiably Opti-mal Rule Lists (CORALS) algorithm [41, Figure 3]. From Figure 3in [41], we observe that prior count is the only feature used indi-vidually to predict arrests, while age is paired with sex and prior counts respectively. While considering second-order intersection-ality, FairXplainer (ùúÜ = 2) yields priors count, (sex, age), and (age,priors count) as three of the top four features that explains the ob-servation of [41] better than SHARP and FairXplainer (ùúÜ = 1) consid-ering only individual features. Therefore, FairXplainer demonstrates a clearer understanding on the sources of bias of a classifier by simultaneously quantifying on intersectional influences and individual influences.",146,FairXplainer,
11,ComparisonWithSHAP,Perceived_Problem,"Individual vs. Intersectional FIFs. Now, we aim to understand the importance of intersectional FIFs over individual FIFs in Fig-ure 4. We consider COMPAS dataset with race ‚àà {Caucasian, non-Caucasian} as the sensitive feature, and a logistic regression classi-fier to predict whether a person will re-offend crimes within the next two years. Since the classifier only optimizes training error, it demonstrates statistical parity of 0.174, i.e. it suggests that a non-Caucasian has 0.174 higher probability of re-offending crimes thana Caucasian. Next, we investigate the sources of bias and present individual FIFs in Figure 4a, and both individual and intersectional FIGs in Figure 4b. In both figures, we present influential features and their FIFs in the descending order of absolute values. Accordingto FairXplainer, the priors count (FIF = 0.1) dominates in increas-ing statistical parity‚Äîbetween Caucasian and non-Caucasian, their prior count demonstrates the maximum difference in the scaled variance of positive prediction. Other non-sensitive features have almost zero FIFs. However, in Figure 4a, higher-order FIFs (ùúÜ > 1)increases statistical parity by 0.031, denoting that the data is cor-related and presenting only individual FIFs is not sufficient for understanding the sources of bias. For example, while both sex and age individually demonstrate almost zero influence on bias(Figure 4a), their combined effect and intersectional effects which-charge degree, priors count, and juvenile miscellaneous count contribute highly on statistical parity. In contrast to FairXplainer, SHAP only estimates individual FIFs (Figure 4c) and approximate statistical parity with higher error than FairXplainer. Interestingly, FairXplainer yields FIF estimates of the classifier trained on COMPAS dataset that significantly matches with the rule-based classifier extracted from the COMPASS dataset using Certifiably Opti-mal Rule Lists (CORALS) algorithm [41, Figure 3]. From Figure 3in [41], we observe that prior count is the only feature used indi-vidually to predict arrests, while age is paired with sex and prior counts respectively. While considering second-order intersection-ality, FairXplainer (ùúÜ = 2) yields priors count, (sex, age), and (age,priors count) as three of the top four features that explains the ob-servation of [41] better than SHARP and FairXplainer (ùúÜ = 1) consid-ering only individual features. Therefore, FairXplainer demonstrates a clearer understanding on the sources of bias of a classifier by simultaneously quantifying on intersectional influences and individual influences.",146,In contrast [...] SHAP,
12, , , , , ,
13, , , , , ,
14, , , , , ,
15, , , , , ,
16, , , , , ,
17,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
18,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
19,ComparisonWithSHAP,constrainsAgent,FairXplainer,"Individual vs. Intersectional FIFs. Now, we aim to understand the importance of intersectional FIFs over individual FIFs in Fig-ure 4. We consider COMPAS dataset with race ‚àà {Caucasian, non-Caucasian} as the sensitive feature, and a logistic regression classi-fier to predict whether a person will re-offend crimes within the next two years. Since the classifier only optimizes training error, it demonstrates statistical parity of 0.174, i.e. it suggests that a non-Caucasian has 0.174 higher probability of re-offending crimes thana Caucasian. Next, we investigate the sources of bias and present individual FIFs in Figure 4a, and both individual and intersectional FIGs in Figure 4b. In both figures, we present influential features and their FIFs in the descending order of absolute values. Accordingto FairXplainer, the priors count (FIF = 0.1) dominates in increas-ing statistical parity‚Äîbetween Caucasian and non-Caucasian, their prior count demonstrates the maximum difference in the scaled variance of positive prediction. Other non-sensitive features have almost zero FIFs. However, in Figure 4a, higher-order FIFs (ùúÜ > 1)increases statistical parity by 0.031, denoting that the data is cor-related and presenting only individual FIFs is not sufficient for understanding the sources of bias. For example, while both sex and age individually demonstrate almost zero influence on bias(Figure 4a), their combined effect and intersectional effects which-charge degree, priors count, and juvenile miscellaneous count contribute highly on statistical parity. In contrast to FairXplainer, SHAP only estimates individual FIFs (Figure 4c) and approximate statistical parity with higher error than FairXplainer. Interestingly, FairXplainer yields FIF estimates of the classifier trained on COMPAS dataset that significantly matches with the rule-based classifier extracted from the COMPASS dataset using Certifiably Opti-mal Rule Lists (CORALS) algorithm [41, Figure 3]. From Figure 3in [41], we observe that prior count is the only feature used indi-vidually to predict arrests, while age is paired with sex and prior counts respectively. While considering second-order intersection-ality, FairXplainer (ùúÜ = 2) yields priors count, (sex, age), and (age,priors count) as three of the top four features that explains the ob-servation of [41] better than SHARP and FairXplainer (ùúÜ = 1) consid-ering only individual features. Therefore, FairXplainer demonstrates a clearer understanding on the sources of bias of a classifier by simultaneously quantifying on intersectional influences and individual influences.",146,"In contrast to FairXplainer, SHAP"
20,Intersectionality,constrainsAgent,FairXplainer,"Individual vs. Intersectional FIFs. Now, we aim to understand the importance of intersectional FIFs over individual FIFs in Fig-ure 4. We consider COMPAS dataset with race ‚àà {Caucasian, non-Caucasian} as the sensitive feature, and a logistic regression classi-fier to predict whether a person will re-offend crimes within the next two years. Since the classifier only optimizes training error, it demonstrates statistical parity of 0.174, i.e. it suggests that a non-Caucasian has 0.174 higher probability of re-offending crimes thana Caucasian. Next, we investigate the sources of bias and present individual FIFs in Figure 4a, and both individual and intersectional FIGs in Figure 4b. In both figures, we present influential features and their FIFs in the descending order of absolute values. Accordingto FairXplainer, the priors count (FIF = 0.1) dominates in increas-ing statistical parity‚Äîbetween Caucasian and non-Caucasian, their prior count demonstrates the maximum difference in the scaled variance of positive prediction. Other non-sensitive features have almost zero FIFs. However, in Figure 4a, higher-order FIFs (ùúÜ > 1)increases statistical parity by 0.031, denoting that the data is cor-related and presenting only individual FIFs is not sufficient for understanding the sources of bias. For example, while both sex and age individually demonstrate almost zero influence on bias(Figure 4a), their combined effect and intersectional effects which-charge degree, priors count, and juvenile miscellaneous count contribute highly on statistical parity. In contrast to FairXplainer, SHAP only estimates individual FIFs (Figure 4c) and approximate statistical parity with higher error than FairXplainer. Interestingly, FairXplainer yields FIF estimates of the classifier trained on COMPAS dataset that significantly matches with the rule-based classifier extracted from the COMPASS dataset using Certifiably Opti-mal Rule Lists (CORALS) algorithm [41, Figure 3]. From Figure 3in [41], we observe that prior count is the only feature used indi-vidually to predict arrests, while age is paired with sex and prior counts respectively. While considering second-order intersection-ality, FairXplainer (ùúÜ = 2) yields priors count, (sex, age), and (age,priors count) as three of the top four features that explains the ob-servation of [41] better than SHARP and FairXplainer (ùúÜ = 1) consid-ering only individual features. Therefore, FairXplainer demonstrates a clearer understanding on the sources of bias of a classifier by simultaneously quantifying on intersectional influences and individual influences.",146,FairXplainer demonstrates a clearer understanding on the sources of bias of a classifier by simultaneously quantifying on intersectional influences and individual influences
