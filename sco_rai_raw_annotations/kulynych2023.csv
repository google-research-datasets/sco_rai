,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{kulynych2023, author = {Kulynych, Bogdan and Hsu, Hsiang and Troncoso, Carmela and Calmon, Flavio P.},
title = {Arbitrary Decisions Are a Hidden Cost of Differentially Private Training},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,PredictiveMultiplicityEffects,Perceived_Problem,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,predictive-multiplicity effects in realistic settings,
10,CreditCardApplicants,Agent,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,the application and the applicant.,
11,IndonesianCouples,Agent,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,Indonesia [...] demographic and socio-economic characteristics of a married couple,
12,InsituteOfRadiology,Agent,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006,
13,HighPrivacyModels,Artifact,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,models with higher level of privacy,
14,LowPrivacyModels,Artifact,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,for lower privacy levels,
15,Researchers,Agent,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,we,
16,CreditApprovalDataset,Artifact,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,Credit Approval tabular dataset (Credit),
17,ContraceptiveMethodChoiceDataset,Artifact,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,Contraceptive Method Choice tabular dataset (Contraception) based on 1987 National Indonesia Contraceptive Prevalence Survey,
18,MammographicMassDataset,Artifact,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,Mammographic Mass tabular dataset (Mammography) ,
19,RandomnessInTraining,Perceived_Problem,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,randomness in training,
20,VariedMultiplicity,Perceived_Problem,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,groups of individuals with different values of predictive multiplicity,
21, , , , , ,
22,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
23,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
24,PredictiveMultiplicityEffects,constrainsAgent,Researchers,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,"To study predictive-multiplicity effects in realistic settings, we"
25,CreditCardApplicants,hasProducedArtifact,CreditApprovalDataset,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.
26,IndonesianCouples,hasProducedArtifact,ContraceptiveMethodChoiceDataset,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,Contraceptive Method Choice tabular dataset (Contraception) based on 1987 National Indonesia Contraceptive Prevalence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple
27,InsituteOfRadiology,hasProducedArtifact,MammographicMassDataset,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes
28,HighPrivacyModels,reflectsPrecept,PredictiveMultiplicityEffects,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity
29,HighPrivacyModels,reflectsPrecept,RandomnessInTraining,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training
30,LowPrivacyModels,reflectsPrecept,VariedMultiplicity,"In this section, we empirically explore the predictive multiplicity of DP algorithms. We use a low-dimensional synthetic dataset inorder to visualize the level of multiplicity across the input space. To study predictive-multiplicity effects in realistic settings, we use real-world tabular datasets representative of high-stakes domains,namely lending and healthcare, and one image dataset. The code to reproduce our experiments is available at:github.com/spring-epfl/dp_multiplicity Datasets and Tasks. We use the following datasets:â€¢ A Synthetic dataset containing data belonging to two classes with class-conditional distributions ğ‘‹0 âˆ¼ N (ğœ‡0, Î£0) andğ‘‹1 âˆ¼ N (ğœ‡1, Î£1), respectively. We set the distribution param-eters to be:ğœ‡0 = [1, 1], Î£0 =( 1 1/21/2 1),ğœ‡1 = [âˆ’1, âˆ’1], Î£1 =( 1 1/101/10 1).(11)The classes in this synthetic dataset are well-separable by a linear model (see Fig. 1)â€¢ Credit Approval tabular dataset (Credit). The task is to predict whether a credit card application should be approved or rejected based on several attributes which describe the application and the applicant.â€¢ Contraceptive Method Choice tabular dataset (Contracep-tion) based on 1987 National Indonesia Contraceptive Preva-lence Survey. The task is to predict the choice of a contra-ception method based on demographic and socio-economic characteristics of a married couple.â€¢ Mammographic Mass tabular dataset (Mammography) collected at the Institute of Radiology of the University Erlangen-Nuremberg in 2003 â€“ 2006. The task is to predict whether a screened tumor is malignant or benign based on several clinical attributes.â€¢ Dermatology tabular dataset. The task is to predict a der-matological disease based on a set of clinical and histopathological attributes.â€¢ CIFAR-10 [23], an image dataset of pictures labeled as one of ten classes. The task is to predict the class.We take the realistic tabular datasets (Credit, Contraception, Mam-mography, and Dermatology) from the University of California Irvine Machine Learning (UCIML) dataset repository [12]. In Ap-pendix B, we provide additional details about processing of the datasets, and a summary of their characteristics (Table 1). For the synthetic dataset, we obtain the training dataset by sampling 1,000 examples from each of the distributions. In order to have precise estimates of population accuracy, we sample a larger test dataset of 20,000 examples. For tabular datasets, we use a random75% subset for training, and use the rest as a held-out test dataset for model evaluations. For CIFAR-10, we use the default 50K/10k Train-test split.Models and Training Algorithms. For the synthetic and tabular datasets, we use logistic regression with objective perturbation [7].For the image dataset, we train a convolutional neural network onScatterNet features [25] using DP-SGD [1], following the approach by Tramer and Boneh [32]. We provide more details in Appendix B.Metrics. The goal of our experiments is to quantify predictive multiplicity and explain the factors which impact it. For all settings,we measure disagreement to capture the dissimilarity of predictions, and predictive performance of the models to quantify the effect of performance on multiplicity. Concretely, we measure:â€¢ Disagreement for examples on a test dataset, computed us-ing the unbiased estimator in Section 4. As this disagreement metric is tailored to binary classification, we use a special procedure for the ten-class task on CIFAR-10: we treat each multi-class classifier as ten binary classifiers, and we report average disagreement across those ten per-class classifiers.Additionally, in Appendix B, we also report predictive multiplicity in terms of confidence scores instead of predictions following the recent approach by Watson-Daniels et al. [36].â€¢ Performance on a test dataset. For tabular datasets, wereport the standard area under the ROC Curve (AUC for short). For CIFAR-10, we report accuracy.Experiment Outline. For a given dataset and a value of the privacy parameter ğœ€, we train multiple models on exactly the same data with different randomization seeds.For the synthetic and tabular datasets, we use several values of ğœ€between 0.5 (which provides a good guaranteed level of privacy [see,e.g. 37, Section 4]) and 2.5, with ğ›¿ = 0. For each value of ğœ€ we trainğ‘š = 5,000 models. For CIFAR-10, we train ğ‘š = 50 neural-network models because of computational constraints. We use DP-SGD parameters that provide privacy guarantees from ğœ€ â‰ˆ 2 to ğœ€ â‰ˆ 7 at the standard choice of ğ›¿ = 10âˆ’5. First, we empirically study how multiplicity evolves with increas-ing privacy. In Fig. 1, we visualize the two-dimensional synthetic examples and their disagreement for different privacy levels. Asprivacy increases, so do the areas for which model decisions exhibit high disagreement (darker areas). Although the regions with higher disagreement correlate with model confidence and accuracy, the level of privacy contributes significantly. For instance, some points which are relatively far from the decision boundary, which means they are confidently classified as either class, can nevertheless have high predictive multiplicity. Fig. 3 shows the experimental results for our tabular datasets and CIFAR-10. On the left side, we show the relationship between the privacy level and performance. On the right, between the pri-vacy level and disagreement. As with the theoretical analysis and the results on synthetic data, we can clearly see that models with higher level of privacy (low ğœ€) invariably exhibit higher predictive multiplicity. Notably, even for datasets such as Mammography And CIFAR-10 for which average disagreement is relatively low,there exist examples whose disagreement is 100%. See Table 2 in the Appendix for detailed information on the distribution of the disagreement values across the test data.Implications. The increase in the privacy level results in making more decisions which are partially or fully explained by randomness in training. Let us give an example with a concrete data record from the Mammography dataset representing a 56-year-old patient labeled as having a malignant tumor. Classifiers with low level of privacy ğœ€ = 2.5 predict the correct malignant class for this individual most of the time (approx. 55% disagreement). If we set the level of privacy to the high ğœ€ = 0.5, this record is classified close to 42% of the time as benign, and 58% of the time as malignant (approx. 97%disagreement). Thus, if one were to use a model with the high level of privacy to inform treatment of this patient, the modelâ€™s decision would have been close in its utility to a coin flip. In the previous section, we showed that the increase in privacy causes an increase in predictive multiplicity. It is not clear, however,what is the exact mechanism through which DP impacts predictive multiplicity. Hypothetically, the contribution to multiplicity could be through two pathways:(1) Direct: The increase in predictive multiplicity is the result of the variability in the learning process stemming from randomization, regardless of the performance decrease.(2) Indirect: The increase in predictive multiplicity is the result of the decrease in performance.These two options are not mutually exclusive, and it is possible that both play a role. In both cases, the desire for a given level of privacyâ€”which determines the degree of randomization added dur-ing trainingâ€”is ultimately the cause of the increase in multiplicity.Nevertheless, how randomization contributes to the increase has practical implications: If our results are explained by pathway (2),we should be able to reduce the impact of privacy on predictive multiplicity by designing algorithms which achieve better accuracy at the same privacy level.For output perturbation, our analysis in Section 3 shows that multiplicity is directly caused by randomizationâ€”pathway (1)â€”asonly the privacy level, confidence, and the norm of a predicted ex-ample impact disagreement. Therefore, performance does not have a direct impact on predictive multiplicity in output perturbation.In Fig. 4, to quantify the impact of performance on predictive multiplicity for the case of objective perturbation, we show the top 5% disagreement values for varying levels of accuracy on the synthetic dataset. We use the synthetic dataset to ensure that test accuracy estimates are reliable, as we have a large test dataset in this case. We see that, for a given level of accuracy, different privacy. parameters can result in different disagreement. This suggests that randomization caused by DP training can have a direct effect on predictive multiplicity, so we observe pathway (1).Implications. This observation indicates that there exist cases for which improving accuracy of a DP-ensuring algorithm at a given privacy level will not necessarily lower predictive multiplicity. The visualizations in Fig. 1 show that different examples can exhibit highly varying levels of predictive multiplicity. This observation holds for real-world datasets too. Fig. 5a shows the distribution of the disagreement values across the population of examples in the test data for tabular datasets. For example, for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity. As the level of privacy increases (low ğœ€), the disagreement tends to concentrate around 1, with decisions for a majority of examples largely explained by randomness in training.Next, we verify if the differences in the level of disagreement also exist across demographic groups. In Fig. 5b, we show aver-age disagreement across points from three different age groups in the Contraception dataset. As before, for low levels of privacy(high ğœ€) we see more disparity in disagreement. The disparities even out as we increase the privacy level (low ğœ€), with groups having average disagreement closer to 1. Thus, disagreement is not only unevenly distributed across individuals, but across salient demo-graphic groups.Implications. As some groups and individuals can have higher predictive multiplicity than others, evaluations of training algorithms in terms of their predictive multiplicity must account for such dis-parities. For instance, our experiments on the Contraception dataset(in Fig. 5b) show that, for different privacy levels, decisions for individuals in the 16â€“30 age bracket exhibit higher predictive multiplicity than of patients between 30 and 40 years old. Predictions For individuals under 30, therefore, systematically exhibit more dependence on randomness in training than on the relevant features for prediction. This highlights the need to conduct disaggregated evaluations as opposed to only evaluating average disagreement on whole datasets.",1613-16,"for lower privacy levels (high ğœ€) on the Contraception dataset, there are groups of individuals with different values of predictive multiplicity"
