,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{widder2023, author = {Widder, David Gray and Zhen, Derrick and Dabbish, Laura and Herbsleb, James},
title = {It’s about Power: What Ethical Concerns Do Software Engineers Have, and What Do They (Feel They Can) Do about Them?},
year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,SoftwareEngineers,Agent,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,SOFTWARE ENGINEERS [...] practitioners,
10,EthicalConcerns,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,ETHICAL CONCERNS,
11,MilitaryApplicationConcerns,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,concerns related to military applications of their work.,
12,IdeologicalConcerns,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,"broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?”",
13,PrivacyConcerns,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,"concerns relating to privacy,most commonly about geotracking",
14,ConcernsAboutSpam,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,concerned about building spam email systems,
15,RacismConcerns,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist.,
16,SurveillanceConcerns,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,described being asked to contribute to systems used to surveil workers or citizens.,
17,EnvironmentalConcerns,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,practitioners included environmental impact,
18,ConcernsAboutLaborMarketEffects,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,the software system could very well put some people out of a job,
19,ConcernsAboutDiscrimination,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,there’s no way they could do this with-out some form of systemic discrimination,
20,LackOfInvestmentInCodeMaintenance,Perceived_Problem,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,tend not to invest in code maintenance as long as the software is minimally functional,
21,NonTechnicalFirms,Agent,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70, ,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,EthicalConcerns,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,SOFTWARE ENGINEERS ETHICAL CONCERNS
26,MilitaryApplicationConcerns,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,17 practitioners wrote about concerns related to military applications of their work.
27,IdeologicalConcerns,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,"several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” "
28,PrivacyConcerns,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,"14 practitioners expressed concerns relating to privacy,most commonly about geotracking"
29,ConcernsAboutSpam,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,Practitioners were concerned about building spam email systems
30,RacismConcerns,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist.
31,SurveillanceConcerns,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,11 respondents described being asked to contribute to systems used to surveil workers or citizens.
32,EnvironmentalConcerns,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,10 practitioners included environmental impact
33,ConcernsAboutLaborMarketEffects,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,I thought the software system could very well put some people out of a job
34,ConcernsAboutDiscrimination,constrainsAgent,SoftwareEngineers,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,"statistically, there’s no way they could do this with-out some form of systemic discrimination"
35,LackOfInvestmentInCodeMaintenance,constrainsAgent,NonTechnicalFirms,"RQ1: WHAT ARE SOFTWARE ENGINEERS ETHICAL CONCERNS? We answer this research question in two ways: firstly, explaining the kinds of ethical concerns raised in our survey most frequently has surfaced by our card sort. Secondly, as a spectrum illustrating the different scopes of practitioners’ concerns, according to how much of their organization’s priorities their concern calls into question.3.1 Kinds of Ethical ConcernsMilitary: 17 practitioners wrote about concerns related to military applications of their work. Of practitioners who disclosed details about the systems they worked on, the most common concerning system was autonomous drone navigation software (n=5): “Workon autonomous drone visual navigation in a GPS-denied environment”(S98). Other respondents develop training software: “software in support of simulations used to train US warfighters” (S161), logistics software for military organizations:“I contributed to the development of a proprietary platform-as-a-service used in defense contracts” (S174) and engineering support software: “an analysis tool that automati-cally finds errors in airplane jet engines.” (S188) Respondents were primarily concerned that their work would physically injure or kill others: “I was concerned whether the software I was contributing to was being used to harm innocent civilians or infringe on human rights” (S174), but several raised broader ideological concerns with the militaries who used their systems, one asking: “am I indirectly contributing to the ills of imperialism?” (S161)Privacy: 14 practitioners expressed concerns relating to privacy,most commonly about geotracking (n=4), one saying they “grab[ed]geolocation data from customers [but] our product doesn’t use geoloca-tions.” (S67). Others were concerned about “stor[ing] user keystrokesin a signup form to a marketing and analytics platform before theuser actually submitted the form” (S272), scraping social media pro-files “as part of additional information to include when making loandecision.” (S114), and privacy involved in data labeling on privatefootage: “contractors [were] to label hundreds of thousands of [homesecurity] video clips” (S26), or requiring personal data “not necessaryto have for the task at hand” (S69).Advertising: 13 survey respondents reported concerns about advertising. Practitioners were concerned about building spam email systems, or “bypass [spam] prevention measures” (S139), concerned that spam breached customers’ privacy (S86), delegitimized email marketing (S139) and did not do good in the world (S230). One respondent wrote that being asked to “develop a computer vision system that accurately classifies someone’s demographics for cus-tomer segmentation marketing” (S78) as something he believed to be inherently racist and sexist. Other practitioners wrote about implementing dishonest interfaces to “push users to buy something because stock was “almost out”” (S2) when in fact it was not, helping to air ads that were“ degrading toward women” (S22), and about advertising “scummy for-profit schools.” (S102)Surveillance: 11 respondents described being asked to contribute to systems used to surveil workers or citizens. Four respondents re-counted concerns about working on existing workplace surveillance and algorithmic management (i.e., [38]) systems, such as “observ-ing how well grocery stockers stayed on task” (S82). Their concerns included “overwork [and] anxiety” (S82), that it might be “illegal to measure employees’ pee time” (P74), and that “low sales numbers”(m)ight be used to unjustly“fire employee[s].” (S10) Other practition-ers were invited to work on surveillance systems for governments.One interviewee (I14) was asked to architect an intelligence gathering platform for a foreign government. Another respondent made improvements to an existing telecom surveillance system (S13).Other practitioners did not build surveillance systems directly, butwere worried their system might be used as such downstream: “the big problem was that I didn’t see a way or a use case, where [facial]identification would be used in a non-ethically problematic way. So Those would be at frontiers, at airports, identification in police stations.”(I14)Environment, Labor Displacement, Inequality, and others: Categories of concerns expressed by less than 10 practitioners in-cluded environmental impact (n=4) “monitoring system for agropecuary[livestock] business [which] is highly damaging to the environment”(S21), labor displacement (n=3) “I thought the software system could very well put some people out of a job” (S201), and exacerbating inequality (n=3) “statistically, there’s no way they could do this with-out some form of systemic discrimination.” (S44) Other harms cited included overcharging customers (S54), contributing to addiction products (S150, S174), cryptocurrency as multi-level marketing(S70), inaccessibility of software (S50), jeopardizing healthcare out-comes (S66), legality (S115, S95), botnets (S133), implementing dark patterns (i.e., [23]) (S100), autonomous vehicle safety (S118), and political manipulation (S104). Some had concerns with the software development process itself: using vulnerable frameworks (S143,S39), underpaid data labelers (S26), or closed-source software (S106). 3.2 Scope of concern: concerned with a bug, or your whole industry?We also found that ethical concerns varied wildly in scope: varying in how much the organization’s goals or priorities a given ethical concern questions. While they overlap, we illustrate this using four scopes of concern: those arising from bugs, intentional features,whole products, and finally concerns which question their organiza-tion’s raison d’être. Scope affected outcomes: concerns questioning entrenched organizational goals were harder to resolve (see Sec. 5.3),and affected the kinds actions practitioners took (Sec. 4).Bugs: Some practitioners described fixing bugs as their core ethical obligation, one saying: “for a software developer, [software] quality is the core of ethics. Because if your product is unreliable, then your representations about the product are probably unethical.” (I17). In Some cases, proposing to fix bugs is uncontroversial, since main-taining intended functionality is often within an organization’s best interest. For example, when a practitioner raised concerns about a bug in construction crane safety, they the practitioner describe how this was enthusiastically received and resolved: “there were a lot of really high profile accidents with lifting cranes [...] Everybodywas really super on edge about making sure that our simulations were correct. [...] And so when I brought that issue up [...] they did a big investigation and found out that it was a data entry error.” (I10)However, organizational incentives can instead stifle practitioners efforts to identify, fix and prevent bugs. For instance, one respondent felt non-technical firms tend not to invest in code maintenance as long as the software is minimally functional: “non-tech companies[...] just care about business continuity” (S81) Another interviewee explained how cost cutting at his consulting firm made it difficult to do work of acceptable quality.A specific feature: Unlike bugs, features were intentional: practitioners were directed to implement them by their manager or client, and therefore questioning them often required more directly questioning their organizations’ objectives. For example, one interviewee was asked to implement a feature that would round downGPS coordinates on properties being evaluated for insurance eli-gibility, which “would have denied people access to certain types of insurance.” (I10) Another interviewee working on workplace com-pliance software reported that his boss asked him to implement a feature that he felt was privacy invasive: “My boss [said] we need to put in a thing on the app so that we can see where people are all the time. And I told him [...] most of the people install it on their personal phone.” (I6) Other concerns arose when practitioners were disallowed from implementing features they felt were ethically im-portant. For example, an interviewee developed ethical concerns about how her product may be exclusionary: “A really famous VRsoftware at the time, had done inclusivity in terms of the color of the skin [...] and allowing for people with one hand to operate it. [...] I Brought it up as an option” (I11), but this was not pursued and she was told “well, nobody asked for it.” (I11)An entire product: Practitioners also surfaced ethical concerns about entire products, or, as consultants, entire contracts they were assigned to. When respondents had concerns about a product's very existence, many felt concern could only be resolved if the product is shut down or dramatically altered. One contractor at a marketing consulting firm was assigned to develop a customer seg-mentation model, to help their client profit from high interest loans: “find[ing] customers that were likely to [...] take on unsustainable amounts of debt.” (I5) In this case, changes to the implementation of the product would not reconcile the practitioner’s concern that building a product to sell “unsustainable” loans was unethical. An-other interviewee reported being assigned onto a project to make improvements on telecom software which he suspected was being used for telecom surveillance: “One of the main managers mentioned that the their main client for the device at the time was AT&T. [...]based on what the device was doing, they figured [...] the main use case[was] NSA tracking.” (I13) In this case, the practitioner’s concern was with misuse of the product he was working on, which could not be resolved until the product was terminated, or its core use cases rethought.An organization’s raison d’être: Finally, some practitioners reported concerns with their organization’s or industry’s goals or business practices. Many practitioners were concerned that their work was used for military purposes, constituting the most com-mon concern type. These included concerns of direct harm, suchas “the software I was contributing to was being used to harm in-nocent civilians” (S174), but also ideological issues, one pondering“am I indirectly contributing to the ills of imperialism?” (S161) Onepractitioner cited his newly-held Buddhist faith as the origin of his concerns that working in the “weapons domain” at all is “really not good karmically” (I1), later reflecting that “if you pay attention to what was going on, like in the wars, it doesn’t have to be so esoteric as like Buddhist precepts.” (I1) One interviewee, working at a fintech firm, felt his work “preventing [fraudulent use] was not really an ethical challenge. The issuewas more than the company as a whole, the business model [...] Itwas, you know, payday lending.” (I2) In this case, the intervieweefelt was concerned about the very reason the company existed,reflecting that this made raising any concerns feel futile: “you’reactually asking to shut down the business. [...] you might as well sayto the founders, like, ‘Hey, either you shut down or I’m leaving’, andthey’ll be like, ‘Alright, leave, I guess.’ It’s not really a concern youcan raise.” (I2) Even firms that offer services instead of products can be held to this level of scrutiny; as one practitioner held thattheir consulting firm’s willingness to do business with shady clients comprised a core part of their business model: “The company [...]does a fair amount of work for [...] oil companies, [...] firearms, [...]British American Tobacco [...] not exactly paragons of morality.” (I5).  ",468-70,non-technical firms tend not to invest in code maintenance as long as the software is minimally functional
