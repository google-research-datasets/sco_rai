,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{bell2023simplicitybiasleads,
    author = {Samuel J. Bell and Levent Sagun},
    title = {Simplicity Bias Leads to Amplified Performance Disparities},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-06, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,FairFace,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,FairFace,
10,RaceLabels,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,race ... labels,
11,AgeLabels,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,age ... labels,
12,GenderLabels,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,gender ... labels,
13,AgeClassificationTask,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,age classification task,
14,WhiteAnnotations,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,white-annotated,
15,BlackAnnotations,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,black-annotated,
16,ResNet,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,a randomly-initialized ResNet-18 model,
17,ResNet,Agent,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,a randomly-initialized ResNet-18 model,
18,Balance,Strategy,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,reasonably balanced,
19,AgeDistributionMatch,Strategy,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,we match the age distribution in each group to remove spurious correlations between race annotation and age annotation,
20,SimplicityPreference,Strategy,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,selectively prioritized other groups that it finds simpler,
21,RacialPerformanceGap,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected",
22,AlgorithmicBias,Perceived_Problem,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,the presence of algorithmic bias,
23,Researchers,Agent,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,we,
24,ModelEvaluation,Artifact,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,evaluate performance disparities between group,
25, , , , , ,
26,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
27,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
28,Researchers,hasProducedArtifact,FairFace,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,FairFace [41] is a dataset of human face images intended for fairness research and audit purpose
29,Researchers,hasProducedArtifact,RaceLabels,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"intended for fairness research and audit purpose ... each sample annotated with per-ceived age, race, and gender labels"
30,Researchers,hasProducedArtifact,AgeLabels,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"intended for fairness research and audit purpose ... each sample annotated with per-ceived age, race, and gender labels"
31,Researchers,hasProducedArtifact,GenderLabels,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"intended for fairness research and audit purpose ... each sample annotated with per-ceived age, race, and gender labels"
32,Researchers,hasProducedArtifact,AgeClassificationTask,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,we construct an age classification task
33,Researchers,hasProducedArtifact,WhiteAnnotations,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"which we describe as, for example, “black-annotated” or “white-annotated”"
34,Researchers,hasProducedArtifact,BlackAnnotations,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"which we describe as, for example, “black-annotated” or “white-annotated”"
35,Researchers,hasProducedArtifact,ResNet,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,We train a randomly-initialized ResNet-18 model
36,Researchers,hasProducedArtifact,ModelEvaluation,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"we construct an age classification task, and evaluate performance disparities between groups,"
37,ResNet,hasProducedArtifact,RacialPerformanceGap,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected"
38,SimplicityPreference,constrainsAgent,ResNet,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,the model has selectively prioritized other groups that it finds simpler.
39,FairFace,influencesPrecept,AlgorithmicBias,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,FairFace serves as a useful illustration of the presence of algorithmic bias
40,FairFace,reflectsPrecept,Balance,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,While FairFace is reasonably balanced
41,FairFace,reflectsPrecept,AgeDistributionMatch,"FairFace [41] is a dataset of human face images intended for fairness research and audit purposes. It comprises a subset of the YFCC-100M Flickr dataset [75], with each sample annotated with per-ceived age, race, and gender labels, and aims to capture a reasonably balanced distribution with respect to race and gender. We provide an extended commentary on the nature of the annotations in § 8.5, though for our purposes, FairFace serves as a useful illustration of the presence of algorithmic bias even when using a balanced dataset. Having discarded information on perceived gender, we construct an age classification task, and evaluate performance disparities between groups, where a group comprises all samples with the same race annotation, which we describe as, for example, “black-annotated” or “white-annotated”. While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match theage distribution in each group to remove spurious correlations between race annotation and age annotation. We train a randomly-initialized ResNet-18 model to classify each sample into one of nine age buckets. We evaluate models trained on each race-annotation group independently, and models trained on all samples together. See appendix D for details. Figure 6a shows the observed (opaque) and estimated (translucent) performance disparity between black-annotated images and other race-annotation groups. For all but one comparison, the observed disparity exceeds the estimated disparity, indicating the presence of difficulty amplification. Due to our balanced and distribution-matched dataset construction, we can confidently say that this is not a result of an imbalanced dataset or group/label associations.In practical terms, this figure shows that this particular model, ResNet-18 trained from scratch, performs worst on black-annotated samples, and crucially that this performance gap is worse-than-expected: the model has selectively prioritized other groups that it finds simpler. In contrast, Figure 6b shows the same comparison of each race-annotation group, but compared against a white-annotated baseline instead. Here, we observe the opposite effect compared with fig. 6a. Observed disparity (opaque) is always lower than estimated dis-parity (translucent), indicating attenuation of difficulty for this group relative to the given model. Unlike the worse-than-expected disparity the model exhibits on black-annotated samples, for white-annotated samples the model demonstrates lower-than-expected disparity. These results reinforce that performance disparities are complex and hard to predict, and may have heterogeneous impact across different groups.",360-1,"While FairFace is reasonably balanced, we further apply subsampling to precisely equalize the number of samples in each group, and we match the age distribution in each group to remove spurious correlations between race annotation and age annotation."
