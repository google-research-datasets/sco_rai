,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{radiya-dixit2023asociotechnicalaudit,
    author = {Evani Radiya-Dixit and Gina Neff},
    title = {A Sociotechnical Audit: Assessing Police Use of Facial Recognition},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-07, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,LFRDeployment,Artifact,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,operational trial deployments of live facial recognition,
10,SWP,Agent,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,South Wales Police,
11,CourtOfAppeal,Agent,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,Court of Appeal,
12,RulingOfUnlawfulness,Artifact,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,ruled that these deployments were unlawful,
13,LackOfGuidance,Perceived_Problem,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,no clear guidance,
14,Unlawfulness,Perceived_Problem,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,these deployments were unlawful,
15,ImpactAssessmentDeficiency,Perceived_Problem,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,data protection impact assessment was deficient,
16,IgnoranceOfPotentialBias,Perceived_Problem,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,did not take reasonable steps to find out if the software had a racial or gender bias,
17,Researchers,Agent,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,our,
18,AuditRevealingConcerns,Artifact,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case,
19,InterferenceWithExpression,Artifact,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,interfering with the human rights to freedom of expression and assembly,
20,DisparatePeformance,Artifact,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,does not perform well or similarly across demographic groups,
21,LackOfOversight,Perceived_Problem,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,a lack of effective oversight,
22,NoExperts,Perceived_Problem,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"no independent experts in human rights, equality, or data protection in attendance",
23,LackOfIndependence,Perceived_Problem,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,concerns about the committee’s independence,
24,NoPublicConsultation,Perceived_Problem,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"no consultations with the public, especially marginalized communities",
25, , , , , ,
26, , , , , ,
27,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
28,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
29,SWP,hasProducedArtifact,LFRDeployment,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP)
30,SWP,hasProducedArtifact,InterferenceWithExpression,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly"
31,SWP,hasProducedArtifact,DisparatePeformance,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"SWP’s data protection impact assessment and policy documents did not acknowledge nor address LFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups"
32,CourtOfAppeal,hasProducedArtifact,RulingOfUnlawfulness,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,the Court of Appeal ruled that these deployments were unlawful
33,Researchers,hasProducedArtifact,AuditRevealingConcerns,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case
34,LackOfGuidance,constrainsAgent,SWP,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist"
35,Unlawfulness,constrainsAgent,Researchers,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful"
36,ImpactAssessmentDeficiency,constrainsAgent,SWP,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that [...] a  data protection impact assessment was deficient"
37,IgnoranceOfPotentialBias,constrainsAgent,SWP,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that [...] the force did not take reasonable steps to find out if the software had a racial or gender bias"
38,LackOfOversight,constrainsAgent,SWP,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"While SWP had early engagements with the SWP Joint Independent Ethics Committee, regular and transparent oversight was not provided"
39,NoExperts,constrainsAgent,SWP,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance"
40,LackOfIndependence,constrainsAgent,SWP,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"Moreover, there remained concerns about the committee’s independence"
41,NoPublicConsultation,constrainsAgent,SWP,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"there were no consultations with the public, especially marginalized communities"
42,RulingOfUnlawfulness,reflectsPrecept,LackOfGuidance,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,the Court of Appeal ruled that these deployments were unlawful as [...] there was no clear guidance on where [LFR] could be used and who could be put on a watchlist
43,RulingOfUnlawfulness,reflectsPrecept,ImpactAssessmentDeficiency,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,the Court of Appeal ruled that these deployments were unlawful as [...] a data protection impact assessment was deficient
44,RulingOfUnlawfulness,reflectsPrecept,IgnoranceOfPotentialBias,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,the Court of Appeal ruled that these deployments were unlawful as [...]  force did not take reasonable steps to find out if the software had a racial or gender bias
45,AuditRevealingConcerns,reflectsPrecept,LackOfOversight,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case [...] there was a lack of effective oversight over the use of LFR.
46,AuditRevealingConcerns,reflectsPrecept,NoExperts,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case [...] no independent experts in human rights, equality, or data protection in attendance"
47,AuditRevealingConcerns,reflectsPrecept,LackOfIndependence,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case [...] concerns about the committee’s independenc
48,AuditRevealingConcerns,reflectsPrecept,NoPublicConsultation,"Our first case is the operational trial deployments of live facial recognition (LFR) conducted by South Wales Police (SWP) fromMay 2017 to April 2019. In R (Bridges) v. Chief Constable of SouthWales Police, the Court of Appeal ruled that these deployments were unlawful as ""there was no clear guidance on where [LFR] could be used and who could be put on a watchlist, a data protection impact assessment was deficient and the force did not take reasonable steps to find out if the software had a racial or gender bias"" [41, 82]. As shown by the scorecard summary in Table 1, our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case. First, SWP did not establish limits on the use of LFR at assemblies. In fact, the technology was used at a peaceful anti-arms protest [7,14], interfering with the human rights to freedom of expression and assembly, without evidence that the legal requirement of ""necessary in a democratic society"" was met. SWP’s data protection impact assessment and policy documents did not acknowledge nor addressLFR’s impact on the rights to freedom of expression and assembly. Second, LFR does not perform well or similarly across demographic groups. Out of the matches that LFR generated, only 24%were verifiably correct. There was also a higher false positive rate for women (82%) compared to men (66%). This raises serious concerns that people faced unwarranted police interventions due to misidentifications. Additionally, there was a lack of effective oversight over the use of LFR. While SWP had early engagements with the SWP Joint Inde-pendent Ethics Committee, regular and transparent oversight was not provided throughout the lifecycle of the LFR project. During committee meetings, there were no independent experts in human rights, equality, or data protection in attendance, even though such expertise has been documented as crucial for the oversight of technologies such as LFR [54, 74, 93, 99]. Moreover, there remained concerns about the committee’s independence. Although there were some independent members, the committee also included police officers and is a body situated within the police force. In fact, during meetings, 63% of attendees were members of SWP and 71% were members of either SWP or the SouthWales Police and Crime Commissioner. Finally, there were no consultations with the public, especially marginalized communities, on how and whether LFR was implemented.",1339-40,"our sociotechnical audit revealed additional legal and ethical concerns beyond the scope of the court case [...] no consultations with the public, especially marginalized communities"
