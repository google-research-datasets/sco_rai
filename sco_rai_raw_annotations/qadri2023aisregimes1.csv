,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{qadri2023aisregimes,
    author = {Rida Qadri and Renee Shelby and Cynthia L. Bennett and Emily Denton},
    title = {AI’s Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-09, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,ResponsibleAIFrameworks,Artifact,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,responsible AI frameworks,
10,FlawedDataAndModels,Artifact,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,"lawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies",
11,NonPortableWesternFrameworks,Perceived_Problem,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,The non-portability of Western frameworks ,
12,CulturalHarms,Artifact,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,cultural harms ,
13,ParticipatoryPractices,Strategy,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,incorporate participatory practices,
14,WesternDominance,Perceived_Problem,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,the dominance of Western perspectives and experiences,
15,NonWesternCommunities,Agent,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,non-Western communities,
16,LackOfKnowledge,Perceived_Need,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,"foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100]",
17, , , , , ,
18, , , , , ,
19, , , , , ,
20, , , , , ,
21, , , , , ,
22, , , , , ,
23,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
24,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
25,ResponsibleAIFrameworks,reflectsPrecept,WesternDominance,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks
26,FlawedDataAndModels,reflectsPrecept,NonPortableWesternFrameworks,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,"The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]."
27,NonPortableWesternFrameworks,constrainsAgent,NonWesternCommunities,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,"When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities"
28,CulturalHarms,reflectsPrecept,LackOfKnowledge,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,"Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100]"
29,ParticipatoryPractices,constrainsAgent,NonWesternCommunities,"There is a growing body of scholarship calling attention to the dominance of Western perspectives and experiences embedded within responsible AI frameworks [60, 84, 86, 99, 100, 102], which are not transferable across cultural contexts [8, 118]. The non-portability of Western frameworks can lead to flawed data and model assumptions, evaluation methods that overlook culturally-specific axes of discrimination, and cultural incongruencies [86, 101, 102]. When operationalized in model testing and evaluation, exclusive use of Western-oriented frameworks risks development of applications that dispossess the identity of non-Western communities [76], by centralizing the epistemologies used and power to build algorithmic systems in the hands of a global minority [50]. Compared to other AI harms, such as representational or allocative harms, much less attention has been devoted in computing literature to understanding cultural harms, leaving these “under articulated” in the field [105,p. 18]. Current approaches to understanding cultural harms focuses on how they can foreclose ways of understanding the social world [95], leading to systemic erasure [38], proliferating false ideas about cultural groups [100], and exporting Western ideas to theGlobal South [76]. However, the nuanced ways these take shape for different non-Western communities are not well-understood. More globally inclusive and community-centered approaches to AI fairness and cultural harms require recontextualizing data and model evaluation — with an explicit incorporation of contextual axes of discrimination [102]. Particularly there are calls to meaningfully center different global communities and institutions in knowledge production processes [5, 104] and incorporate participatory practices that allow production of ML frameworks by impacted communities. [102]. Combined with community-centered research, ML practices that center reciprocity, reflexivity, and empowerment can help reshift power dynamics between technologists and marginalized communities [16, 61].",507,incorporate participatory practices that allow production of ML frameworks by impacted communities.
