,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{panigutti2023, author = {Panigutti, Cecilia and Hamon, Ronan and Hupont, Isabelle and Fernandez Llorca, David and Fano Yela, Delia and Junklewitz, Henrik and Scalzo, Salvatore and Mazzini, Gabriele and Sanchez, Ignacio and Soler Garrido, Josep and Gomez, Emilia}, title = {The Role of Explainable AI in the Context of the AI Act}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,NeedForAIProctoring,Perceived_Problem,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,AI-based student proctoring,
10,OpenFace,Agent,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,Openface 2.0,
11,FacialData,Artifact,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,FacialData,
12,ExtractionSuperimposedVideo,Artifact,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,a video with the extracted facial information superimposed over the original input video ,
13,LogsWithConfidenceIntervals,Artifact,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10),
14, , , , , ,
15, , , , , ,
16,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
17,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
18,NeedForAIProctoring,constrainsAgent,OpenFace,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,AI-based student proctoring [...] an example of these systems is OpenFace 2.0
19,OpenFace,hasProducedArtifact,FacialData,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,"OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs)."
20,OpenFace,hasProducedArtifact,ExtractionSuperimposedVideo,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,"The tool provides two different outputs: a video with the extracted facial information superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10)"
21,OpenFace,hasProducedArtifact,LogsWithConfidenceIntervals,"This section aims at showcasing how some technical and non-technical measures can address the opacity of AI systems in the context of the AI Act through the example of a specific AI system: AI-based student proctoring 9.6.1 Use case descriptionThe main computational task of these systems is usually based on facial behavior analysis. This implies taking facial images or videos as input data and perform some AI-driven processing to in-fer high-level information of a person’s facial behavior including micro-expressions, emotions, gaze direction or head pose. Facialbehaviour analysis algorithms may be used in a wide range of appli-cations areas [58]. An example of these systems is OpenFace2.0 [14],which provides facial landmark detection (FL), head pose estimation(HP), gaze direction analysis (GA), and the intensity of activation of a set of 18 facial micro-expressions called Action Units (AUs). AUs encode the movements of facial muscles and their intensity according to the Facial Action Coding System (FACS [35]). Examplesof AUs include: brow lowerer (AU4), cheek raiser (AU6) or lip corner puller (AU12). Thus, AUs provide an objective and fine-grained description of a person’s facial behavior. This toolkit combines a pipeline of deep learning and other machine learning techniques with real-time video processing capabilities, and is able to run from a simple webcam and in any type of hardware. The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10) Online proctoring software is usually complemented by other AI-based computational tasks for, for example, the detection of other persons in the room or of relevant objects such as books,notes or mobile phones. In addition, online proctoring systems usu-ally integrate voice detection systems. The outputs of these online proctoring systems often include alerts for suspicious behavior, which are triggered to instructors for review and action. Further-more, their functionality may go much further: the systems often incorporate a user interface including mechanisms for the instruc-tor to review the elements that have triggered the alert and allow for real-time communication with the students, either via chat or audio. That is, we can consider a system for online proctoring as containing multiple components and computational tasks, some of them AI-based and some of them not (see Figure 2). ",1145,"The tool provides two different outputs: a video with the extracted facial informa-tion superimposed over the original input video (Figure 1-top), and a CSV file with detailed logs including confidence values on the predictions made (Figure 1-bottom 10)"
