,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{nannini2023explainabilityinai,
    author = {Luca Nannini and Agathe Balayn and Adam Leon Smith},
    title = {Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-08, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,WhiteHouse,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,White House,
10,EOOnAI,Artifact,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Executive Order (EO 13859) on Maintaining American Leadership in AI,
11,AIStrategy,Strategy,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,national R&D strategy to keep up its global competitiveness,
12,AIAdvocacy,Perceived_Problem,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"advocated for ethical, responsible, and transparent development of AI",
13,AIGuidance,Artifact,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Guidance for Regulation of AI Applications,
14,OMB,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Office of Management and Budget,
15,AIStandards,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable",
16,RegulatoryProposal,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments.",
17,NIST,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,NIST,
18,Mandate,Artifact,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities,
19,DiscountedExplainability,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,explainability is ascertained below the concept of trustworthiness,
20,FinalReport,Artifact,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Final Report,
21,AISecurityComission,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,National Security Commission on AI,
22,LackOfInformedPerspective,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,reflect a lack of informed perspective over AI explainability for civil rights,
23,AIRightsBlueprint,Artifact,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Blueprint for the AI Bill of Rights,
24,OSTP,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Office of Science and Technology Policy,
25,PerceptionOfCivilRightsFramework,Causal_Theory,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties",
26,JustificationConditions,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks.",
27,Legislature,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,legislative,
28,NationalAIAct,Artifact,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,National AI Initiative Act,
29,DutyForRiskManagement,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107]",
30,TrustworthinessDefinitions,Causal_Theory,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,common definitions and characterizations of aspects of AI trustworthiness such as explainability,
31,AlgorthimicAccountabilityAct,Artifact,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Algorithmic Accountability Act,
32,Companies,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,companies,
33,ImpactAssessmentRequirement,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,required to perform impact assessments,
34,ExplanationRequirement,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,requirements for business explanations over data collection and maintenance,
35,EvaluationStandards,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,evaluation standards,
36,EndUsers,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,end-users,
37,SystemExplanations,Causal_Theory,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,explanations of system features contributing to the decision output,
38,SystemInformation,Causal_Theory,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,information about the system and process,
39,NeedToEngage,Perceived_Need,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability,
40,Stakeholders,Agent,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,stakeholders,
41, , , , , ,
42,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
43,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
44,WhiteHouse,hasProducedArtifact,EOOnAI,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House
45,OMB,hasProducedArtifact,AIGuidance,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Guidance for Regulation of AI Applications issued by the Office of Management and Budget
46,NIST,hasProducedArtifact,Mandate,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities."
47,AISecurityComission,hasProducedArtifact,FinalReport,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Final Report [118] issued by the National Security Commission on AI
48,OSTP,hasProducedArtifact,AIRightsBlueprint,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy
49,Legislature,hasProducedArtifact,NationalAIAct,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,legislative output in the enactment of the National AI Initiative Act
50,Legislature,hasProducedArtifact,AlgorthimicAccountabilityAct,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,legislative output [...] National AI Initiative Act
51,AIStrategy,constrainsAgent,WhiteHouse,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness
52,AIAdvocacy,constrainsAgent,WhiteHouse,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI."
53,LackOfInformedPerspective,constrainsAgent,AISecurityComission,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,found a wider output in the Final Report  [...] These strategic documents reflect a lack of informed perspective over AI explainability for civil rights
54,PerceptionOfCivilRightsFramework,constrainsAgent,OSTP,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties."
55,DutyForRiskManagement,constrainsAgent,NIST,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107]"
56,TrustworthinessDefinitions,constrainsAgent,NIST,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability."
57,ImpactAssessmentRequirement,constrainsAgent,Companies,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,companies deploying such systems will be required to perform impact assessments
58,ExplanationRequirement,constrainsAgent,Companies,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance
59,EvaluationStandards,constrainsAgent,Companies,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards
60,SystemExplanations,constrainsAgent,EndUsers,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,there is a need to deliver end-users explanations of system features contributing to the decision output
61,SystemInformation,constrainsAgent,EndUsers,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process"
62,NeedToEngage,constrainsAgent,Stakeholders,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability.
63,EOOnAI,reflectsPrecept,AIStrategy,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness
64,EOOnAI,reflectsPrecept,AIAdvocacy,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. "
65,AIGuidance,reflectsPrecept,AIStandards,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable"
66,AIGuidance,reflectsPrecept,RegulatoryProposal,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments."
67,FinalReport,reflectsPrecept,LackOfInformedPerspective,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,National Security Commission on AI  [...] These strategic documents reflect a lack of informed perspective over AI explainability for civil rights
68,AIRightsBlueprint,reflectsPrecept,PerceptionOfCivilRightsFramework,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties"
69,NationalAIAct,reflectsPrecept,DutyForRiskManagement,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107]"
70,NationalAIAct,reflectsPrecept,TrustworthinessDefinitions,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability."
71,AlgorthimicAccountabilityAct,reflectsPrecept,ImpactAssessmentRequirement,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments"
72,AlgorthimicAccountabilityAct,reflectsPrecept,ExplanationRequirement,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance "
73,AlgorthimicAccountabilityAct,reflectsPrecept,EvaluationStandards,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards."
74,AlgorthimicAccountabilityAct,influencesPrecept,SystemExplanations,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Algorithmic Accountability Act [...] Also, there is a need to deliver end-users explanations of system features contributing to the decision output,"
75,AlgorthimicAccountabilityAct,influencesPrecept,SystemInformation,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,Algorithmic Accountability Act [...]  further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability.
76,AlgorthimicAccountabilityAct,influencesPrecept,NeedToEngage,"Strong in its position as a global technological powerhouse, the US AI regulatory trajectory started to gain momentum in February of 2019, when the Executive Order (EO 13859) on Maintaining American Leadership in AI was announced by the White House [108]. Despite the nature of the order addressing specifically a national R&D strategy to keep up its global competitiveness, the policy tone thereby advocated for ethical, responsible, and transparent development of AI. This bal-ance informed the Guidance for Regulation of AI Applications issued by the Office of Management and Budget (OMB) later in 2020 [109].The Guidance stated that AI systems should be designed accoringly to be trustworthy, auditable, and thus explainable - especially for the mentions of transparency and disclosure reported in section(8.). The interpretability of AI systems is also briefly announced in the Appendix to enhance transparency for oversight. But rather than leveraging XAI methodologies, indirect regulatory processes are proposed for that aim, i.e., impact analysis, public consultation,and risk assessments. A more structured answer to EO 13859 was elaborated by NIST. Indeed, the Order mandated the agency to develop a plan [106] to tackle Federal priorities for a robust and safe AI R&D while promoting international standardization activities. In the plan, explainability is ascertained below the concept of trustworthiness, one of nine key areas of focus identified for AI standards. These governmental policy communications found a wider output in the Final Report [118] issued by the National Security Commission on AI (NSCAI) on March 1st, 2021. The federal commission, created in August 2018 and dismantled in October2021, had the mandate to advise the US President and Congress on AI R&D for national security and defense needs, reflecting the willingness to maintain its technological competitiveness through workforce development, international cooperation, and AI ethics.There, explainability is only mentioned among areas of R&D, alongside remarks for the transparency and accountability of AI deployment in national security applications. These strategic documents reflect a lack of informed perspective over AI explainability for civil rights, filled only partially by the release in October 2022 of the Blueprint for the AI Bill of Rights by the Office of Science and Technology Policy (OSTP) [104]. Rather than industry oriented, the Blueprint can be seen as a civil rights framework for ensuring that automated decision-making systems (ADMs) are used in ways that respect American values such as privacy, autonomy, and other civil liberties. Under the principle of Notice and Explanation, automated decisions shall be justified through ""clear, timely, understandable, and accessible"" use in connection to valid explanations tailored to purpose, audience target, and level of risks. In terms of bills, the US R&D AI trajectory found legislative output in the enactment of the National AI Initiative Act on January 1st, 2021 [2]. The Act remarked the duty (Sec. 22A(c)(2)) to establish, within NIST a voluntary risk management framework by 2023[107], detailing also common definitions and characterizations of aspects of AI trustworthiness such as explainability. Yet for civil rights and litigation in March of 2022 a relevant bill was introduced for possible enforcement of explanations within ADM/AI systems. In Section 4 of the Algorithmic Accountability Act [3], companies deploying such systems will be required to perform impact assessments. Among the most impacting provisions figure requirements for business explanations over data collection and maintenance (Sec. 4(7)(A)(ii)) and evaluation standards. Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process (Sec. 4(8)(B)(i)). Interestingly, provision Sec. 4(11)(C) further addresses the need to engage stakeholders to provide feed-back on improvements for the ADM/AI system and also for their explainability. To conclude, in the context of Commission oversight,Sec.5(1)(H)(i) mandates the submission of documentation from the impact assessment over system transparency and explainability measures.",1202,"Algorithmic Accountability Act [...] Also, there is a need to deliver end-users explanations of system features contributing to the decision output, as well as overall information about the system and process"
