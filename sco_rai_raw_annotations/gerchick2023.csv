,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{gerchick2023, author = {Gerchick, Marissa and Jegede, Tobi and Shah, Tarak and Gutierrez, Ana and Beiers, Sophie and Shemtov, Noam and Xu, Kath and Samant, Anjana and Horowitz, Aaron}, title = {The Devil is in the Details: Interrogating Values Embedded in the Allegheny Family Screening Tool}, year = {2023}}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-13, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Researchers,Agent,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,we,
10,Developers,Agent,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,developers,
11,PostProcessingDecisions,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,series of post-processing decisions [95] related to the aggregation and communication of the AFST’s outputs,
12,HouseholdAggregation,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,the household aggregation occurs;,
13,ProtocolPolicy,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"Under a protocol policy like that currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral ",
14,RacialDisparity,Perceived_Problem,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households",
15,AnalysisOfTrainingData,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family,
16,JuvenileProbationRecord,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral",
17,EverInFeatures,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,he use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems,
18,Referrals,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,referrals,
19,WelfareWorkers,Agent,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,child welfare workers,
20,AnalysisOfTrainingData,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features.",
21,AlleghenyFamilyScreeningTool,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,the AFST,
22,RiskScorePresentationDecisions,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs ",
23,RiskScores,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,risk scores between 1 and 20,
24,PolicyDecisions,Strategy,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,these decisions are effectively policy choices,
25,ComplexInterpretation,Perceived_Problem,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,the interpretation of scores for each child is somewhat complex before the household aggregation occurs,
26,AlleghenyCounty,Agent,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,the County,
27,AlgorithmicRecourse,Strategy,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,opportunities for recourse and contesting the tool’s predictions.,
28,ReductionInDisparity,Artifact,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11%,
29, , , , , ,
30, , , , , ,
31,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
32,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
33,Researchers,hasProducedArtifact,AnalysisOfTrainingData,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features."
34,Developers,hasProducedArtifact,AlleghenyFamilyScreeningTool,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"In creating the AFST, the developers of the tool"
35,Developers,hasProducedArtifact,RiskScorePresentationDecisions,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs "
36,Developers,hasProducedArtifact,RiskScores,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20
37,PostProcessingDecisions,reflectsPrecept,PolicyDecisions,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,series of post-processing decisions [95] related to the aggregation and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices
38,HouseholdAggregation,influencesPrecept,ComplexInterpretation,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level
39,ProtocolPolicy,influencesPrecept,RacialDisparity,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"Under a protocol policy like that currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size."
40,RacialDisparity,constrainsAgent,AlleghenyCounty,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size."
41,AnalysisOfTrainingData,influencesPrecept,RacialDisparity,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results"
42,JuvenileProbationRecord,influencesPrecept,RacialDisparity,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims."
43,EverInFeatures,influencesPrecept,AlgorithmicRecourse,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions."
44,Referrals,reflectsPrecept,RacialDisparity,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral"
45,WelfareWorkers,hasProducedArtifact,ReductionInDisparity,"We analyze de-identified data produced in response to our data request by the Allegheny County Department of Human Services.3 The data comprised approximately the same number of unique child-referral records from 2010 to 2014 as described in [98, p. 10]. Ap-proximately the same number of records as described in [98, p. 10]had been screened in for investigation. Amongst those screened in,roughly 70% had been designated by the County as training data and 30% were designated as testing data. The data was very similar to the data used to train the version of the AFST described in [98](we refer to the version described in [98] as AFST V2). The data differed slightly from the AFST V2 training data because of changes in the data that had occurred since the AFST V2 was developed. In Particular, the data we were provided contained a few thousand child-referral records that were not used to train the AFST V2, and we were missing a small number of child-referral records that were used in the training process for the AFST V2. Both of these data sets contained roughly 800 variables, including, for each family, informa-tion about prior referrals and child welfare records, jail and juvenile probation records, behavioral health information, birth record in-formation, and demographic information. These variables include indicators for whether a child on a referral is labeled as an “alleged victim” with regard to the referral to the hotline. Throughout our analysis, we use the term “alleged victim” to refer to individuals with this indicator, and we note that in the documentation for theAFST V1, the developers describe the County’s labeling of which child or children on the referral are indicated as the alleged victim(s)as “somewhat arbitrary” because County staff are required to assess all children on a referral [99, p. 14].In addition to this data, we were provided with weights and information about several versions of the AFST. This information included the weights for the model corresponding to Version 1 of the AFST (described in [99]) and the weights for Version 2 of theAFST (as described in [98]). We were also given the weights for the model in use at the time we received data (in July 2021), which was developed using the procedures described in [98], but differs slightly from the weights described in [98] because of updates to the model (and therefore the weights) in the time period between when[98] was written and when data was shared with us by the County.In this work, we refer to the iteration of the tool that we analyzed as FAST V2.1 (this is our term, not the County’s, which we use to distinguish from AFST V2 described in [98]). We were also provided with three months of production data from early 2021 for AFST V2 and with information about the weights and design process for the version of the tool in use in Allegheny County at the time of writing of this paper (which we refer to as AFST V3). We did not have the training data or production data that corresponded to AFST V1 orV3, and accordingly, we conducted the vast majority of our analysis using the AFST V2.1 weights that were in use at the time data was shared with our team. During the time that the AFST V2 and 2.1were in use in the County, the “protocols” associated with AFSTrisk scores changed several times (see [84, p. 7] for further details about the changes over time). In another paper, which has not yet been released, we explore the policy impacts of these changing protocols. For our analyses, we use the protocol most recently in place in the County associated with AFST V2 and V2.1 — and to our knowledge, also currently in place at the time of writing forAFST V3, which applies the:• “High-risk protocol” to referrals where at least one person in the household has an AFST score of 18 or higher and there's at least one child on the referral under age 16. Referrals in this “protocol” are subject to mandatory screen-in, unless a supervisor overrides that default policy [98, p. 6].• “Low-risk protocol” to referrals where all AFST scores in the household are 12 or lower and all children on the referral are at least 7 years old [84, p. 7]. For referrals in this protocol,screen-out is recommended.• “No-protocol” to referrals that do not qualify as either “high-risk” or “low-risk.” This protocol is not associated with explicit screening decisions; discretion is left to call screeners[98, p. 6]. There is no “medium-risk” protocol.Though the County is using AFST V3 as of the time of writing,to our knowledge, several of the design decisions that we analyze in this work are still shaping this most recent version of the model.Further details about the tool and the training data are included in the model card [64] we developed for the AFST, included asAppendix C. To understand the development and use of the AFST, we conducted an exploratory analysis of the training data used to develop the model, including an examination of the context of data sources,the processes used to construct features, and racial disparities in those features. Our analysis focused, in part, on racial disparities between Black individuals and households and non-Black individ-uals and households represented in this data, a grouping we used to align with the developers’ reporting of results in [98]. We also conducted a review of documents related to the AFST to understand how policy choices like the screening recommendations associated with each “protocol” were made and promulgated in the context of the AFST. Based on this exploration, we selected three values embedded in the development of the AFST to highlight through the lens of design decisions related to data collection, feature selection,and the post-processing of model outputs. We analyze the impact of these design decisions on screen-in rates, racial disparities, and some of the metrics used in the development process to evaluate the AFST’s performance. Our use of various metrics — including Area Under the Curve( AUC), the Cross-Area Under the Curve (xAUC, explained in [47])and False Positive Rates (FPR) — is not intended to suggest how the AFST should have been developed or to analyze whether thetool is fair. Reporting results for these purposes would require a complex understanding of the values that are embedded in these metrics. For example, how can we understand the tool’s accuracy or evaluate when it makes errors when the tool predicts future agency actions, and the ground truth outcome upon which such results are based (whether a child is removed from their home) is informed by the tool, creating a feedback loop? We do not seek to answer such questions here. Rather, where possible, we use many of the same metrics that the County and development team used to justify the tool’s creation and adoption to highlight how these design decisions have a significant effect on the tool’s performance as assessed by its developers, even if we disagree with the values embedded in their assessments of the tool. In creating the AFST, the developers of the tool made several consequential decisions about how to present risk scores to screening staff, ultimately transforming the model’s outputs — predicted prob-abilities for individual children — into the format shown to call screeners — a single risk label or numeric score between 1 and 20 representing all children on a referral. In this section, we analyze this series of post-processing decisions [95] related to the aggrega-tion and communication of the AFST’s outputs. We argue first that these decisions are effectively policy choices, and that the BEST method of grouping risk scores presents a misleading picture of families evaluated by the tool, treating families as “risky” by association, even when the risk scores of individual family members may be perceived as low. Viewing these decisions as policy choices, we highlight several additional ways these decisions could have been analyzed throughout the AFST’s design and deployment process,which produce varying pictures of how the tool performs.4.1.1 The AFST’s method of grouping risk scores. In the first public development report about the AFST, the tool’s developers wrote that“of considerable debate and discussion were questions surrounding how to present the risk scores to hotline screening staff” [99, p. 27]. Ultimately, the developers decided to transform the AFST’s predicted probabilities into risk scores between 1 and 20, where each score represents five percent of the child-referral combinations in the testing data used to develop the model (i.e., using ventiles) [98,p. 10]. Perhaps worth noting is that in their 2018 analysis of theAFST, Chouldechova et al. characterize the choice of ventiles for the FAST as “not a principled decision” [20, p. 5]. For an example of how to interpret an AFST score, for a referral occurring in 2021,when the AFST V2.1 was in use, a score of 20 for a child indicates that the estimated probability of removal for that child was within the top five percent of probabilities in the testing data used to develop the model, which was based on referrals made between 2010 and 2014. Here, being in the top five percent is a relative measure —as highlighted in other analyses of the AFST [20], individuals who receive the highest possible risk score experience removal less than 50% of the time, a result that might differ starkly from intuitive interpretations of a score in the top five percent In addition to using ventiles for the scores, the FAST aggregate risk scores for all children in a household, presenting a single score or label that represents an entire family to call screeners. Though Predictions are generated at the child level for each referral, call screeners either see only the maximum score across all children on a referral or a single risk label (e.g., “high-risk”) that is determined in part by the maximum score of all children on the referral. Aggregation of risk scores sometimes occurs in other settings, including in the context of pretrial risk assessments in the criminal legal system, where researchers have repeatedly raised concerns about the combination of risk scores related to predictions of different outcomes for the same person [35, 62]. In the context of the AFST, the interpretation of scores for each child is somewhat complex before the household aggregation occurs; this interpretation is further muddied by the aggregation of scores at the referral level. A score of 20 for a referral means that, for at least one child in the household,the estimated probability of removal is within the top five percent of probabilities in the testing data from 2010 - 2014. Imagine a referral related to a hypothetical family with three children, aged 5, 10, and 15 respectively, with AFST scores of 5,10, and 18. One child, the five-year-old child with a risk score of5, is labeled as the alleged victim by the County on the referral. How could this information be communicated to the call screener for the referral? As noted in Section 3, the County has a policy of evaluating all of the children on a referral when a call is received— not just those indicated as alleged victims — and this policy pre-dates the AFST [99, p. 14]. But the existence of this policy alone does not answer this question of how scores are communicated to call screeners. For example, one option would be to show eachchild’s individual score to the call screener, for a total of three scores. Or, with a constraint of only showing one score, the AFSTcould have displayed the score of the alleged victim (a score of 5),or the maximum score of all children (a score of 18), or a label such as “high-risk” for the entire household based on the score and the children's ages, akin to the County’s current protocol policy. Under The policy that, to our knowledge, is currently in use in the County,this family would be grouped into the “high-risk protocol.”Each of these methods would have significant implications for the distribution of risk scores in the testing data used to develop the model. As highlighted in Figure 1, scoring policies that assign a single score to the entire household confuse the interpretation of the ventile scores.4 Under the individual score policy (shown in the first panel of Figure 1), each numeric risk score generally corresponds to roughly 5% of the individuals in the data, and similar percentage of Black and non-Black families are assigned each numeric risk score (with the exception of scores 1 and 20). But under the policies that produce one score for each household (shown in the second and third panels of Figure 1), this distribution is heavily shifted upwards and disparately shifted for Black households.5 For these policies, risk scores generally increase for everyone compared to the individual score policy. But racial disparities in the higher-score ranges are severely exacerbated by the household score policies —non-Black families are more likely to have scores below 10, andBlack families are more likely to have scores above 10, with severe disparities at the numeric score of 20. Under a protocol policy likethat currently in use in the County — where families are assigned to either the “high-risk protocol,” “low-risk protocol,” or “no-protocol”based on the maximum score in the household and the ages of the children on the referral (shown in the fourth panel of Figure 1)— 33% of Black households would have been labeled “high-risk,”compared to 20% of non-Black households. Household size 6 does not account for these disparities; Black households are, on average,assigned higher risk scores than non-Black households of the same size. 4.1.2 Ways of measuring the AFST’s performance. In the FAST development reports, the tool’s developers generally present results about the performance of the AFST using individual scores and measuring individual outcomes, examining whether, for each child, a removal occurred within two years of a referral. These results inform key design decisions, including the modeling approach ultimately selected for the tool [98], but there is a mismatch between how the tool’s results are reported (at the individual level) and how the tool is actually deployed — where each household receives only one score. To evaluate the impact of this mismatch, we define and analyze six different ways that risk scores could have been communicated and that outcomes could have been measured in the context of the AFST, which we refer to for this analysis as policies.These six policies — which could be shared and debated as formal policies about risk scores and household treatment — represents the possible combinations of several different score aggregation methods (using individual scores, maximum household scores, the alleged victim child’s score, or household “protocols”) and outcome measurements (measuring outcomes at the individual level or the household level, examining whether a removal occurred for any of the children on the referral within two years). The specifics of these combinations and a description of how scores and outcomes would be measured under each policy for the hypothetical family discussed in the previous section are included in Appendix A.1.For each of these “policies,” we evaluate the AFST’s performance with metrics that were used in the tool’s development reports and analysis about the tool, including the area under the receiver operating characteristic (ROC) curve (AUC) as in [98] and False PositiveRates (FPR) as defined in [20] for the policies that output ventilescores (the definition of a false positive for the policies that output protocols is included in Appendix A.1). We also generate results using the Cross-Area Under the Curve (xAUC) metric and asso-ciated Cross-Receiver Operating Characteristic (xROC) curve as proposed by Kallus and Zhou [47], which recognizes that predic-tive risk scores are often used for ranking individuals in settings with binary outcomes. The developers of the AFST and other researchers, such as Chouldechova et al. [20], make arguments about the fairness of the AFST in part based on race-specific AUC metrics.However, simply grouping by race and computing AUC for each group does not fully reflect the way that models like the FAST are used in practice. The BEST estimates the likelihood of a binary outcome: whether or not a child will be removed within two years.But the scores produced by the AFST are not just utilized in a binary manner: as Chouldechova et al. highlight in their visualization of the referral process [20, p. 12], the BEST informs workers’ screening decisions as well as recommendations about service information and provision. As such, we can think of the FAST as seeking to rank children who will be removed above those who will not be removed,so we also present results for Black families and non-Black families using the xAUC metric [47].Our results, summarized in Table 1 and broken down in more detail in Appendix A.1, indicate that how the tool is measured inconsequential for our understanding of how the AFST performs. Foreach metric, we compute results for the AFST using each “policy,”and demonstrate how these policies produce varying pictures of theAFST’s performance by including the range of possible performance results generated for each metric in Table 1 (individual results foreach policy and metric are included in Appendix A.1).In our analysis, the AFST often produces the “best” results (e.g.,with the lowest FPR and highest AUC) when it is measured as theCounty measured it: at the individual score and outcome level. But When we measure the AFST in a manner more closely aligned with how it is deployed — using a maximum score policy or a policy of assigning a “protocol” to each family — we sometimes see a lowerAUC, a higher false positive rate, and greater racial disparities in performance results (see Appendix A.1 for further details). The Cross-AUC analysis — across all of the policies — suggests signifi-cantly worse performance for Black people compared to non-Blackpeople; additional detail is included in Appendix A.1 Our findings highlight that post-processing decisions about how to communicate and aggregate model outputs can be consequential. Determinations about how to measure an algorithmic tool’s performance are not objective; they are subject to multiple alternatives, each with important consequences.These results support the emerging literature examining the impor-tance of measurement in the design and oversight of AI systems,which posits that measurement is in itself a governance process and explores how harms stemming from algorithmic systems can sometimes be traced in part to measurement mismatches [44, 45].Importantly, in this analysis, we do not impose normative judg-ments or recommendations about what values would represent an acceptable result on each of these metrics. We also do not intend for this analysis of metrics to be used to argue about whether thetool is fair, or to advocate for the use of specific metrics to define fairness in this context, recognizing that debates about defining algorithmic fairness in the context of specific decision-points often fail to address the realities of how algorithmic systems operate in practice. Models that appear to be fair using these kinds of metrics can still operate in and exacerbate the harms of oppressive systems[38]. One of the County’s goals in adopting the AFST, as stated in a 2018 process evaluation, was to “make decisions based on as much in-formation as possible” [43]. This “information” included data from the County’s “Data Warehouse” [22] such as records from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs. Each of these databases contributed features that were used in the model, which was developed using LASSO logistic regression “trained to optimize forthe [Area Under the ROC Curve] AUC” [98].7 In this section, we explore the impacts of the inclusion of these features in the model,focusing on features related to behavioral health — which can in-clude or be related to disability status — and involvement with the criminal legal system. We highlight concerning disparities related to these features and show that excluding these features from the model would not have significantly impacted the main metric the model was trained to optimize for — the AUC [98]. Ultimately, we find that the County’s stated goal to “make decisions based on as much information as possible” [43] comes at the expense of already impacted and marginalized communities and risks perpetuating systemic racism and oppression.4.2.1 Features from the criminal legal system in the AFST. Everyversion of the AFST has included features related to involvement with the criminal legal system, including incarceration in the Al-legheny County Jail or interactions with juvenile probation (see[99] and [98] for details on these features in V1 and V2 respectively;these types of features are also used in more recent versions of the model). As part of our analysis of the AFST feature selection process— discussed further in Appendix A.2 — we examined whether there were features of the model that were disproportionately associated with Black children compared to white children. Some of the largest racial disparities we found were for features of the model related to juvenile probation, including indicator features for whether the alleged victim had ever been involved with juvenile probation or whether the alleged victim was involved with juvenile probation at the time of the referral (these features are discussed further in [98]).Black alleged victim children were almost three times more likely to have been or to currently be on juvenile probation at the time of the referral compared to white alleged victims. The FAST also includes features for whether other members of the household have ever been in the juvenile probation system (see [98]), regardless of how long ago or the details of the case. All of these features can increase an individual’s and household’s AFST score, raising serious concerns that including these racially disparate features —which reflect the racially biased policing and criminal legal systems[4, 9, 60, 80, 83] — could exacerbate and reify existing racial biases.Including these variables has the effect of casting suspicion on the subset of referrals with at least one person marked by the “ever-in juvenile probation” flag, which disproportionately marks referral-households with at least one member whose race as reported in the data is Black. By this definition, 27% of referrals involving Blackhouseholds in the county data have a member with the juvenile probation flag, compared to 9% of non-Black referral-households.Overall, 69% of referral-households with this flag are Black.As shown in Table 2, removing the juvenile probation system data from the training data had a minimal impact on the County's Own measure of predictive performance, AUC, which changed from 0.739 to 0.737 (the retraining process is described in more detail in Appendix A.4). While removing the variables related to the juvenile probation system would have impacted the performance of the model by a negligible amount, it would have reduced the percentage of families affected by juvenile probation labeled as high-risk by over 10%, as shown in Figure 2.4.2.2 Disability-related features in the AFST. In developing theAFST, the County and the research team that developed the tool used multiple data sources that contained direct and indirect ref-erences to disability-related information. For example, the first version of the AFST [99] and the version currently deployed in Allegheny County as of the time of writing (AFST V3) include features related to whether people involved with a referral have recorded diagnoses of various behavioral and mental health disorders that have been considered disabilities under the Americans with Dis-abilities Act (ADA). Versions of the tool [99] have also included features related to public benefits — such as Supplemental Secu-rity Income (SSI) benefits — that may be related to or potentially proxies for an individual’s disability status [3]. Some iterations of the tool have excluded some of the features pulled from public benefits or behavioral health sources (public reports indicate that the County’s behavioral health agencies focus on services related to mental health and/or substance abuse [22]). For example, some pub-lic benefits data that was included in AFST V1 was excluded fromAFST V2 and V2.1 because of changes in the data format [98, p. 4]. Importantly, these records come from public agencies, so families who access disability-related health care through private services are likely not recorded in these data sources. As highlighted in a2017 ethical analysis [25] of the AFST commissioned by the County,the people whose records are included in these databases may have no way to opt out of this kind of data collection and surveillance.We analyze the inclusion of three features directly related to dis-ability status included in V2.1 of the AFST — an indicator variable for whether the “victim child” has any behavioral health history in the database, an indicator variable for whether the alleged “per-petrator” has any behavioral health history in the database, and,for a “parent” with a behavioral health history in the database, the number of days since they were last seen in behavioral health ser-vices (see [98, p. 4] for further discussion of these features). These Three features are the only variables from the behavioral health data source with a non-zero weight in V2.1 of the AFST. However,disability status may have a larger overall impact on the model of other features in the model (like features related to eligibility for public benefits programs, involvement with the criminal legal system, or others) are proxies for disability status. Because of the weights assigned to these three features in V2.1 and the binning procedure used to convert probabilities to AFST scores, being asso-ciated (through a referral) with people who have a disability and access services related to those disabilities — as encoded through these variables — can increase an individual’s AFST score by several points. This finding, discussed further in our Appendix A.2, is notjust a theoretical possibility. In both the training data we reviewed and the production data from 2021, we identified several examples of individuals who had identical values for each feature consid-ered by the model except for the indicator variable for whether the alleged “perpetrator” had any behavioral health history in the database. Among these matches, individuals with the behavioral health indicator had scores 0-3 points higher than those without the behavioral health indicator.In addition, retraining the model with this behavioral health data removed as a source of potential feature candidates produces a model with a very similar AUC (see Table 2), raising the concern that the inclusion of these features may have an adverse impact without adding to the model’s “predictive accuracy” as defined by the tool developers.4.2.3 Discussion. Feature selection is one of many consequential decisions with policy impacts in the design of algorithmic tools.There are many different ways to perform feature selection, which is often focused on ensuring that only those variables that allows model to perform best on a performance metric decided on by the model’s developers are kept in the model [17, 53, 54]. One com-mon approach for feature selection includes using some kind of accuracy maximization as a lone heuristic for feature selection,potentially based on a misplaced belief that there may be a single most-accurate model for a given prediction task [13, 63]. How-ever, emerging research has highlighted the prevalence of model multiplicity — tasks or contexts where several different models produce equivalent levels of accuracy while using different features or architectures [13]. The development of the AFST was guided byAUC-maximization [98], an imperfect measure of accuracy [59],and one that is unable to meaningfully distinguish between models with and without disability-related and juvenile probation-related features. Given this model multiplicity in the context of the AFST,deciding whether to include or exclude variables from the juvenile probation system is fundamentally a policy choice. Even taking as given a prioritization of some measure of accuracy in the model development process, model multiplicity could allow tool developers and designers to prioritize considerations beyond accuracy [13] —including interpretability, opportunity for recourse, and fairness.4.3 Marked in perpetuityWhen algorithmic decision-making systems are deployed, impacted communities are often left without concrete protections and ac-tionable resources to respond to those systems and harms that may stem from them [56]. Emerging literature focused on the concept of algorithmic recourse [48, 55, 82] explores opportunities for individuals affected by algorithmic systems to contest and challenge system outputs, potentially resulting in changes to the model's predictions or broader decision-making processes [15, 48, 78, 87].In the context of the AFST, we examine recourse for call screeners who act on the tool’s outputs and families who are evaluated by thetool, focusing on whether these groups have the ability to be aware of a risk score, understand the specific reasons and model features that led to the risk score determination, and contest both the risk score generated and the inputs to the model. We also explore the use of “ever-in” features — which indicate whether someone involved with a referral has ever been eligible for public benefits programs or been affected by the racially biased criminal legal and policing systems — through the lens of algorithmic recourse. We argue that including these static features in the model is a policy choice with serious impacts for families evaluated by the tool, including for opportunities for recourse and contesting the tool’s predictions.We explore racial disparities in the presence of these features and examine whether “ever-in” features add “predictive value” as defined by the tool’s creators. We find that the use of these features compounds the impacts of systemic discrimination and foreclosed opportunities for meaningful recourse for families impacted by the tool.4.3.1 “Ever-in” features and implications for recourse. The FAST includes several features that mark individuals or families in perpe-tuity — we refer to these collectively as “ever-in” predictors, as they are all defined in terms of a person ever having a record in a given data system, and “ever-in” is also used by the tool designers to refer to these features [98, 99]. These systems include whether a member of the household has ever been in the Allegheny County Jail or ever been in the Juvenile Probation system, as well as whether house-hold members have ever been eligible for each of a range of public benefits programs administered by Pennsylvania’s Department ofHuman Services, including Temporary Assistance for Needy Fami-lies (TANF) and SSI (see [98, 99] for details on these features). Ashighlighted in Section 4.2, some public benefits features used in earlier versions of the tool were excluded from this version of thetool. Because they are immutable, these predictors have the effect of casting permanent suspicion and offer no means of recourse for families marked by these indicators. If a parent in the household ever spent time in the Allegheny County Jail (regardless of the charges or whether that charge resulted in a conviction) or was ever eligible for public benefits (meaning they were enrolled in the state's managed Medicaid program, HealthChoices, whether or not they then used the benefits), they are forever seen as riskier to their children compared to parents whom these systems haven’t reached.Data stemming from criminal justice and policing systems is notoriously error-ridden and reflects the discriminatory practices and racially disproportionate harms of those systems [60, 83]. Ouranalysis of referrals from 2010 - 2014 showed that 27% of referrals of households with at least one Black member were affected by the“ever-in” Juvenile Probation predictor, compared to 9% of non-Blank Referral-households. Similarly, 65% of Black referral-households in the data were impacted by the “ever-in” jail indicator variable, com-pared to 40% of non-Black referral-households. Examining the public benefits features, we found that 97% of Black referral-households in the data were impacted by at least one of the “ever-in” variables coming from the public benefits data sources, compared to 80% ofnon-Black referral-households. As is the case with the behavioral health data discussed in the previous section, only families who access or establish eligibility for public services are represented in the public benefits data ingested by the AFST, meaning this data also reflects the historic and ongoing oppression and racial discrim-ination that contributes to higher rates of poverty for Black families compared to non-Black families and the historical racism that has shaped the benefits programs themselves [81, 86]. Public benefits databases are also not immune to serious data errors. For example, Colorado’s state public benefits database, which is used as a datasource for a similar predictive tool used by at least one child welfare agency in Colorado [97], has suffered from systematic errors for decades [42].The use of these features in the AFST has the effect of creat-ing an additional burden for families who have been impacted by public data systems. This additional burden is imposed not after political debate or an adversarial legal process, but quietly, through the decision to encode membership in a County database with a 1 or 0, and the related decision to use that encoding in a predictive model, despite a lack of demonstrable predictive benefit. When were-trained the model with the “ever-in” variables excluded from the training data, we found that the baseline model had an AUC of 0.739, and the model without the “ever-in” variables had an AUCof 0.737 (see Table 2), a difference of only .002. The overall effect of this inclusion is compounded by the decision to aggregate risk scores by taking the maximum score across all individuals on a referral. Figure 3 shows that, as the number of people associated with a referral increases, the likelihood that at least one of them will have some history with the Allegheny County Jail and/or theHealthChoices program increases as well, and this is especially true for referrals associated with households who have at least oneBlack member. Overall, using the testing data from the model devel-opment process and the county’s “high-risk protocol” to measure screen-ins that would have been recommended by the tool, we see referrals where at least one of the associated household members is Black (as recorded in the data) being recommended for screen-inat systematically higher rates than non-Black referral-households,and the disparity grows with the size of the referral (see AppendixA.3 for more detail). For referrals containing five or more individual records, 47.4% of Black referral-households are recommended for screen-in compared to 30% of non-Black referral-households, for a disparity of 17.4%. These disparities persist, but are reduced, after removing the “ever-in” variables before training the model. For Instance, the racial disparity for referral-households with five or more people drops to 15.6% under the model without the “ever-in”predictors. We include additional detail and summaries in AppendixA.3.4.3.2 Recourse in deployment. The notion of algorithmic recourse has been defined and operationalized in various ways, invoking different aspects of an algorithmic system’s development and use and focusing on different types of agency that recourse could afford to individuals and groups affected by algorithmic systems [48, 78]. For example, some definitions suggest that algorithmic recourse is just the ability for affected parties to receive explanations about how an algorithm generated a particular output [48], while other definitions suggest that recourse is the ability to not only knowhow an algorithm reaches its decisions, but also to change decisions stemming from an algorithmic system [100]. Through its use of“ever-in” features, the AFST encodes a lack of recourse for families who have been affected by the criminal legal system or who have enrolled in public benefits programs.We can also examine the context in which the tool operates to understand potential algorithmic recourse for people impacted by the AFST, including families and others who interact with the tool.Prior research and reporting about the AFST has touched on many elements of recourse related to the tool’s deployment. For example,reporting about the AFST has highlighted how many affected fam-ilies may be unaware that a predictive tool is even being used in the child welfare system in Allegheny County [41], and even when families do know a predictive tool is being used, emerging research suggests they are concerned about the tool’s use and oversight, but lack a clear avenue for their concerns to be addressed [94]. Recourse For call screeners is also an important question; for instance, Chenget al. [18] found that some call screeners think families with previ-ous system involvement are assigned higher risk scores than other families, and to adjust for this perception, call screeners would sometimes disregard the AFST's risk assessment determination if they thought that the principal reason for a family’s high risk score was their socioeconomic status. By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11% [18]. Much of the call screeners’ work of looking for the reasoning behind racially and socioeconomically disparate risk score outputs from the FASTmodel may be guesswork [49], as call screeners are not shown the precise values of features that are used to generate scores.  ",1294-1301,"By challenging the outputs of the AFST, child welfare workers reduced the racial disparity in screen-in rates between Black and white children that would have resulted from complete adherence to the algorithm by 11%"
