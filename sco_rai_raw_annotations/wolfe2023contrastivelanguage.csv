,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{wolfe2023contrastivelanguage,
    author = {Robert Wolfe and Yiwei Yang and Bill Howe and Aylin Caliskan},
    title = {Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-19, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,SexualizationBias,Perceived_Problem,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7","evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts",
10,Women,Agent,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",women,
11,ObjectificationMediatedEmotion,Causal_Theory,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",recognition of emotional state is mediated by whether the subject is fully or partially clothed,
12,WikipediaContributors,Agent,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7","EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information",
13,LION-400M,Artifact,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",LION-400M,
14,LanguageVisionAIModels,Agent,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",language-vision AI models trained on web scrapes with theContrastive Language-Image Pretraining (CLIP) objective ,
15,SOBEMDatabase,Artifact,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",Sexual OBjectification and EMotion Database,
16,SexualizedImage,Artifact,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7","sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion",
17,WedImageNetCorpus,Artifact,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7","WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]",
18,Misogyny,Causal_Theory,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",Pornographic images and misogynistic text,
19, , , , , ,
20,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
21,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
22,SexualizationBias,constrainsAgent,LanguageVisionAIModels,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7","language-vision AI models trained on web scrapes with theContrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts"
23,Women,hasProducedArtifact,SOBEMDatabase,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",images of women from the Sexual OBjectification and EMotion Database
24,ObjectificationMediatedEmotion,constrainsAgent,LanguageVisionAIModels,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed
25,LanguageVisionAIModels,hasProducedArtifact,SexualizedImage,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7","a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%."
26,WikipediaContributors,hasProducedArtifact,WedImageNetCorpus,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7","WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M"
27,LION-400M,influencesPrecept,Misogyny,"Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evalu-ated for evidence of a bias studied by psychologists: the sexual objec-tification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objecti-fication and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests(EATs) return significant effect sizes for both anger (𝑑 > 0.80) and sadness (𝑑 > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age]year old girl"" generates sexualized images (as determined by anNSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17),and up to 42% of the time for Stable Diffusion (ages 14 and 18);the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. [...] This research examines pretrained CLIP models available in theCLIP library, including three Vision Transformers (denoted ViT)and five ResNets (denoted RN). The open source CLIP model Open-CLIP (ViT-B32-quick gelu) is pre trained on a different internet-scale multimodal corpus (LION-400M) [40] and is examined in this work to assess generalization of results across internet-scale training cor-pora. 3.2.1 CLIP Training Corpus CLIP trains on the WebImageText(WIT) corpus, which contains 400 million images and correspond-ing captions [59]. The query list for generating WIT includes name of Wikipedia articles, words occurring 100 or more times on EnglishWikipedia, bigrams from Wikipedia with high pointwise mutual information, and all WordNet synsets [59].3.2.2 LAION-400M The OpenCLIP model examined trains for 32 epochs on LION-400M, a corpus constructed to provide an open source alternative of comparable scale and content to WIT [67]. Pornographic images and misogynistic text were identified in an audit of LION-400M [7].3.2.3 VQGAN-CLIP Training Corpus VQGAN-CLIP generates im-ages using a pre trained CLIP and a pre trained VQGAN [17]. Thus, its outputs are dependent on the WIT training dataset [59], and on the data used to train VVQGAN. Publicly available VQGAN check-points include those trained on databases of art (WikiArt [66]), faces(FFHQ [42]), and more general checkpoints for producing images,such as ImageNet [19]. Because the present research is concerned with the generation of more realistic, sexualized images of girls and women, the ImageNet 16384 checkpoint is assessed, as it is able to produce more realistic images than WikiArt, and is less constrained than a checkpoint trained only to generate faces. Similar to theCLIP’s training corpus, ImageNet is organized according to theWordNet hierarchy. 3.2.4 Stable Diffusion Training Corpus Stable Diffusion-v1-4 istrained on a subset of the pairs of images and captions in LAION-5B, a dataset consisting of 5.85B CLIP-filtered image-text pairs.3 .2.5 Antarctic Captions Data Antarctic Captions uses the Concep-tual Captions dataset [69] to fine-tune a BART language model [45]for caption generation. The model selects candidate n-grams from which BART forms sentences from 50k CLIP-encoded unigrams and bigrams [78]. The SOBEM database is the sole standardized and controlled picture database available and designed to study sexual objectificationRuzzante et al. [65]. SOBEM contains 28 standardized photographs each of 10 Caucasian women. Four emotional states are included:Neutral, Angry, Sad, and Happy [65]. The Angry, Sad, and Happystates include high-emotion (more clearly visible on the face) andlow-emotion (emotion more subtle) images [65]. Each emotional state includes two photographs (hair is tied behind the head and hair falls loose over the shoulders) of the subject in a Non Objectified Condition, and two photographs in an Objectified condition [65].In the Objectified condition, the female subject is photographed from the waist up wearing a black bra but no shirt [65]. In theNonobjectified condition, the same subject is photographed from the waist up wearing a black shirt which covers the entire chest[65]. The expression of emotion is in response to an instruction to display that particular emotion [65]; the experiments herein test model associations with the individual’s intended emotional state,not the emotional state as assigned by another perceiver. We collect nonsexualized images of professionals to study the rela-tive association of women and men with Sex vs. Profession using EATs for the Science, Medicine, and Business domains. Commen-surate with methodology employed for prior work on semantic bias in self-supervised computer vision [73], visual stimuli used inSex vs. Profession EATs are collected from Google Image searches.For Science, queries are ""female scientist"" and ""male scientist""; forMedicine, queries are ""female doctor"" and ""male doctor""; and forBusiness, queries are ""female CEO"" and ""male CEO."" Twenty top female images and male images are selected for each query. Images Are excluded if they depict more than one person; if the person in the image is occluded by an object; if the image is sexually sugges-tive such that the experiment might be confounded; if the person in the image is famous (known by name to the author(s)); or if the image bears a watermark or overlaid text.","1174, 1176-7",Pornographic images and misogynistic text were identified in an audit of LION-400M
