,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation, ,"{abdu2023anempiricalanalysis,
    author = {Amina A. Abdu and Irene V. Pasquetto and Abigail Z. Jacobs},
    title = {An Empirical Analysis of Racial Categories in the Algorithmic Fairness Literature},
    year = 2023
}", , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-08, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,Researchers,Agent,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Researchers,
10,Simplicity,Perceived_Need,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,for analytic simplicity,
11,UnjustifiedCategorization,Perceived_Problem,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,typically fail to justify their choice of a particular racial categorization schema,
12,EarlierChoices,Strategy,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,choices made at an earlier stage by someone else,
13,DataQualityDistrust,Perceived_Problem,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,distrust in data quality,
14,DataCollectors,Agent,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,data collectors ,
15,RacialCategories,Artifact,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,the racial categories adopted in algorithmic fairness,
16,InputTypes,Causal_Theory,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,what types of inputs they can handle,
17,Algorithms,Agent,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Algorithms used to identify or mitigate unfairness ,
18,BlackWhiteSchema,Artifact,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,"a two-category schema, this time distinguishing between Black and white",
19,StatisticalConcerns,Perceived_Problem,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Statistical robustness may motivate ,
20,TechnicalConstraints,Perceived_Problem,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,technical constraints and desiderata lead,
21,AlgorithmicFairnessNorms,Goal,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,"as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor",
22,JustificationsOfCategorizations,Artifact,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Justifications of racial categorization schemas,
23,AppealToScientificRigor,Strategy,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,"draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision",
24,CulturalContext,Causal_Theory,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,the cultural context in which the work is situated.,
25,AIFairnessInstitutions,Agent,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,algorithmic fairness institutions.,
26, , , , , ,
27, , , , , ,
28,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
29,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
30,UnjustifiedCategorization,constrainsAgent,Researchers,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Researchers typically fail to justify their choice of a particular racial categorization schema
31,Simplicity,constrainsAgent,Researchers,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Researchers may choose not to modify the schema used in their data for analytic simplicity
32,EarlierChoices,constrainsAgent,Researchers,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,"To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. "
33,DataQualityDistrust,constrainsAgent,Researchers,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,"Finally, distrust in data quality may lead researchers to choose a particular schema:"
34,DataCollectors,hasProducedArtifact,RacialCategories,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,data collectors exert influ-ence over the racial categories adopted in algorithmic fairness
35,InputTypes,constrainsAgent,Algorithms,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle
36,Researchers,hasProducedArtifact,BlackWhiteSchema,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,"the researchers once again choose a two-category schema, this time distinguishing between Black and white"
37,StatisticalConcerns,constrainsAgent,Researchers,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size.
38,TechnicalConstraints,constrainsAgent,Researchers,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.
39,AlgorithmicFairnessNorms,constrainsAgent,Researchers,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,"as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community."
40,AlgorithmicFairnessNorms,constrainsAgent,AIFairnessInstitutions,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions.
41,JustificationsOfCategorizations,reflectsPrecept,AppealToScientificRigor,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,"Justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision"
42,CulturalContext,constrainsAgent,Researchers,"We find that researchers typically fail to justify their choice of a particular racial categorization schema. In total, only 13 papers (21.7%) provide any reasoning behind their chosen schema. Evenwhen subsetting to the 24 papers that adopt a specific categorization schema, only 9 (37.5%) present some form of justification. When justifications are provided, they fall into five broad categories: data availability, technical factors, appeals to prior work, epistemic concerns, and relevance. These types of justifications are not mutually exclusive, and are often inter-related. We summarize each typeof justification below, with relevant examples from the annotated papers. Researchers may adopt a particular racial categorization schema based on how race is presented in the data they use for analysis. Researchers may choose not to modify the schema used in their data for analytic simplicity, as in the following example: “We use race and ethnicity as a combined field in this paper because that is how the data was collected and organized in the LA City Attorney’s Office system.” –Rodolfa et al. 2020 (p. 147). In this case, the researchers default to the decision made by the Los Angeles City Attorney’s Office for its own administrative purposes. Even if researchers choose not to adopt the same schema as in their data, they may still be affected by the choices of data collectors. To the extent that researchers use data that they did not originally collect, they may be limited by choices made at an earlier stage by someone else if relevant information is obscured under the original data collection schema. In the following example, race data are not collected at all, leading the researchers to use an arbitrary variable in its place: “the first binary feature was used as a substitute sensitive feature since we did not have access to sensitive features.” –Dwork et al. 2018 (p. 3). Finally, distrust in data quality may lead researchers to choose a particular schema:“based on our analysis of the consistency of racial classification within the court data, we have determined this categorization scheme introduces the fewest problems with inconsistent classification” –Lum et al. 2020 (p. 488) Based on these examples, we argue that data collectors exert influ-ence over the racial categories adopted in algorithmic fairness both directly (by foreclosing certain analyses) and indirectly (through defaults and varying data quality). A number of technical considerations may influence a researcher’s choice of racial categories. Algorithms used to identify or mitigate unfairness may be constrained in what types of inputs they can handle, particularly in the case of novel methods. This often leads to an emphasis on binary racial categorization schemas, such as the privileged/not-privileged dichotomy chosen in the example below:“Some algorithms additionally require that the sensitive attributes be binary (e.g., “White” and“not White” instead of handling multiple racial cat-egorizations) - for this version of the data (numeri-cal+binary) we modify the given privileged group tobe 1 and all other values to be 0.” –Friedler et al. 2019(p. 332) Computational efficiency for complex algorithms or in the anal-ysis of large data sets may also lead researchers to choose simpler categorization schemas. In the following example, the researchers once again choose a two-category schema, this time distinguishing between Black and white.“To make brute force auditing computationally tractable,we designate only two attributes as protected; pct white and pct black, the percentage of each community that consists of white and black people respectively.” –Kearns et al. 2019 (pp. 106-108). Finally, statistical robustness may motivate researchers to choose racial categorization schemas such that each category has a suffi-ciently large sample size. This could lead researchers to omit groups with small populations or to combine these groups into larger cate-gories, as in the following example:“[S]everal demographic categories appeared rarely, ifat all, in the Twitter data. For the sake of more r-bust statistical comparisons, some analyses below collapse these race categories to, for example, {White;Black; Hispanic; Other; Don’t Know}.” – Borradaile etal. 2020 (p. 574). In each case, technical constraints and desiderata lead researchers toward simpler categorization schemas with fewer racial categories.Though justifications for racial schemas are rare in the annotated papers, the prevalence of binary schemas suggests that technical motivations may play an important role in the adoption of racial categories within the algorithmic fairness community. Some justifications draw on prior academic research. These justifi-cations often draw from beyond the algorithmic fairness literature,which is relatively new and has fewer established standards com-pared to, for example, the dermatology community cited in the following case.“We chose the Fitzpatrick six-point labeling system to determine skin type labels given its scientific origins.Dermatologists use this scale as the gold standard for skin classification and determining risk for skin cancer” – Buolamwini & Gebru 2018 (p. 6). However, as the algorithmic fairness community begins to establish its own norms, publication standards, and notions of rigor, future work in the field may instead appeal to existing work from within the community. In the following example, the authors cite the Buo-lamwini & Gebru paper quoted above as justification for adopting a similar racial categorization schema in a similar context.“Similar to prior work, skin color is used as a surrogate for race membership because it is more visually salient.” –Yang et al. 2020 (p. 554). This process of self-perpetuation and naturalization points to the importance of norms and standards within algorithmic fairness institutions. In addition to referencing specific scientific work, justifications of racial categorization schemas also draw on more general notions of scientific rigor by appealing to epistemic principles like reliability,consistency, objectivity, and precision: “ Importantly, we determined that different coders following this protocol could reliably classify the race and gender of users. ” – Borradaile et al. 2020 (p. 574)“Since race and ethnic labels are unstable, we decided to use skin type as a more visually precise label to measure dataset diversity. Skin type is one phenotypic attribute that can be used to more objectively char-acterize datasets along with eye and nose shapes.” –Buolamwini & Gebru 2018 (p. 4)As the algorithmic fairness community begins to establish its core epistemic values through publishing standards and methodological norms, these values will likely influence how researchers adopt racial categories and justify their choices. Some papers justify their use of a particular categorization schema based on its relevance to the context of study. Researchers may adopt racial categories that reflect the cultural context in which the work is situated. In the case of the algorithmic fairness literature,researchers often draw on the U.S. context. As a result racial categories typically reflect notions of race stemming from the U.S.’s particular histories of slavery, segregation, and discriminatory pol-icy. In the following justification, the researchers explicitly attempt to capture social understanding of race in the U.S. setting:“ Gender and race are fluid and socially constructed categories, and there are other possible ways of cat-egorizing the gender and race of users. However, we believe these categories provide a reasonable, though necessarily simplified, reflection of race and gender divisions in the US.” – Borradaile et al. 2020 (p. 574)Though justifications are rarely given in the annotated documents, cultural context can explain the prevalence of categorization schemas that center Blackness and whiteness. These categories of analysis are particularly relevant due to legacies of anti-Black racism and white supremacy. Beyond the larger cultural setting, justifications may also focus on racial categories’ relevance in a particular domain of study.“Furthermore, skin type was chosen as a phenotypic factor of interest because default camera settings are calibrated to expose lighter-skinned individuals.”– Buolamwini & Gebru 2018 (p. 4)The contextual relevance approach to racial categorization high-lights the fact that inconsistencies across or even within papersare not necessarily a problem. Differences in racial schemas may reflect important differences in the social groups that are relevant to understanding and intervening in discrimination",1329-31,Researchers may adopt racial categories that reflect the cultural context in which the work is situated.
