,A,B,C,D,E,F
1,~~~ Paper Details ~~~~, , , , ,
2,bibliographicCitation,"{richardson2023addremoveor,
    author = {Brianna Richardson and Prasanna Sattigeri and Dennis Wei and Karthikeyan Natesan Ramamurthy and Kush R. Varshney and Amit Dhurandhar and Juan E. Gilbert},
    title = {Add-Remove-or-Relabel: Practitioner-Friendly Bias Mitigation via Influential Fairness},
    year = 2023
}", , , ,
3,~~~ Annotator Details ~~~~, , , , ,
4,identifier,dk, , , ,
5,role,annotator,DO NOT CHANGE, , ,
6,generatedAtTime,2023-12-11, , , ,
7,~~~ Class Instance / Individual Annotations starts below ~~~~, , , , ,
8,individualName,type,sourceExcerpt,sourcePageNumber,sourceExtractedDescription,
9,AlgorithmicBias,Perceived_Problem,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",Algorithmic bias is a persistent obstacle in the realm of machine learning (ML),
10,NeedForExplanations,Perceived_Problem,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",require explanations of unfairness,
11,GlassBoxAccess,Other_Precept,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model.",
12,FewRegulations,Other_Precept,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.)",
13,Protocols,Causal_Theory,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",follow strict protocols that prohibit certain types of modifications to the model.,
14,StakeholderDemands,Perceived_Problem,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","They have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105].",
15,BetterDataCollection,Strategy,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",most likely to utilize methods that allow them to collect better data instead of making modifications to existing data,
16,NoNewData,Perceived_Need,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","they have no access to new data, perhaps due to the cost or imprac-ticality of data collection",
17,DataAugmentationAndSynthesis,Strategy,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.,
18,Collaboration,Strategy,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",those who work in collaboration with the practitioners with access to the model internals and training data.,
19,NonCollaboration,Strategy,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",do not work in collab-oration with practitioners with access,
20,MachineLearning,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",machine learning (ML),
21,ImageRecognition,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",image recognition and object detection,
22,HealthAssessment,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",health assessment,
23,AdvertisementSystems,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",advertisement systems ,
24,Practitioners,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",Practitioners,
25,FreeRangePractitioner,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Free Range Practitioner,
26,ProtocolPractitioner,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Protocol Practitioner,
27,DataLimitedPractitioner,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Data-Limited Practitioner,
28,AuditingPractitioner,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Auditing Practitioner,
29,CooperativePractioner,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Co-Operative Practitioner,
30,IndpendentPractioner,Agent,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",independent practitioners,
31,BlackBoxAccess,Perceived_Problem,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model.",
32, , , , , ,
33,~~~ Property / Axiom / Triple Annotations starts below ~~~~, , , , ,
34,annotatedSource,annotatedProperty,annotatedTarget,sourceExcerpt,sourcePageNumber,sourceExtractedDescription
35,AlgorithmicBias,constrainsAgent,MachineLearning,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]."
36,AlgorithmicBias,constrainsAgent,ImageRecognition,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]."
37,AlgorithmicBias,constrainsAgent,HealthAssessment,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]."
38,AlgorithmicBias,constrainsAgent,AdvertisementSystems,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]."
39,NeedForExplanations,constrainsAgent,Practitioners,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",Practitioners require explanations of unfairness
40,GlassBoxAccess,constrainsAgent,FreeRangePractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model."
41,FewRegulations,constrainsAgent,FreeRangePractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","The Free-Range Practitioner. [...] they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.)"
42,GlassBoxAccess,constrainsAgent,ProtocolPractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data."
43,Protocols,constrainsAgent,ProtocolPractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Protocol Practitioner [...] they also follow strict protocols that prohibit certain types of modifications to the model.
44,StakeholderDemands,constrainsAgent,ProtocolPractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","The Protocol Practitioner [...] They have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]."
45,BetterDataCollection,constrainsAgent,ProtocolPractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Protocol Practitioner [...] This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data
46,GlassBoxAccess,constrainsAgent,DataLimitedPractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data.
47,NoNewData,constrainsAgent,DataLimitedPractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","The Data-Limited Practitioner [...] they have no access to new data, perhaps due to the cost or impracticality of data collection "
48,DataAugmentationAndSynthesis,constrainsAgent,DataLimitedPractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",The Data-Limited Practitioner [...] Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.
49,BlackBoxAccess,constrainsAgent,AuditingPractitioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9","The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model."
50,Collaboration,constrainsAgent,CooperativePractioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data.
51,NonCollaboration,constrainsAgent,IndpendentPractioner,"Algorithmic bias is a persistent obstacle in the realm of machine learning (ML), impacting nearly every industry where it is applied. Concerns have arisen with respect to image recognition and object detection [20, 112], health assessment [40, 80, 90], advertisement systems [21, 42, 69, 109], and others [5, 19, 78]. Fairness research focuses on both the detection and the mitigation of bias in machine learning algorithms. The bulk of these contributions have been in the last decade; across the landscape of this research, there is an incredibly large collection of mitigation methods catering to diverse use cases, tutorials for utilizing mitigation methods, and toolkits that easily integrate into existing ML pipelines [89]. Furthermore,institutions are encouraging the production of fair machine learn-ing tools now more than ever [51]. Despite the prolific number of research works, there is a severe lack of application of fairness technologies in practice [48, 88, 98, 105]. Practitioners require explanations of unfairness, transparent mitigation methods, and methods that are sensitive to their limitations as practitioners who abide by protocols and regulations established by their employer or by their own accord.Influence functions from robust statistics [30] have been trans-formative for transparent and explainable ML. Using the Hessian matrix and the loss gradient, one can compute the influence that each training point has on a test outcome. This strategy has been shown to increase transparency, explain model behaviors, and iden-tify adversarial examples [62]. Furthermore, recent work by [92]has shown that influence functions can also be used with group fairness objectives. This work aims to demonstrate the extent of that finding and employ the properties of influence functions to induce more transparency in fair machine learning pipelines.This work introduces a new avenue of fair AI research coined influential fairness (IF). IF aims to add explainability, transparency,and contextualization to the procedure of bias mitigation via fair-ness influence functions. Furthermore, it aims to provide a diverse array of solutions to fit the diverse needs of practitioners across applications. Through the formulation of practitioner profiles, we delineate four types of practitioners that emerge in fairness liter-ature. Utilizing these profiles, we design mitigation methods that satisfy the user needs required by these practitioners. Our proposed mitigation methods take complex functions from robust statistics and transform them into simple add, remove, or transform strategies that encourage human-in-the-loop implementation. We propose four mitigation methods and provide additional implementation options to align with the limitations of our practitioner profiles.We demonstrate the effectiveness of our proposed strategies by testing their performance on benchmarked fairness datasets with several group fairness objectives. Our research contributions are as follows:• A novel formulation of profiles that organize the diverse needs and limitations of practitioners.• New mitigation methods utilizing group fairness influence functions, designed with transparency and practitioner limi-tations in mind, by allowing practitioners complete control of the type and number of modifications to their data.• A novel evaluation of black-box and glass-box estimates of fairness influence functions to assist practitioner profiles with only black-box access to models.• Lastly, a mapping of our curated methods to the practitioner profiles that best match their limitations. [...] We define four distinct practitioner roles.The Free-Range Practitioner. This practitioner has glass-box access to their model, the training data used to train the model, andexternal data that can be incorporated into their model. Further-more, they have few regulatory protocols limiting their ability to modify the data and the models (remove bad data, update model,control over data collection, etc.). There are no assumptions needed with respect to the free-range practitioner. Most existing solutions assume that most practitioners are free-range. Such a practitioner can do any type of modification because they are limited neither by protocols nor data. The Protocol Practitioner. This practitioner also has glass-box access to their model, the training data, and external data.However, they also follow strict protocols that prohibit certain types of modifications to the model. These protocols can be based on personal philosophy or they could be limitations put in place by the practitioner’s corporate or government regulations [81, 88, 98].Transparency is especially important for this practitioner. They Have stakeholders that they must explain modifications to, so it is critical they understand the functionality of their mitigation method[57, 105]. Furthermore, they have regulations that disallow for their labeling of points, the deletion of data, or often even the use of sensitive attributes [81]. This practitioner is most likely to utilize methods that allow them to collect better data instead of making modifications to existing data [88, 105].The Data-Limited Practitioner. This practitioner also has glass-box access to their model and the training data. They have freedoms with respect to manipulating the data and the model, but they have no access to new data, perhaps due to the cost or imprac-ticality of data collection [8, 28]. A special category of data-limited practitioners are those without access to sensitive attributes [70].Several works propose viable solutions for this type of limitation[31, 104, 116, 123]. The data-limited practitioner requires bias miti-gation methods that utilize the data that the practitioner does have. Their reliance on data might discourage them from removing data. Existing bias mitigation methods that this user is the most likely to engage with include data augmentation or data synthesis.The Auditing Practitioner. This practitioner has only black-box access to the model and access to external data. They can run data through the model and get outcomes, but they cannot see the internal mechanisms of the model. They also do not have access to the data used to train the model. Auditing practitioners can be further delineated into cooperative and independent. Co-operative practitioners are those who work in collaboration with the practitioners with access to the model internals and training data. In contrast, independent practitioners do not work in collab-oration with practitioners with access. Auditing practitioners are rarely considered in fairness research despite the fact that many fairness practitioners are auditing practitioners.","736-7, 738-9",independent practitioners do not work in collab-oration with practitioners with access
